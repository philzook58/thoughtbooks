

-- tree like composition
- indetity function is also project functions.
data PrimRec = 
Const Int | Id Int Int | Succ | Comp PrimRec [PrimRec] 
   | Rec Int {-g -} PrimRec {- h -} PrimRec

   -- number of carrie
   d along variables
-- derivation = method

mu :: (Int -> Bool) -> Int
mu f = head [x | f x]

--program enumeration

data SKI = S | K | I | Comp Prog Prog deriving Enum

class Language p where
    goodprogram :: p -> Bool
    to_ski :: p -> Maybe SKI -- Nothing if bad program
    from_ski :: SKI -> p

goodprograms = [p  | p <- allprograms, goodprogram p]



reject bad orograms
accept good programs

homeomorphisms of 4d manifold triagnlation is unsolvable?


elaboration
add in type annotation
every bidning site has a type attached
typeclass is a record actually put in

/\a. \(d: Ord a). \(xs : a). sort @a d (reverse @a xs)


classic damas-milner
algorithm W
unification variables
zonking

principal type - unique most general type
typeclass solving is similar

deferring solving
atapl last chapter


language of constraints
Huh. Rather than hacking GHC at the type level
One could directly write programs in the constraint language
using GHC as a library

level numbers every unification cariable has a level number
NOOOOOOOOOOOOOOOO SIMOOOOOONN!!!!




"meta type variable"




Simon peyton jones
type inference engine


towers of intepreter
user level
meta level

BLack - kenichi asai 1996

chez scheme


what is the catoergical analog of polymorphic lambda
Girard-Reynold-?

Multipole expansion and OPE
If we consider just the gaussian perturbation theory case
And we wish to accelerate the computations
It already makes sense to use the FMM
GGGGGGGGG with complex index structure.
field theory in a box
Integration part though... Hmm. Well we can perform precise integrals of the expanded pieces
probably.

The OPE cuts the knot by supposing the far field expansion exists, but is shady about the near field
The near field is a problem anyhow

pull the computation back into the QFT

ln( |z-z| ) =? Re ln (z - z) ~ ln(z (1 - z/z)) ~ ln(1 + )
z = Z + e
ln(Z + e - z) ~ ln((Z - z) + e) ~ ln (Z-z)(1 + e/(Z-z)) ~ ln(Z-z) - q + q^2 - q^3 + q^4 ... 
q = e/(Z-z)

1/(1+x) = 1 + x - x^2/2 + x^3/3 + 


1. (far field) series expansion (larent? Pade?). Polynomials. Perhaps multivariate.
2. 
3. 

The fft can be written as a sparse matrix if you use auxiliary variables.
Low rank matrices can also be written as sparse

# # #
# # # # -- 4 blocks is enough to make progress.
# # # # #
#1d - edges aee annoying
# what is "field"? charge values at grid points? Positions of charges sorted?
# what deos fmm return? Coefficients of far field expansion? And what exactly is the form?
# if we cache field expansion coefficients, polls to fmm take ln(N) times.
# does it return evaluation of the field at all grid points?
def fmm(x, field, left_edge = True, right_edge=True): # x is position to evaluate at.
    N_Blocks = 5
    N = field.size
    blockedfield=  field.reshape( (N_blocks, N//N_blocks ))
    fmm( field[0,:] , left_edge=False)
    fmm( field[-1,:] , right_edge=False)
    
    for block in range(1,N_blocks-1)

input points in quadtree?
What if input is a spline function describing charge distrubtion and we're doing true integration.

I feel like taking the primitive thing flow aorund to be coefficient expansions helps.
Powser
If we include error terms, we can merge things judging which is mort important

newtype ErrorNum = ErrorNum (Double, Double)
data ErrorStream = [ ErrorNum]
-- pull on them evenly or pull on the worse error.  
 ((x,ex) : ) + ((y,ey), ys) | ex <= ey =  (x + y, abs ey + abs ex  ) :  xxs + ys
                            | ex > ey =  (x + y, abs ey + abs ex  ) :  xs + yys

If we can keep the actual computation protected as lazy also, we in ordainry use will probably just use the error
until it is good enough, and then open up the answer.


Poly (ErrorStream) = polynomial with coefficients that possess error themselves.
Necessary for shifting operations, because evaluation of the polynomial can only proceed so far.

-- one way of doin it
translate PointedPoly dx = 

No. I'm totally wrong. You CAN finitely translate expansions...? wat?
Only laurent expansions?
Î»



tensor multipole method

Poly = ([coeffs], expansion_point )


[( coeff, error )]

conversion of laurent series to taylor.
recentering taylor  - laurent = recentering taylor at inifnity
obviously not possible. we'd need to exvaulate the series to get it's exact value at another position
(x - a)

Hmm. The expansion he shows is not in (delta/R). It is just in one of the coordinates
Doesn't really seem right


Static analysis 1
a moeller " static program analysis"


imp
simp
just integer types?
just 8 bit types?
data 

data Command = Assign Var Expr | While Expr Command |  ITE Expr Command Command
data Expr = Plus Expr Expr | Times Expr Expr | LT Expr Expr | Const Int 
data IMP = Seq Imp Imp | Pass | While 


data Tensor = SumSpace [Char] TensorTerm 
              | G Char Chat 
              | V Char Char Char Char
              | J char 
              | Plus [Tensor]
              | Const Double -- FreeNum? Const is basically I form
              | Times [TensorTerm]

| Delta Char Char -- does the lambda calulus or other binding forms allow Delta? It seems rare.

\x y -> (delta x y) -- means that really the function is partial?
Sum x y (delta x y) = N
\x -> delta x y -> -- this makes sense. it introudces y as a new binding form
delta = let expressions


  (Times xs) * (Times ys) = Times (xs ++ ys)
  (Times xs) * ys         = Times (xs ++ [ys])
  x          * (Times ys) = Times (x : ys)
  x         * y           = TImes [x,y]

-- don't I also have to do some index renaming? yeaaa
-- either output the new string or take it as input
-- so maybe start by renaming all bound variables such that they are not 
derivJ :: Maybe String -> Tensor -> (Tensor, String)
derivJ None (Sum [i] t) = Plus [  rename i $ Sum [the rest ] (derivJ i t)         |  i <- is] 
derivJ _ G = Const 0
derivJ _ V = Const 0
derivJ i (J i') | i == i' = Const 1
                | otherwise = Const 0
derivJ i (Times ts) = 



-- d (f exp J G J) = (d f + G J) exp J G J
expderiv t = let (t,j) = deriv t in t + (Sum [i] (G j i) * (J i))


{-
Find shared subterms
Reorder operations for easier evaluation

-}

Sum [i,j] $ (J i) * (G i j) * (J j)

Poly Tensor -- polynomial in small factor? Surrrre




  valid substitutions

  data AsymptoticComplex = Asym Int -- just the exponent, or maybe constant outside factor?
  -- or we could just use a polynomial data type. 
  cost ::  Tensor ->  
cost G = 1
cost J = 1
cost V = 1
cost (Plus xs) = sum $ map cost xs
cost (Times xs) = prod $ map cost xs
cost (Sum Char x) = n * (cost x)
cost (Const _) = 1



ocaml playing atari games
supervised learning


type camel_or_python

classifer ; image -> caml_or_python
loss : predictions : camel_or_python list -> labels : -> float

classifier : parameter -> img -> camel_or_poython

what if I had a good language of classifiers? enumerate them all
I mean that is kind of the decsion tree methodology

grad relies on pytorch

pytorch 
Tensor.of_float [| 1; -1; 2.|]

let deriv x = let x = Tensor.og_float0 x |> Tensor.set_requres_grad ~r:true
in
let y = Tensor.(x * x + x) in 

python-ocaml ffi?

hmm Almost complete match of python api
ocaml gives yo unothing except that it is in ocaml
makes sense I suppose

foo :: IO _
print _foo



type mod isomorphisms I said = Nat but that made me uncomfortable
1-1 correspondence is better
WHich includes infinite crazy things too
we demosntrate the tpyes are of the same cardinality
(a -> a) ~ 

set difference as a galois connection?
A\/B sub C <=> A sub C - B

most of these things are algorithms
for streams
(a -> b, b -> a)

enumeration. requires uniqueness?
e :: [a] is an enumeration
f ++ e is also an enumeration

infinite :: () -- value = () or _|_ bottom
infinite [] = ()
infinite (_ : xs) = infinite xs

bottom :: a
bottom = bottom -- has value _|_

type Bool' = ()
true :: Bool'
true = ()
false :: Bool'
false = bottom

-- data Stream = Stream Stream
-- type Nat' = Stream
data Nat' = Succ Nat'
zero = bottom
one = Succ zero
-- can I define plus?
plus (Succ x) y = Succ (plus x y)
plus 

--- I can define min
min (Succ x) (Succ y) = Succ (min x y)

-- max?
max 


and :: Bool' -> Bool' -> Bool'
and () () = true
and _  _  = false

eq () () = 

eq :: Nat' -> Nat' -> Bool'
eq (Stream )

infinite e = _|_ and
ifinite f = () implies
infinite (f ++ e) = _|_ 

diagonalargument :: ()

a cantorian diagonal in haskell
defaults are confusing
diagonal :: [[Int]] -> [Maybe a]
diagonal ( (y:ys) :xxs ) = (y + 1) : diagonal (map tail xxs)
diagonal ( [] : xxs ) = (0) : diagonal (map tail xxs)

class Quotient a where
   inType :: a -> Bool
newtype Evens = Even Int
instance Quotient Evens where
  inType (Even x) = x `mod` 2 == 0

why are data structures possible. Word division of the machine


top for haskell
suppose I had just a signature and no defining equations
Then I feel perhaps we could say this functions returns top. Is top?

top :: a


denotePoly :: SystemFType -> Relation -- ?


Typ = Forall String Type | Arr Typ Typ | Var String
Term = Lam | TLam | Var | App | TVar

data Term var = Lam var (Typ tvar) Term | TLam tvar 



-- basic system F lambda
data Typ = Forall String Type | Arr Typ Typ | TVar String
data Term = Lam String Typ Term | App Term Term | TApp Term Typ | TLam String Term | Var String

data Var a = Var String

data Term a where
  Lam :: Var a -> Term b -> Term (a -> b)
  TLam :: Term (f a') -> Term (forall a. f a)
  Var :: Var a -> Term a
  App :: Term (a -> b) -> Term a -> Term b


forall counterexamples, there exists a proof?



data Term var a where
  Lam :: var a -> Term b -> Term (a -> b)
  TLam :: forall a'. Term (f a') -> Term (forall a. f a)
  Var :: var a -> Term a
  App :: Term (a -> b) -> Term a -> Term b



reynolds :: Type -> Rel a b

reynolds (Forall String Type) renv = 



TermSysF -> Rel a b
interp (TLam ) = 
interp Lam

class Reynolds t where
   abstraction :: Rel a b

instance Reynold (a -> b) where
   type A =  
   relation :: 


I am fundmaentally confused on the different between a lambda and a plymorphism quantifier
It feels like they instantiate the same
a forall binding has a /\ in the term
a lambda in the type has no term associated with it until it is applied?


a -> Double
(a -> Double) -> Double covector
(a -> Double) 

type Dual x = x -> Double 
type T = Dual
T (X) = 
T T (a -> Double) ~ a -> Double

a -> Double == \f -> f (\g -> )
to v = \f -> f (\g -> g v)
from w = w 

from (to v) =  from (\f -> f (\g -> g v)) = (\f -> f (\g -> g v)) $ const 1
= const 1 (\g -> g v) = 


WHat is a vector?
It is a column of numbers. In other words an n-tuple of numbers
It is components. It is a function from which component you want to a number
type V = XYZ -> Double

Covectors are linear functionals of vectors. They take a vector and return a number
type CoV = V -> Double
s.t. 
f (v + w) = (f v) + (f w)
This equality is not expressible in Haskell if we use functions to represent our vectors

CoV ~ V
to f = \x -> f 

type CoCoV = CoV -> Double
CoCoV ~ V


There is a very big difference between the push and pull

Level-0 vectors are things like Vector Double, [Double], V3 Double
They can push numbers
Level-1 is (a -> Double) 
They pull indices. These correspond to partially applied indexing functions
from v = \a -> let i = fromEnum a in v !! i
to f   = [f X, f Y, f Z]
Level 2
These correspond to partially applied dot product functions / iteration sweep reducing functions
(a -> DOuble) -> Double
to02 v = \f -> fold (\acc x -> acc + f x) v 
from02 f = 
Level-3 is "naturally" equivalent to Level-1 


Infinite Vectors
[Double]
Level-1
Int ->  Double
They are still interconvertible


vector equality
f ~ g  is not generally possible

Level-2
(Int -> Double) -> Double -- if this returns, it can actually only probe a finite prelude
So in fact this HAS to be a (finitely) sparse vector.
Now a natural thing to do might be to terminate probing based on what we see.
This won't be linear anymore though. There could be a NUTS big coefficient later on
even with some weirdo form of Double based equality

This has a form similar to impossible functionals

(Int -> Double) -> Double equality testing is probably possible

Dynamical systems ands Imperative programs

There is a great deal of similarity between dynamical systems and programs, in their analysis and structure.

check out the work of Andre Platzer.

In a certain sense this is unsurprising? Comuting systems live in the physical world. The operations of an abstract machine are exactly that, abstractions of the movement of electrons in transistor circuits, a chunky discretization of their movement.

In a somewhat generic framework, we can treat them similairly. Transition relations might be a good framework.

There is a notion of time. There may be a somewhat flexible notion of time if we extend ourselves to the relativistic or the asynchronous case. It's interesting that I believe I have heard Einstein's work as a patent clerk reviwing mechanisms for the synchronization of telegraph equipment may have had some influence on his ideas.
Analogies like this are explicit in Lamport's work on vector clocks.
Time can be continuous or discrete, partially or totally ordered.


There is a notion of state. In physics, one is inclined to think about continuous state of momentum and position of all degrees of freedom. In a program, once considers all the. 
We are often highly desirous of limitting the size of our state space for the purpose of analysis. In principle, perhaps very physical system has the state space consisting of the entire universe. Likewise a programs' state space is the entirety of of the computer memeory and register content. Or again perhaps the netire universe.

There are equations of motion. The transition relations

Clean physics systems do not have dissipation and have hamiltonian structure and time independence. But in the more general context of dynamical systems, all these things occur.

time independent equations of motion. Programs are usually not best thought of in this way. If we include a clock or an insturction pointer register, we can recover time independence in a cheating kind of way.

backwards and forward sets.

Of fundamental importance to both is the notion of invariants. Simple invaraints, onece hypothesized, can be checked by seeing if they rae conserved by a single step of the transition relation / checking if their time derivative is 0.
Energy, momentum, spin.
tree balancing, sortedness, positivity

Time can be chunked into periodicity for ocnitnous systems. Sometimes we want to talk about invariant of the cycle.


Symmettry and conservation laws. It is one of the rather spooky principles of modern physics that symmettry and conservation laws are intrinsically linked. The connection of this point to programs is unclear. Programs tend to not be hyper symmetrical? Parallel processing models. I've heard weirdo rumblings connecting polymorphism to noether's theorem I think. What if I could annotate polymorphic variables with group annotations?
(a,a,a) -> a -- full permutation available
(a,b,c) -> g  -- Z3 only?
Or what about my V2 code. I want it to be invariant under SO(2).
Does noether's theorem require a lagrangian formulation? incompressible dynamics?
It's interesting. Symmettry is a good principle. Abstract interpetation via symmettries. Symmettrical sets. Maybe this is a good technique for nuermical programs. Symmettry is one of the standard tests by which we determine something is fucked.

Incompressibility. Linear Types? Reversible computing. non reversible processes as projections of reversible ones.


Lyapunov functions and termination metrics. It is not clear if a system will eventually go to some region or position. Once methodology for proving this is to cook up a lyapunov function, a function that is always decreasing sufficiently and bounded below. Ibce we have this function, we know that the execution must always be.  Similarly, recursion and iteration may not terminate. We need a bounded function that is always decreasing and then this is sufficient to show that they do.


Physics is strongly interested in geometrical space. Programs seem to be much  less so. Perhaps seperation logic and "regions" of memory. Perhaps artificially imposing low dimensional spatial notions upon memory might have benefits? Pure speculation. adress space is 1 dimensions usually.
matrices. For matrices and parallelism, the circle is closed because we are often using those in application to physical problems. Computers are built in phsycial reality, and then they can simulate it. kind of fucky huh?
Multiple variable values or multiple adresses can be
embedded as an n-d space. HOwever, the usually geometrical symmettries seem to not matter so much. Maybe translational symmettry?

Topology in code. ? Topology in physics is a weird subject anyhow. Classical topological invaraints of dislocations. Topological band theory. Homotopy type theory. Topological considerations of types escardo. "Continuity" gets intepretted in different ways.


Consider the code of an ode solver.
From this consideration, the universal program analysis must inlcude as a sepcial case the analysis of physics. Yikes. 
consider the code of a parallel pde solver

hybrid systems and the bouncing ball

finite state machines

induction







doing something with polytops might be cool. Not sure they are applicable though. Ki d of seems like they are

1 - kk/|k|^2 is divergence free projection
kx = np.fftpoints
ky = 
KX, KY = np.meshgrdi
K2 = KX**2 + KY**2
KXnorm = Kx / K2
KXnorm[0,0] = 0




fvx
fvy
1 - 
kdotv = (KX * vx) + (KY * vy)
vx -= (kdotv) * KXnorm #KX /  K2 #divide by zero problem
vy -= (kdotv) * KYnorm



#algebra of programming

# concise ness matters
how many C++ talks have code?
Aaron Hsu

# the point of algebra
rich manipulation to prove and discover new facts

# Combinators

# Fusion

# equational reasoning

# inequational reasoning

# names
Gibbons
Backhouse
Bird


fuzzing and RRT trees.



What are proofs?
Proofs are breaking down of big unconvincing claims into lots of little ones
Proofs are the prcoess/objects that convince people of truth

What is an example of something to be proved?
The pythagorean theorem? Too pure. I'm heavily ready to believe the pythoagroean theorem as a given. Sometimes it is an axiom
(x+y)**3 = x**3 + 3 x y + 3 x * y + y**2
This is not entirely obvious. How do I prove it?
Well, by a sequence of calculation
I know how to foil. 

data = X | Y | Plus | Times | Const deriving Eq

distribute (Time)
commute



localizing into subparts

What about a proof object? Well objects are data types.
I can write a DSL for expressing programs made of the above combinators and then run them through an intepreter.
interp Dist term
interp

When I run this program and it returns the appropriate final result,to my taste it does indeed constitute a proof of the above statement.

Types = Propositions. 
What are the origins of types. First off it is a natural concept. 
Secondly they appear in the history of mathematics and logic
The lambda calculus appears around the same time when people are clarfying the nature of proof and process.

I should fiddle with that two vect stuff


Make proposal about
Topological quantum computing
Classical Simulators



You can stack container like objects. YOu can have a Maybe of List of V2

In Haskell types this is written like this
Maybe ([] (V2 Double))

Graphically we write this as 
 | | |

 We can write functions that change the container type without looking at the stuff inside it
 as boxes.
kill:: Maybe a -> Maybe a
kill _ = Nothing

listify :: Maybe a -> [a]
lisitfy (Just x) = [x]
listyf Nothing = []

concatMaybe :: [Maybe a] -> [a]

SOme special functors
Compose
Id a
Const


Some functions have rich laws anf algebraic sturcture available in them
These functions are often members of typeclasses.

join
return

Vectors

What are vectors?
That which has direction and magntitude.
Stacks of numbers. Graphically these are written like so.

There are a running 
Baby O(1) - 2d 3d 4d vectors appearing in computer graphics, geometry, physics
Medium - O(n^d) - Time series, data sets, electric Fields, velocity fields, 
Mondo Vectors (2^n) - probability distributions, multi-particle quantum wavefunctions

That quantum computers let you perform a kind of lacroscopic manipulation of these ridiculously huge vectors is the origin of it's hypothesized power
A 7 qubit system is significantly easier to simulate than am ordinary fluid simulation

Styles of vectors:
- Unityped Vectors -
- Nat Typed Vectors
- Nominally typed Vectors

In many application, I think it would be unsual to want your vectors labelled only by integers. It is very common have semantically different objects that just so happen to have the same size.

One can, and it is the best approach at the moment, to wrap 

Representation:
Unityped
  []
  Vector
  Repa
  Massiv
Nat Typed
   Vec
Nominally Typed
   V2, V3, V4 and other friend from the 
   (a -> Double)
   [(a,Double)]

The linear monad

Representable functors
Naperian/Logarithmic

The perspective of the indices
Kron = (,)
DSum = Either

The perspective of the Functor type
Kron = Compose
DSum = Product


sequence = swap

String diagrams for Vectors
Every line is another index.
Every Line is a new space.

Linear Functor

scalars <-> 1D vector space V1
Cups
Caps

Adjunction between index and functor representation



do 
  i <- sum
  j <- sum
  (i,j,k) <- sum 
  g i j k
  k <- g i j


relational spec > functional spec.

data Plus a b where
   PZ :: Plus ('Z, a) a
   PS :: Plus (a,b) c -> Plus ('S a, b) c 

We have taken all the pattern matching
Placed it in the right hand side

class CPlus a b c | a b -> c where
  prel :: Plus (a,b) c
instance CPlus a b c => CPlus ('S a) b c where
  prel = PS (prel @a @b @c)
instance CPlus 'Z b c where
  prel = PZ

data And a b where
  TT :: And ('True, 'True) 'True
  FU :: And ('False, a) 'False
  UF :: And (a, 'False) 'False

data Fst a b where
  Prj1 :: Fst '(a, b) a

data Snd a b where
  Prj2 :: Snd '(a, b) b

Fan Prj1 Prj2 ~~ Id

-- polymorphic identity
data Id'' a b where
  Refl :: Id'' a a

-- monomorphic identy
data IdBool a b where
  ReflTrue :: IdBool 'True 'True
  ReflFalse :: IdBool 'False 'False

class Category k, Reflect a => Interp k' k a b a' b' where 
  interp :: k a' b'
  interp :: k a b -> k' a' b'
(or we can look at the structrue of your dervivation)
(make axiomatic insttiantion or striaght up intepretghe structure)

instance Interp (->) Plus where
  interp PZ = \(_,x) -> x
  interp (PS r) = \(Succ x, y) -> Succ $ (interp r) (x,y)


Indeed these are proof relevant relations
And they compose via ProCompose
. But they aren't profunctors

data Fan k k' a b where
  Fan :: k a b -> k' a c -> Fan k k' a '(b,c)
  {-
data Split k k' a b where
  Split :: k a c -> k' b c -> Split k k' ('Either a b) c
data Split k k' a b where
  Left :: k a c -> k' b c -> Split k k' ('Left a) c
  Right :: k a c -> k' b c -> Split k k' ('Right b) c
  -}
data Split k k' a b where
  Left :: k a c -> Split k k' ('Left a) c
  Right :: k' b c -> Split k k' ('Right b) c

newtype Dup a b = Dup (Fan Id' Id' a b)
newtype Par k k' a b 

data Trans k a b where
  Trans :: k '(a,b) c -> Trans k a '(b,c)

-- almost certainly a kan extension. Hmm.
data RDiv g j a b where
  forall k. RSub (ProCompose k g) j
-- equivlaent to Kan extensions form.

-- I kind of wonder if RSub should be in the kind system?
data RSub k k' = RSub {to :: forall a b. k a b -> k' a b}
data REq k k' = REq {to :: RSub k k', from :: RSub 'k k}
data REq' k k' = REq {indirect :: forall k''. RSub (RCompose k k'') (RCompose k' k'')
REq \(RCompose (Fan Prj1 Prj2) k'') -> RCompose Refl k''
REq \(RCompose z k'') -> case z of
                         Left Inj1 _ -> Refl
                         Right _ Inj2 -> Refl

Every defining case of a function becomes a right hand side.
Every subfunction call is a held relation
This includes recursive calls.

Constructors are functions
Patternmatching is

data Fix a b where
    Fix :: Fix f a ('Fix a)   ---  (f ('Fix f)) ('Fix f)
data UnFix a b where
    UnFix :: UnFix ('Fix a) a
data FMapMaybe k a b where
   FMapJust :: k a b -> FMapMaybe k ('Just a) ('Just b) 
   FMapNothing :: FMapMaybe k 'Nothing 'Nothing

instance Functor ('Nothing) ('Nothing)
  type FMap = FMapMaybe
  fmapcase = FMapNothing




These two definitions are the same. Funky huh

data Left a b where
   Left :: Left a ('Either a b)
data Inj1 a b where
   Inj1 :: Inj1 a ('Left a)
data Inj2 a b where
   Inj2 :: Inj2 a ('Right a)

proof1 :: REq (RCompose Inj1 (Split f g)) f

Allowing in Profnctor: these are going to be singletonized functions we're dimapping over. since our type variables are always. We have no way of doing that to the relations as we've been defining them. We could wrap them in ProYoneda, which gives us the free profunctor instance. We could do it by AXIOM, making the implentation of profunctor = undefiined. That will allow us to manipulate the side va functions.

data RConverse




data Right a b where
  Right :: Right b ('Either a b)



data OR

Typeclass based derivations are somewhat rigid. They don't allow for real search. They don't allow for failure. I kind of wish I could put it in a monad.

data Minus a b c where
  Minus :: Plus (b,c) a -> Minus (a,b) c

-- is function (exists f, Plus (a,b) (f a b))



Meet
Join
Shrink

Kmett-iverse


data Cons a b where
   Cons :: '(x, xs)  (x ': xs)

data Nil a b where
   Nil :: ()  '[]

-- these aren't right. They should be datakinds in the fields
data Cata k a b where
   Cata :: k (f a) a   -> Cata k (Fix f) a

data FMap k a b where
  FMap :: k a b -> FMap k (f a) (f b)

data DiMap k k' a b where
  DiMap :: k a' a -> k' b b' -> DiMap k k' (p a b) (p a' b')
I use backticked tuples for tuples actually existing in the relations
and reuglar tuples for holding arity of relations


Functor f where
   fmap :: k a b -> k (f a) (f b)
   Compose k k' -> FMap k . FMap k' a b

type family FMap k = 

Nothnig
MapMaybe where
    -> 'Just a 'Just b 
    -> 'Nothing 'NOthing




type a |- b = Seq a b

I don't even particular want a list of things.
Could I use sandy maguire type sets for the environment?

data Sequent a b where
  Refl :: Seq a a
  Conj :: Seq '[a,b] c -> Seq '[a] (b -> c)
  Weaken :: Seq '[a] '[b] -> Seq '[a,a'] '[b]
  Comm :: Seq '[a, b] c -> Seq '[b, a] c
  ConjI :: Seq '[a,b] c -> Seq (a,b) c
  ConjE :: Seq (a,b) c -> Seq '[a,b] c

newtype Top a b = Top ()
newtype Id a b = Id (Eq a b)
newtype Bottom a b = Bottom Void







How do we lift typeclass requirements? We can desugar them
Functor f =>  
{fmap :: (a -> b) -> (f a -> f b)}

cata :: (Functor f) => (f a -> a) -> (Fix f -> a)

Cata (fmap :: k :: f (* -> *) -> (a -> b -> *) -> k a' b' -> k )

data 

Cata FMapMaybe k


data FMapMaybe k a b



depth limitted recursion scheme

cata :: Int -> Rel (f a) a -> Rel (Fix f) a
cata 0 = bottom
cata n = cata (n - 1)
 


 or a depth limitting function of some kind
cata :: (Fix f -> Bool)

or we have to count on laziness somehow
fincata n = take n (cata rel)

rdiv probably goes to another area of the arithmetical hierarchy.

The arithmetic hierarchy and relational algebra. The airthmetic hierarchy plays with quantifiers. Relational algebra hides quanitifiers inside the operators.
The polynomial hierarchy appears to exist





A theory feels less like bullshit to me if I can represent it in a computer in a way that doesn't feel entirely syntactic. This is a subjective notion. This is roughly model thoery I think.


algebrauc effects

elgebraic theories / universa algebra

example
group.
(G, dot , invterse, )

operations + equations

good to avoid quantifiers.
for example quantifying out the inverse

signature = (op_i, n_i)
set of operations. op are operation symbols
n is natural number that is arity

data Sig a = [(a, Int)]

A term in context x1 .. xn
 - on of the variabes
 - or op(t1,t2, ..) t are terms


data Term a = Var Int | Op a [Term] 

type Sig = f

-- pretty close to free monad
data Term f = Var Int | Term (f (Term f) )

Theory is (equational theory)
T is given by (Sig, Eq)

Equation = context, two terms
data Eqaution f = Eq {cntxt :: Int, Term f, Term f} 


exmaples
ring
empty theory
theoyr of pointed set
field - problematic. inverse of 0 not defined
semilattice

field not algerbaic in usual formulation

interpetations nad models
T theory. Interpetation I  of signature is given by
- carrier set |I| 
- (op, n) in T, [| op |] : |I| x |I| ... n times -> |I|

implciit coercion in coq?

Each term x1 .. xk | t is interpetted as a map

[|  x1,x2, ...  | t  |] => |I|^k --> |I|




allInt = [0 .. ]

mu :: (Int -> Bool) -> Int
mu f = head [ j | j <- [0 ..] , f j]



Harper - foundations of programming langes

what is existance for programmng lang
statics - what are the programs
dynamics - how run

concrete syntax - linear rep

abstract syntax
conrext sensitive conitions

godel T

anstraxt binding trees
tree + binding and scope

types tau . nat, arrow
unary, 

alpha equivalence
substitution - avoiding capture

context sensitive formation

typing judgements
tau is type
e is tau

compositionality
formal = typing is inductively defined

formal means of form. syntactic

context matters

hypothetical judgement
turnstile

structural type theory
reflexivity, composition

strongest relation closed under rules
turnstile is a relation between syntax

dynamics
states
initial
final

nats are obsercalbe. function extensionality
huh. F doesn't have this.

structural operational semantics
eager lazy
call value call name

dynamics and statics cohere - safety

Higher oder relations

Compose a b where
   (forall a b. k b c -> k' a b -> k'' a c) -> Compose (k,k') k''

RCompose is very constructive
but, let's say we 

data Succ a b
   Succ :: Succ x ('S x)
data Succ2 a b where
   Succ2 :: Succ x ('S ('S x))

Compose '( 'Succ, 'Succ) 'Succ2
data Compose 
  Compose :: Equiv (RCompose Succ Succ) Succ2 -> Compose '( k, k' ) k''

succ2 = succ . succ
vs 
succ2 x = (S (S x))

newtype Succ2 = Succ2 (RCompose Succ Succ) is the first style
data Succ2 is the second

Higher order relations
FMap :: (a -> b) -> (f a -> f b)


MapMaybe 
  :: MapMaybe k 'Nothing 'Nothing
  :: k a b -> MaybeMaybe k ('Just a) ('Just b)

class Functor Maybe where
  type Lift a = a'
  fmap :: k a b -> k' (Lift a) (Lift b)
  type FMap k k'

  type R
  fmap :: k a b -> FMap k R
  type FMap k = k'

Equiv (k <<< k')




The other idea was to use all newtypes. And store the actual implemnting fucntions in.
Buttttt. 
newtype Id a b = Id () 


FMap k k'
  FMap (RCompose k k')


dedekind cut
Ratio Int -> Bool  (with guarantee of monotonicy)

the bisection method is somehow the dual of interval airthmetic?

compare d1 d2 = findwitness

(Ratio -> Bool) -> Bool
dedekind completeness - iterating construction doesn't chnaged anything

bisection method 
Dedekind -> [Ratio, OverUnder]
[Ratio] -> Dedkind?

add :: DD -> DD -> DD
sqrt n =  


marshal - very interesting
Limitting themselves to ertain kinds of functions

monad under union?

what about convex sets by construction?

topology of computability
topology of complexity?
arithmetic hiearchy <-> polynomial hierarchy

He is discussing defunctionalization? categorically.
Or closures.
Presheves give free exponential objects

 


PArametric polymorphism
make polymorphic code perfomrany
kind directed compilation
polymorphic recursion
boxing - ocaml ints are the same as boxes
monomorphization - Rust C++, no fancy types, MLTon.
run-time specizilation - C#. TIL for ML
"Kinds are calling conventions"

subkinding of openkind

never store levity polymorphic value
no levity polymorphic variables
no levity polymorphic passed into a funtion


Fix in boerhm bernaducci encoding roughly is fix
fix :: (a -> a) -> a
Fix :: f (Fix f) -> Fix f 
pattern matching
(Fix f ) g = g f 
(f (Fix f) -> r) -> r
(f r -> r) -> r
maybe fix isn't encodable?
forall r. (f r -> r) -> r
This in't right
f Nothing = ?

f r -> r is a little like algerba also
(f a -> a) -> Fix f -> a

recurisive positions take the same type as
(a -> a) -> a -> a = Nat
= Fix Maybe
  , ())


what are some reasons to fator out recursion. 1. Memoization
Share

Top-down vs
bottom-up recursion


Reverse topological storage of BDD
Can just go through that bish.
 [  Int]
(differential position?)
Array (f Int)
IntMap ?


unification
interleaving unification.
What is the function form of this?


morton ordering python

d.reshape((4,-1))

[( , )]

islate corner first
# simple but lots o churn. Maybe a good place to start
def morton(x):
  if x.shape = (1,1):
    return x
  ul = morton(x[ :N/2, :N/2 ])
  ur  = morton(x[ N/2:, :N/2])
  dl = morton(x[ :N/2, N/2:])
  dr = morton(x[ N/2:, N/2:])
  np.concatenate(a,b,c,d)

#flip flopping temp arrays
for i in range(log2N)
temp[:N*N/4] = t[ :N/2, :N/2 ]
temp[]
temp.reshape((-1,N/2,N/2))

Does this work for not divisible by 2?
Does this work for rectabgular?

type fxy = Compose fx fy
fx = V2 V2 V2 V2 
fy = V2 V2 V2 V2

morton ordering = peel off outer V2 -> merge them into V4, recurse

Compose (Compose f) | fmap unCompose f -> f' = sequenceA 

Sorting network, kind of. Sorting network has 1 wire per element. Decomposition of container into sum.
V' = V :+: V

transpose is sequencA

linear maps 
V2 a -> V2 a
V2' f a -> V2' f a --  V2' = Compose V2. Makes sense what a pattern. Another cayley thingo
a slot for both the underlying field and the next container
V2 f a -> V2 f a

Linear Functor V2 f a


sigmax = forall f a. V2 f a -> V2 f a
(Compose (V2 x y)) =  V2 (x ^+^ y )
We could have arranged our hierarchy differently. f could probably implement only the vector type. Then there is nothing bad you can do.

V2 a -> V2 a only works if we implement num for (f a)

tensorial manipular at the type level.


prime factorization of n

V n = ? -- kind of dumb I guess. Array is better. Very similar to what Conal elliott was up to.




type familt PrimeFact n
   PrimeFact = PrimeFact n 2

type family PrimeFact' n
   PrimeFact' n n =
   PrimeFact' n x = ITE (Div n m)  ( x ': (PrimeFact (n Div x)) (PrimeFact' x (m+1))



type family GetV n
   GetV 2 = V2
   GetV 3 = V3
   GetV 4 = V4 -- Well this one won't happen. Ok it might
   GetV 5 = V2 :+: V3
   GetV 7 = V2 :+: V5
   GetV 11 = (GetV 4) :+: (GetV 7)







def demorton(x):


what was I morton ordering before?


band theory

2 hilb?

You can easily add elements to a relation using either the Join combinator, or by defining a new data type that holds the original relation as one of it's cases.
data NewRel a b where
    NewCase :: NewRel 'Fred 'Larry
    OldCase :: OldRel a b -> NewRel a b

Checking if elements are in the relation is far more difficult in general. This is a gift and a curse. The list form couldn't be used describe elations wihout an effective method for producing their elements. I think the type form of relations is rich enough to describe unsolved mathemtical conjectures for example, you just won't be able to find the value that inhabits the type. At the very least, if the relation exists over domain and codomain that are finite, I'm pretty sure I could write some typeclass tomfoolery to inhabit the relations automatically. Then the data type definition of the relation would have one constructor for every element that would have gone into your [(a,b)]. Hadn't really thoguht about that analogy before.

data ListRel xs a b where
    Here :: ( '( a,  b) ': xs) a b
    There :: ListRel xs a b -> ListRel (x ': xs) a b

class ListMem xs a b where
  find = There find

class ListMem 'True ( (a, b) : xs) a b where
  find = Here

instance Find '[] a b where
  find = (Right \r -> case r of _ -> impossible 


The list structure is great as, combined with Eq, you can both produce and consume elements of the relation. It is not good in the sense it is inefficient. Sweeping through a list to find things is the dumbest thing you can do.
Upon combination. these list relations get exponentially large.
The problem here is the same as a database query optimization problem. The best way to query a database depends a great deal on what kind of indexes you have available. Convertings between different data structures for relations is possible, as is reindexing a database, but it costs times and memory.
There are all sorts of ways to combat this. One, very tempting to me,  is to make a DSL to describe the relation algebra and the perform rewrite rules to put it into a more efficient form.
I became overwhelmed by these considerations and stuck to lists for the purpose of the blog post, which I think was the right call. Still chewing on it.


I think I read them in tandem. I find it to be a useful technique to get multiple books on the same subject. 
1. By "diffing" them you see what is important and what was author's personal taste.
2. Plain Dumb repetition is 60% of learning. Rereading the same material in slightly different wording is less boring that rereading the same book.

Having said that, I think the JN Oliveira book is a jewel of pedagogy and the AoP book is tougher going due to it's categorical bent. I think the relation stuff (Part II) is easier than Part I, so I'd recommend starting there.


We just don't really have higher order functions

pointfree
pointful


tensors

tens i j k = 

classical information as a two Category
trace of density matrices

maybe I should just use idris

no but I can't case on the type. But I can do dependent types
V a b c

dats Fib = Tau | Id
v Tau Tau Tau = Bool
v Tau Tau Id = ()
v Tau Id Id = Void

v2 a b c d = sum \i -> (v a b i) * (v c d i)
(*) = Tup


sum :: (Fib -> MyIndex) -> MyIndex
sum f = Sum (f Id) (f Bool)
-- no higher order functions though at the type level

space = v2 Tau Tau Tau Tau 

instead point free form. kind o burdensome

compose v (trans v)

sparse pseudo inderse.
(A x - y) + |x|^2 
tikhonov regulation
(Ax - y)^2 + |x|
a different regularization -> There is no reason to make entires nonzero except those in
this is also the compressed sensing form.... 

ugh.



finlly tagless
class Num rep => Tensor rep where
   sum :: (i -> rep) -> rep 




data MyIndex = Bool | Unit | Void | Tup MyIndex MyIndex | Sum MyIndex MyIndex



compose with g on the outisde is another alternative.
 
newtype V2' g a = V2'' Compose V2 g a
newtype V2'' g a = V2'' Compose g V2 a

newtype Compose2 v w f = Compose2 (v (w f) a)
Compose (v f) (w f) a
Product (v f a, w f a) -- this is still "linear" in f

-- new lifted identity
newtype Identity2 f a = Identity2 (f a)
type Kron' = Compose2


V2' g -> V2'' g 
= sequencA

Absprtion of unit
Kron' f f ~> Identity 

These are single dimensional V2' is a single dimensional thing

V  (f '; fs) 

newtype V '[f, g, h] a = (V2' f) :+: (V3' g) :+: (V2' g)   
The linear algebra of types. We've discussed before how algebraic data types are very much like numbers in that they have natural meanings to mutiplication and addition. 
An important tool that you leanr in school is that of matrices, and matrix vector multipcaiton. Row times column. We need only a summation and a multiplication operation in order to define this. 
What the hell would you want this for?
Well not much.
But consider a transition system where the state space is growing. You could encode this using such a formalism

Translating this linear algerba of types considered as the indices of representable functors is rather mind bending, but doable.

We can also consider the tensor algebra of types. 






Min-Plus types
type Family Min a b where
   Min Void _ = Void
   Min _ Void = Void
   Min () _  = ()
   Min _ ()  = ()
   Min Bool Bool = Bool
   Min (Either a b) (Either c d) = 
fairly goofy

   the one form
V f g k = V2' f) :+: (v3' g) :+:


one form
V2 a -> a
one form exapnded
a -> a -> a

v i1 i2 = 3 * i1 + 2 * i2


Dual v a = v a -> a
type V2 a = a -> a -> a

V2 ~ forall r. (a -> a -> r) -> r

linear types -> enforce numerical linarity at type level?


proof by structual infuction
logical relations
A day in the life of of a pl researcher
1. try to prove something
2



termination
e  : tau
a value v such that e |-> v
if e types
every prgoram has a value

godel's T

proof by structural induction
by inductio on typing
typing derivation

num[k] |-0> num[k]



:+: = Product
 V3 f j k (~> a) V4' 

-- this has implied summation over all particles
newtype V ttt iii tti itt tit
-- could include all indices, even ones that don't exist.

-- partial summations
newtype VI tt ii  =   V V0 ii V0 tt V0
newtype VT ti it tt


newtype VI tt ii = (tt :+: ii) a -- with vertex factors if we had them
newtype VI tt ii it ti = (V1 :*: tt) :+: (V1 :*: ii) :+: (V0 :*: it) ()

-- VT and VI give a way to have a particle hanging off the end.

newtype VIT t
newtype VTI t
newtype VII i 
newtype VTT t i a = VTT ((t :+: i) a)

-- I guess these are leaves?
-- In some sense these things are a family
newtype VTTT a = VTTT (V1 a)
newtype VIII a
newtype VTTI a
newtype VITT a
newtype VTIT a


newtype TLeaf a = Tleaf (V1 a)
newtype ILeaf


VT
VI
TLeaf
ILeaf

VTT VTTT VITT --  (tau  (tau,tau)) with tau at root space ==> 2-d vector space
Yes. We have matching conditions at interior edges. and exterior edges. Not great.
VTT fixes the root and the first left branch as tau

If we had machinery to construct these things, that would help

type family FibV root leaves where
    V 'Tau '( 'Tau, 'Tau) = VTTT
    V 'Tau '( 'Tau , y  ) = VTT (V 'Tau y)
    V 'Tau '( (x , y) , ( z , w)  ) = VT (V 'Tau 'Id)

    V 'Tau 'Tau = TLeaf
    V 'Tau 'Id = V0
    V 'Id 'Id = ILeaf
    V 'Tau (a,b) = VT ((V 'Tau a) :*: (V 'Tau b))  (((V 'Tau a) :*: (V 'Id b)))  (((V 'Id a) :*: (V 'Tau b))) 

    -- VT is a kind of summation symbol



    V r '(x, y) = 


class FMove root where
instance FMove root where
  braid :: V Tau (a, (b, c)) -> V Tau ((a,b), c)
  braid (VT )



class Braiding a b c where 
  braid :: V c (a,b) -> V c (b,a)

instance braiding Tau Tau Tau
  braid 

Tensor


V f a -> V a


f g h are elements of these 2Vects
morphisms take guys to other guys
morphisms are not askell functions. They are newtype defintions
-- Something like this
newtype Morph v f g h = v (V3 :*: f :+: V2 :*: g) (V3 :*: f :+: V2 :*: g)
Morph :: (a -> b -> *) -> (x -> y -> z -> *)
where a b x y z are all actually * -> *

And then 2-morphisms
Morph1 v ~> Morph2 v


-- kind defintion
type Vect = * -> *
type TwoV2 = Vect -> Vect -> Vect -- like Double -> Double -> Double dual vector formaulio


with linear/affine types, if we assume the helf quantities are vector spaces and not scalars, we do get linearity?

v :: Additive f => (f a) -o (f a) -o (f a)
v x y = 1.0 ^* x 

with mere additvity (and superclass Functor), we have no way to multiply two vectors together.

If we had
v :: Num a => a -o a -o a
v x y = x * y


type family V
   V Tau Tau Tau =  V1
   V Tau Id Tau =  V1
   V Tau Id Id =  V0

V3'' '[f, g, h] = 

type V = [[[V0, V1], []] , [[],[]], [[],[]] -- a three dimensional tensor filled with vectors spaces

a labelled space

newtype VT Tau tti 

newtype V Tau a

type Kron v1 v2 
  Tensor (g Tau) (f Tau) = V 


type W f g h a -- is one form of 2-Vect
newype M (w :: V -> V -> V -> V) f g -- is morphism of 2 vector. maps 3V to a 2V in some way.
M -> M'  -- is 2-morphism. forall w or for particlar w?


   Tensor 'Tau (VT ) (VI ) = VT V0 V0 (VT :*: VI)

   tensor @Tau (VT x) (VI y) = VT zero zero (kron x y)  

It's difficult to order a loopy boy (topological ordering?)
But a tree network can easily be ordered. the parent is repsobisible for the summation

FOr the List-like Basis, We can keep the 

use
VTI i t 
Since outer tau will be fixed

_|_
Matrices and tensors can be overload

'[Tau, Id, ]


tyep family Dot
   Dot '[] '[] = 

type family Dot3
   Dot v g = (v V1 V0 V0) :*: g V1 V0 V0 


  newtype App f a = App (f a)
  newtype Lam f
  newtype Var x


newtype Let l x e


Sum (V a b c)

data Sum f a where
   (f Tau :+: g Id) a -> Sum f a
 
Yeah. Summation over the type level indices seems quite similar to
my problem. with the System-F 

Rather than using HOAS, use first order form
Sum "x" (f "x") 
-- I think we're okayish with this
type family Sum where
   Sum i b = (Replace i Tau b) :+:  (Replace i Id b)
-- outer or inner evalations first? Matters for index scoping

type Sum f = (f Tau) :+: (f Id)

-- You have to define a bunch of intemediate newtypes which is a bummer
-- It also seems... wrong. 
  V a b c :*: V a b c

type family Sum where

by and large, it is preferable to avoid type families
also type synonyms
They are both somewhat inflexible in surprising ways

Try to use newtypes as much as possible


Vect -> Vect

2vect 

It's actually going down
Objects = N copy of Vect ==> (V2 Vect) -- kind?
morph = Linear Functors === > 'V2 ('V3 V0 V2) ( - -) -- Type. With specific entires
2-morph = morph between linear functors
type Morph2 a = forall f g h. Apply m ('V3 f g h) a -> Apply m' ('V3 f g h) a  -- value ?
-- actual numericla mappings between the spaces held in the typelevel matrices.



for many scientific applications
crashing isn't the primary concern really

Integer indexing. programs for computers vs humans 


:::

Untyped Vectors
  + `[a]`
  + `Vector a`
  + HMatrix - Closest scipy equivalent
  + Massiv - 
  + Repa -
  + Accelerate - A DSL that compiles via LLVM to GPU or CPU
Typed Spaces - Vector space shape described by type
  + Nat Typed
     - `Vec 7 a`
     - Justin Le
  + Index Typed   
    `b -> r`
    `Map b r`
    `[(b,r)]`
    Conal Elliott's thing?
  + Opaque Functor 
     - Kmett's Linear
    `data V4 a = V4 a a a a`




We can include a specific index type to the vector. Sometimes this is helpful.
Other times it is useful to think of the Vector as a shape/container (* -> *) examples include

We can convert from the index form by prtial application on an index type.
  - V2
  - V3
  - (b ->)

For any of the above, there is also a natural Dual representation.
In linear algebra it is common to talk about the dual vector space. It is surprisingly common for constructions to be more elegant and straightforward by using the DUal space.
type Dual v a = v a -> a

type DV2 a = a -> a -> a


There are a couple possiblitilies for descrbing vectorial operations. We are in Haskell ,so we want to describe a generic interface using Typeclasses.

Vectors have th operations of vector addition, and scalar multiplication in combination with distributive laws about them.

In the following, we have an opaque object that we say is in a relationship 
class Vector v r where
   smul :: v -> r -> r
   vplus :: 


class Vector f where
   smul :: Num s => s -> f s -> f s
   vplus :: Num s => f s -> f s -> f s
Not much point to this. The types don't protext us from implementing an incorrect smul or vplus anyhow.

class Functor f => Additive f where



The standard typeclass hierarchy in Haskell let us dervie generic vector operations.
 - Functor -> Scalar multiplication
 = Applicative -> Vector Addition
 - Traversable -> Dot Product



instance Additive f => Vector (f a) a


If we consider the dual of a specifically sized type we can uncurry it.
`V2 a -> a `is isomorphic to  `a -> a -> a`
to f x y = f (V2 x y)
from f (V2 x y) = f x y


Constructing Vector spaces from Vectors spaces.
Kronecker product and Direct Sum. Naperian 



  - Safe interface style

Boolean blindness. It can be considered poor form to implement everything that has two possible values as booleans. One error you are opening yourself up to is possible misplacing two flags for example. You're also not taking full advantage of the types to express intent. A Bool parameter in the type sginature is much less clear than a more specific enumerated value.
Nat typed vector spaces
Only labelling vector spaces by an integer dimensions is lacking for similar reasons. YOu may misplace equally sized vectors that have vastly different meanings. And you are not properly expressing meaning and intent.

A vector space is often not naturally thought of inductively. It is best thought of algebraically. It is the difference between exrpessing yourslf using Unary numbers (peano) and 

In principle, the compiler has a lot of information that it could use to do very performant things and automatic parralelization.
data BV n a where
  Nil :: BV 0 a
  VOne :: a -> V2 (BV n a) -> BV (2 * n + 1) a
  VZero :: V2 (BV n a) -> BV (2 * n) a

V4 could plausibly use AVX instructions

V4 is the equivalent of the following C struct
struct { double x, souble, y, double z, double w} V4
C has poor composability. One could approach this using macros.

Connection between - Representable Functor. Partially applying an index.

The advtanges of newtypes. Newtypes are wrappers that should incur no runtime cost.
type synonyms are more lightweight, being non existent basically.


newtype X a = X (V4 a)
newtype Y a = Y (V3 a)

type Field = Kron X Y

The algebraic composition of vectors spaces

Chaos reigns David picture
Similarly, using a generic vector type with no typelevel indication of size or intent is madness

Polynomials can also be conceived as vector spaces.



Dual f a = (f a -> a)
(Dual f) :*: f ~> V1

V1 ~ Vect

type Dot f g = (f 0) * (g 0) + (f 1) * (g 1) 
type Dot v3 w3 x y z = (v3 x 0 0) * (w3 x 0 0) +

type Dot3
class TMetric f where -- kind class
    type Dot
instance TMetric (* -> * -> * -> *) where
   type Dot v3 w3 x y z = (v3 x 0 0) * (w3 x 0 0) +
instance TMetric V3 where
   type Dot ('V3 f g h) ('V3 k l m) =  

type family Dot



(f -> Dual f)  

Adjoint
Adjoint f = Ran Id f
Ran
forall b (a -> f b) -> g b


Hom(Vect,Vect) a = f a -> g a
type Hom f g a = f a -> g a
Hom (V3 -) (V3 -) = 

type Hom (f :: Vect -> Vect) (g :: Vect -> Vect)



Why not just use
type Vect = *
The index type reprsents the shape just as much as the container type * -> * .
And it is simpler

type M23 = V3 * -> V4 * 
V4 (V3 *) -- 4 x 3 matrix of types analog of [ [ * ] ]
'V4 ('V3 ) ('V3 ) ('V3 )

2 morphisms are the thigns with values. (actual numbers in them)
You can't inhabit M23 because it isn't *.

data Ex2morph :: M23 -> M34 -> * -> * where
     -> Ex2Morph ('V3 ('V2 a b) ( c d)) ()


 is then a 1 morph



type Morph2 = 

intensional / extensional

Hi, author of said post here. I'm not an expert but maybe I can help.

The function to set definition I'm going to intepret as using the `a -> [b]` form to reprsent a relation `R a b`.
It was a constant struggle during writing that post to try and require as little power as possible. I found an acceptable balance for explanatory purposes with requiring Eq, Bounded, and Enum basically willy nilly.
The beauty of `a -> [b]` form is that relational composition can be performed with no requirements on `a`. `b` or `c`. 
rcompose :: (b -> [c]) -> (a -> [b]) -> (a -> [c])
I'm interpretting the question as whether relational division can be performed without either the Eq, Bounded, or Enum typeclass.
I believe the answer is no as I have framed it.
To start off, you want a function of this signature:
rdiv :: (a -> [c]) -> (b -> [c]) -> (a -> [b])
This is basically impossible for the following informal reason. We are given a function that takes an `a` and produces `c`s and a function that takes a `b` and produces `c`s and we need to produce a function that takes `a` and produces `b`s. We just don't have anything to work with that produces b, so we can't do it. 
Formally I believe this can be proved as a free theorem, although I do not worked the details.
Now, perhaps we can require some kind of producer of `b` to get us started
rdiv :: (BEnum b) => (a -> [c]) -> (b -> [c]) -> (a -> [b])
rdiv g j = ?

What can we make?
Well, one thing we could do is just return enumAll :: [b]. This is clearly not what we wanted for relational division.
Instead we can use our premises to generate lists of cs.

cs = j enumAll
rdiv g j = \a -> let cs' = g a
We still need something to do with these two different sources of cs.

rdiv g j = \a -> let cs = g a in 
                 [b | b <- enumAll,  all (\c -> c `elem cs) (j b)]

So it seems to me that the type needs to be                 
(Eq c, BEnum b) =>  (a -> [c]) -> (b -> [c]) -> (a -> [b])


BEnum b) :: (a -> c -> Bool) -> (b -> [c]) -> (a -> [b])

(a -> c -> Bool) -> [(b,c)] -> a -> [b]
rdiv g j = \a -> [ b | (b,c) <- j, all (\c -> g (a,c)) (rightSet j)]



Basically, the Haskell arrow (->) is not constructively invertible wihout additional powers or assumptions.

If you do however want a constructive version of rdiv with no extra assumptions, you can cheat your way into it. You can fold in the (==) function into your reprsentation of set.
Another valid set representation is an indicator function.
A different question is whether there is a symmettrical Relation representation that is rdivable
rdiv :: Rel a c -> Rel b c -> Rel a b


[(a,c)] -> (c -> [b]) -> 

rdiv :: (a -> [c]) -> ([c] -> [b]) -> (a -> [b])
rdiv g j = \a -> let cs = g a

While what I've stated isn't particularly formal, I'd be interested to hear if there is a hole in the reasoning.



Free division


(forall a. Rel a b -> Rel a c)
we can turn this type into Rel b c via applying to rId. (unifying a with b)
We can turn this 
Compositon with Id, gives back same thing. 

partial application of an rdiv will produce this type.
rdiv g
or 
r = (\g -> rdiv g j)
<->
j = r rId 


("codensity" relation reprentation)
r = rdiv g
g = r rId

rId = pure
rdiv = 

-- reconstitute, compose, 
(g / -) . (k / _ ) = (g . k) / -

informally, CPS/Kan/Codensity is more efficient. 

Hmm.
a -> [b] is obviously a profunctor
partially appliced composition gives
(Rel a b -> Rel a c)

partially applied division gives something similar.
forall b. BEnum b => (Rel b c -> Rel a b) 

Hmm. Same thing as a partially applied composition type wise.
forall a. (Rel a c -> Rel a b)
Yoneda f can be viewed as the right Kan extension of f along the Identity functor.

a -> [b]

Codenisty in profunctor actuall looks like the right type kind of?

X . J <= G
X <= G / J
 X . Id <= G  <---> G / Id == G

G / -  ---> G = G / Id
(- / - ) G --> G /

rdiv g <=> g


a -> [b]
b -> [c]

a -> [b]

b -> [c]
a -> [c]


b is not prduced. Big problem.



enumAll :: b

enumAll  -> [(b,c)]
can resontitue [(b,c)] form

can we avoid comparison of c? I don't think so.


\a -> 

type Rel a c = forall b. (b -> [c]) -> (a -> [b])
Rel a 

g . (k / j) = (g . k) / j

x <= k / j
x . j <= k

g . x . j <= g . k

y <= (g . k) / j
y . j <= g . k

suppose g . x <= y

(g . k / j) <= g . (k / j)

The other direction doesn't go through?
suppose 




I have a shit fuck of representations
1. turn funcitons into type families
2. turn functions into GADTs
3. typeclasses?


type T = 'V2 ('V2 Home Drive) ('V2 Drive Work)
or
data T :: Nat -> Nat -> * where
  T00 :: Home -> T 0 0
  T01 :: Drive -> T 0 1
  T10 :: Drive -> T 1 0
  T11 :: Work -> T 1 1

Not auto coercible :(

The dual version 

type family T i j where
  T 0 0 = Home
  T

Fine and all, but not higher order usable (yet).


newtype MMul t1 t2 i j = MMul ((t1 i 0) * (t2 0 j) + (t1 i 0) * (t2 0 j) + (t1 i 1) * (t2 i j)  )

newtype MV t v j = MV (t j 0) * (v 0) +  -- and so on


type Sum2 :: TNum a => (Nat -> a) -> a
type Sum2 v = (v 0)  + (v 1)

Tensor expressions. How do we even deal with them in the value level?
f i j k = yada yada yada


Making (->) a number is CLASSIC baby

instance TNum a => TNum ((->) a) where
   type (+) f g i = (f i) + (g i)

We can also overload *
t1 * t2 =  ((t1 i 0) * (t2 0 j) + (t1 i 0) * (t2 0 j) + (t1 i 1) * (t2 i j)  )


Value level tensors.


hetergenous product.

het sum? Really we need ignore lifting.

class Lift a b c
   lift f g = \c -> lift (f c) g

class Tensor a b c where
   prod :: a -> b -> c

instances Tensor Double b c => Tensor Double (a -> b) (a -> c) where
   prod x y j = prod x (y j)

instance Tensor Double DOuble Double where
  prod x y = x * y

instances Tensor b c d => Tensor (a -> b) c (a -> d) where
   prod x y j = prod x (y j)


produces new tensor that takes in the appropirate stuff.

fusing indices can be achieved via just defining new lambdas.
\i j k l m -> (t i j) * (k ) 

maybe you don't even want prod. Ehhh. It's not the worst
we want combinators.
* is regular elementwise
prod is an outer product.

-- fuses the next two parameters.
fuse t = \i -> t i i
sum t = t 0 + t 1
swap :: (i -> j -> a) -> (j -> i -> a)
swap t = \i j -> j i
dot = sum . fuse 
(fmap . fmap . fmap) swap

sum \i -> (v i j k l) * (g i k) * () 


-- hmm. unofrutnately this isn't the same.
-- newtype has output *, which cuts off the possiblity of further indices.

type Swap :: (i -> j -> *) -> (j -> i -> *)
newtype Swap t i j = Swap (t j i)

newtype Q i j = Q (Sum (R i j)


t i j k = sum \w -> (g i j w) * (r k l) * (delta i j)


-- kroncker delta is equslity types. Interesting, yet unsurpsiing
data Delta i j where
   Refl :: Delta i i 

type Sum k = (k 0) + (k 1)
You need to use intermediate newtype wrappers for Sum, which is annoying.
newtype E1 i j k = (Delta i j) * (G i j k)

-- this is fine. Unless you want to use swap higher order.
type Swap t i j = t j i

newtype Lam i k = Lam (k i)

Why is this no good?


-- the dual space does better usually.
DualT :: (i -> j -> k -> l -> *)-> *

type DSwap t t' = t j i
newtype DSwap t = DSwap (t )

all free indices have to be bound too.

\i j -> sum (\k -> (t j k) * (g k l m) 

prod may be convenient.

TFunctor fa where
 type FMap f fa :: fb 



sum :: Num a => (Bool -> a) -> a
sum f = (f True) + (f False)

sum :: (BEnum b, Num a) => (b -> a)  -> a 
sum = sum (map f enumAll)



Proability of paths in finite state machines is a place where we have a fairly naturla instance of 2-vect?
We at least need to build a larger and larger vector space
Do we have any 2 morphims
The transition matrix ought to be a 2-mrphism
a -> T a - 1 morphism. Transport linear maps on fixed path space... ?
What are those maps?


Not obvious it is really though.

abstraction - less degrees of freedom
the density matrix map, removing degrees of freedom
bisimulation?

Path to abtsract path.


2 vect
objects are n copies of Vects
morph = linear functors. representble by matrices



exv2 :: a -> a -> a
exv2 x y = 2 * x + 3 * y
(a -> a -> a) -> a
exdv2 f = f 2 3
((a -> a -> a) -> a) -> a
exddv2 f = f $ \x y -> 2 * x + 3 * y

v_to_ddv v = \f -> f v
v_to_dv v = \f -> f (v 1 0) (v 0 1)
dv_to_ddv q = \f -> (f \x y -> x)  
ddv_to_v f = \x y ->  


xhat = \x y -> x
yhat = \x y -> y

dxhat f = f 1 0
dyhat f = f 0 1


dv_to_v q = \x y -> (q xhat) * x + (q yhat) * y

dv_to_ddv q = \f -> dot f q -- dot q
dv_to_ddv = dot_dv

dot_v v v' = (v 1 0) * (v' 1 0) + (v 0 1) * (v' 0 1)
dot_v v v' = (dxhat v) * (dxhat v') + (dyhat v) * (dyhat v')
dot_dv q q' = (q xhat)  * (q' xhat) + (q yhat)  * (q' yhat)


ddv_to_v f = \x y -> f $ \g -> g $ \x' y' -> x * x' + y * y' 

ddv_to_dv f = \v -> f $ \r -> r v












http://blog.sigfpe.com/2006/09/practical-synthetic-differential.html

synethietic differential geometry using AD
AD can be a lens hting

Iso ~ Eq   is   HOTT

Hott is trying to do syntehtic differential geometry



The Simplest Vector Type

I realized a very simple type for type constrained vectors that had eluded me before.

newtype V2 a = V2 (a -> a -> a)
-- constructor
v2 :: Num a => a -> a -> V2 a
v2 i j = \x y -> i * x + j * y
x v = v 1 0
y v = v 0 1

vadd :: Num a => V2 a -> V2 a -> V2 a
vadd f g = \i j -> (f i j) + (g i j)
smul :: Num a => a -> V2 a -> V2 a
smul s f = (fmap . fmap) (* s) f

-- kind of makes sense that working at kron level is where -o meets actual linearity
v2' i j = \v v' -> i `smul` v  `vadd` j `smul` v' 


Lens (f :+: g) g

f a -> (g a, g a -> f a) 

f a -> g a -- is a rectoanguar matrix

(f a, g a) -> g a


(f a, g a) -> g a -- below the diagonal matrices
( ,    ) ->

Below the diagonal is a completely forward inference

Should we be thinking in terms of null spaces?

Homogenous systems are easier

Oh that's because I wanted the total vector at the end. right.

f a -> (g a, g a -> f a)

(f n -> (g n, g (n + 1) -> f (n + 1))

Oh me oh my, where oh where will I ever find an application for matrices. I'm just a wittle fella. Oh I dunno... ANY AND EVERYWHERE?!


A line circuit?


Compose (-> a) (-> b) c = Compose (a -> b -> c)

http://comonad.com/reader/2011/free-modules-and-functional-linear-functionals/

As a lens is a way of looking inside tuple structures
f :+: g makes sense

ports



Relations as profunctors.

Monoid is a simple idea. Set + operator.
Embeds in multiple ways to catgoery theory
Monoid = Category with 1 object. Get the appropriate structure for free
Objects = Opaque sets / Types picture.

data Monoid m a b where
    Monoid :: m -> Monoid m m m

instance Category (Monoid m) where
   id = Monoid memmpty -- ooh. Actually... NVM. Right this doesn't type check


monoidal category. Monoid operation on objects. 
Now the intuition is that category is kind of set-like and objects are kind of element-like

Graphs, simple Preorders, 

monoidal categories are hmogenous. Mappings back into the same categroy really


So Relation is kind of an operator on two elements
Relations is kind of inhomogenous. Mappins from two different things into another thing

In the monoidal category style catgoerification elements are objects.

Bisiulation
https://mathoverflow.net/questions/70767/references-to-using-profunctors-in-program-analysis

So profunctors = relations makes sense if the objects you want to be in the relations have some notion of arrows between then and you want the relation to play nice with that.
This does not appear to be the general purpose situation.

What relations are interesting / well motivated to me?
 - functions
 - orders / preorders / lattices
 - transition systems
 - specs
 - linear relations
 - convex relations

Pretty good:
 https://en.wikipedia.org/wiki/Binary_relation



You really want to pun on the Hom functor, which in haskell talk is 
absuing (->) as much a possible.



Profunotrs are analagous to relations as
Monoidal Categories are analagous to monoids


Presheaf is contravriant functor

Functor / Contravariant functor is "proof relevant set"

Regular singletons would be the analogue here.
Regular sets


A subset of a category? Objects as elements.
but a funcotr does not select.
A category as an element

F(c) -> ???

Bool-Cat is true/false as only objects.
F(c) = true or false

empty or singleton




monoidal profunctor. C^op C -> C



Proof relevant = Proof exists as objects.
Tree structures. Values.
Dual vectors as certificates


Fine, but what are proof morphisms? Subset relation?
What are morphisms in source categories

Even(Natop, Nat) -> Set

Even :: Nat -> Set (various proofs of evenness)
Even(0) = {stuff}
Even(1) = {}

Could have <= morphism on Nat
Could have divisible relatio



Rel (SNat a) (SNat b)
Set (SNat a)

still. Can you fmap that bish?

ListRel '[ '()] a b 
  Here SNAt a -> SNat b -> 
  There



C is also set.
<= are set inlucsion

And then output is proofs of being in said set.
 <=

 restriction maps
 I suppose perhaps a proof of a subset beloning in  probably is a simple function of a proof in the larger set
 It is an ungeneralizaed proof

 Yes. Ok.
 Suppose C was had an object for every possible subset
 And then the arrows are inclusion

 If you had a proof object that one subset C was in the Proedicate
 and then you wanted a proof that a subsubset of that D <= C
 Then there is probably some way to transform that proof into another

Predicate:
Even where
  EZ :: Even 0
  SSucc :: Even n -> Even (S S n)

Even is the.... proof? Or the functor? The proof. Right?

SetA where
   In :: SetA 4
   In :: SetA 2
   In :: SetA 6


SetBA where
   InB :: SetB 2
   InB :: SetB 6


newtype SubSet s s' = SubSet (s a -> s' a)

Really feels just like composition.
SubSet SetA Even
SubSet SetB Seta

compose to SubSet SetB Even




All of these are of the same charatcer though. Well. Haskell is a somewhat impoverished place.
Everything is Endo ish


bottom <= top 

map to the bottom top lattices
any subsets of something in the set are also in the set.




I can dig this maybe. A general call for categorification just to see what pops out
Trying to inject tjhings into categrial terms


monoids are to monoidal cats
relations are to profunctors

Hom is something
Hom is a relation



a -> [b]



or it is total bullshit.



Why would you have morphisms on proof objects.
Implications (->) curry howard.
conversion of proof to lesser facts ( basically implication though)

Iso is a pretty nice relation

Profunctor p => p a b -> p s t
The only thing you can do is dimap 'em
dimap f g

Functor f => f a -> f b the only hting you can do it fmap 'em
fmap f

FUnctor f => f a -> f b ~~ (a -> b)

(a -> f b) -> (s -> f t)

f a -> f b -- impossible unless a ~ b


eigenvectors of relations - connected components.

rectangular relations = low rank

vectors of relations = sets




applied ctaegroy thoery

homotopy.io
goldblatt topoi - https://www.maa.org/press/maa-reviews/topoi-the-categorial-analysis-of-logic
Generic figures and their glueings, A constructive approach to functor categories
Conceptual Mathematics
Sheaves in Geometry and Logic
https://github.com/hmemcpy/milewski-ctfp-pdf
https://www.cs.ox.ac.uk/files/10510/notes.pdf - catgoerical quantum mechanics

circuits allegories and graphs


Rel (Sing a) (Sing b)

If morhpisms are set link implication
(forall b. Sing (a :: k) -> Sing (b :: k))
(f a -> f b)
Rel (SetA ) (SetB )

Oh yeah. SetA :: k -> *
and SetB :: SetB -> *

This IS the idea of set inclusion.

Profunctors as relations
There is a structure with respect to the left set and right set.
left set has arrows of inclusion
so does

Pro :: Set x Set -> Set


Is wekest precondition like passing back a value function?
roll out compltely in time with SSA vs allow changes and iterate on V(s,s')
concolis execution ~ iLQR. Concrete rollout, Value function back.

speration logic - no aliasing.
not necessariyl the same thing as the self referential 
int -> int pointer airthmetic
int -> int functions suck.

A predicate transformer semanatics
type Predicate = State -> Bool
skip :: Predicate -> Predicate
while
ite :: Pred -> Bool -> 
assign ::




Linear relations
hrep - Ax = 0
or
vrep - y = sum x_i

hrep <-> vrep = row echelon

in and out variables. In and out subspaces.
in : [] - list of indices
out : []

in is a vrep of input space.

in = projection/injection matrix n x d
out = projection/injection matrix. (d-n) x d
in * out = 0. orthogonal

auxiliary variables allowed

compose relations
in1 out1
in2 out2

in = d x n
stack out1 and in2 into matrix. with them equal.

np.hstack(A1, out1 - in2, A2)
in = no.hatck(in, zeores)
in = no.hatck(zeores, out)

drect sum as monoidal product
block([ A, 0  ],
      [ 0, A  ])
in = [in1, in2]
out = [out1, out2]

converse = flip in out

meet = combine the two constraint matrices
join = convert? combine in out?

1-d as unit object
<= is subspace ordering

negation = orthogonalization
division => 


a linear problem is a linear relation of 1-d.

use in1 and out2 as new in/out

fan 
snd(30,10) = project bottom 10 = idetnity matrix stacked.
fst(30, 10) project top 10
id(20)
id(n) = LinRel(np.zeros(0,2*n), [zeros, eye], [eye, zero] ) 
id(n) = LinRel((0,n), eye(n), eye(n)  )
        LinRel [I, -I], [I, 0], [0,I]

"internal" space
class LinRel():
    def __init__(A, in, out):

svd(in * A , smallest )
1 - A*A

Ax + By = 0

vstack adds constraints
hstack adds variables

def idRel(n):
  return LinRel(sparse.lil_matrix((0,n)), n, n)
def mapRel(A):
  (r,c) = A.shape
  newA = sparse.hstack(A, - sparse.eye(r))
  return LinRel(newA, r, c) 

class LinRelV():
class LinRelH():

class LinRel():
  def init(self,A, in, out):
    self.A = A
    self.in = in
    self.out = out
  def compose(self, b):
    assert(self.out == b.in, "Shapes must match")
    
    i = sparse.eye(b.in)
    cons = sparse.hstack([0, i, -i, 0])
    ina = self.A[:,:self.in]
    auxa = self.A[:,self.in:-self.out]
    outa = self.A[:,-self.out:]
    inb = b.A[:,:b.in]
    auxb = b.A[:,b.in:-b.out]
    outb = b.A[:,-b.out:]

    newA = sparse.bmat(  [[ina, auxa, outa, 0, 0,    0],
                          [0  , 0   , i,   -i, 0 ,   0]
                          [0,  0,   0,    inb, auxb,outb]])
    LinRel(newA, self.in, b.out)
  def meet(self,b):
    #hmm. I suppose we acutlaly should split the thing apart again
    assert(self.in == b.in)
    assert(self.out == b.out)
    assert()
    newA = sparse.vstack([self.A, b.A])
    return ()
  def complement(self):
    linalg.svd(self.A)
    return LinRel(get_nonzeroeigs)
  def __negate__(self):
    return self.complement()
  def rdiv(self):
  def transpose():
    self.converse()
  def T():
    self.converse()
  
  # the svd gives you most of what you need?
  def inclusion():
    x = linalg.nullspace(self.A)
    return b.A @ x == 0 #check that every generator is in. Makes sense. Except numrically is trash.
  def __leq__(self):
    self.inclusion(b)

  

  def converse(rel):
      newA = np.hstack( [ rel.A[:,-rel.out: ] , rel.A[:,rel.in:-rel.out], rel.A[:,:rel.in ] ])
      return LinRel(newA, rel.out, rel.in)


compose(A)
linalg.nullspace(A)
range?



(cons1,d1) = self.A.shape
(cons2,d2) = b.A.shape
constrain = np.hstack 
newA = sparse.vstack 


using bigM I can encode the complement of a H-polytope 
but then what?
I do it again I guess?



Lens as concolic predicate transformers.
Jules Hedges is a proponent of the idea of using the Lens type as general purpose bidrecitonal computations
The type in Haskell
type Lens s t a b  = s -> (a, b -> t)

says nothing about getter/setter semantics of the typical lens.
Instead, reading the type, what have a foward pass (s -> a), nd a backward pass (b -> t) that
may close over some information computed from s.
This is a pattern that exists in a number of applications


skip ::
declare :: Int -> Lens (Pred b) (Pred aa)



continuations are very general purpose control flow mechanisms
We can trasnfrom these into cps form, but then 

weakest precondition
predictae we will model as functions (a -> Bool)
true = const True
false = const False
simple substitution mechanism


kissing number - SOS finding?
-- it can find minimations of polynomials. trhough that distribution nonsense.
x^2 + y^2 >= 


relu control of flappy bird.
policy network 
r(x_rollout) - r(x_opt)

Or just violation?
relu(network) -> crash
(unrolling network )




build opam package



complementation of relation ->
At least one must be inverted.

polytope inclusion
-> search for point not in B.
Or, do sandardinni encoding

Really It should generate new variables for an instantiation.
Snd should not reuse the same variables every time.
class PolyRel()
  invars = []
  constraints = [] # store >= values, not full constraints?
  outvars = []
  def __init__():
    all fresh variables
  def compose(self,b):
    PolyRel(self.constraints + self., invars = outvars )
  def complement():
    zs = []
    for c in constraints:
      z, constraints = reify(c)
      # z = cvx.Variable(1, boolean=True) # actually make same shape as c
      # c += c + M * z
    sum(zs) >= 1 # one or more constraints is disatisfied.
  def rdiv():

yeah. We should use a dsl, compile it, then encode it.
data Rel a b where
  Compose ::
  Complement ::
  Converse ::


relu

l1 >= 0
l2 >= 0
l1 <= M * z
l2 <= M * (1 - z)
x = lambda1 - lambda2
y = lambda1

Maybe insetado f subspaces, we should be thinking ellipses and svd.



Lens playing 2048

Dynamic programming is hylomorphism according to ooege and bird
Dynamic programming is unfold and fold operation
But also we want some sharing to occur

dp is also a control thing though
dp for games

reinforcement learning

so 
s -> f s' -- all possible choices
f r -> r' -- choosing the best of all possible choice
f might be [], but it also might be better typed.

We might do it in an applicative context that allows sharing

Why do we need a lens structure? Is the applicative?
s -> g (f s', f r -> r')

There is a heterogenous search tree.

subreddits:
math
sompsci
programming



-- add twitter to sidebar
add 
I loved that hamster homotopy thing
Maybe I should hire a design firm to make my blog better?

class ConvexRel


Wait. But using z3 was the plan with synbolic rel.
it has relation in z3, but I think they are abstract?

I wasn't htinkning python though

I want my Rel to by mathemtically objects.
eh. Neh. fuq it. verything would need to be thunked. I think. Ugh

predicate functions?

def linrel(a,b):
  and (a == 3) and (b == 4) and ...
neg(f):
  return lambda x: Not(f(x))

compose(f,g):
  fresh y
  lambda x,z: And(f(x,y), g(y,z))


top level not -> metalogic not.
not exsists not -> meta(not) meta(exists) not
meta exists is search.
sbv


do everything homogenously in the first place
Cones.

c . x>= 0
c . x >= 1 <===> c . x > 0 ?

hmm. But how do you deal with integers? They can't be homogenous?

Can I take the union of cones?

generate by rays
x = l r1 + l2 r2
complementary slackness still

0 <= l <= M * z
0 <= l2 <= M * ( 1 - z)

M becomes an effective thing
complementary slackness is sometimes offered as it's own thing
Well, make a point at infinity.

DO I have like 4 layers of prejectiviation at this point?

Can we always get rid of big M this way?





This isn't about positivity of a polynomial though.

are union find and share kind of dual?

GODDAMN I have too many unfinished drafts

Coq learn x in y
My Tensor thing - This one seems fizzled
- Functor Vectors

and now I want to start digging into the polynomial stuff again

Beat one. nice

Yeah. Cox littlewood and osheai is so great



The quotient ring and representing x.
The linear map correspodning to 

You can reduce back to polynomial form by finding the charactersitic matrix.

We could build our own algerbaic numebrs with sympy

Reoresentation theory and solving?

Each of these is a blog post. Keep em short.
Grobenr basis
Classical feynman diagrmas and grobner bases
Every physicsts knows that the momentum and enrgy conservation ptoblem is impossible.
Optics and curves
Actually doe up the sudoku example / graph color
Closed form pendulum.
Integer programming / Graver bases
robot kinematics

factoring is important.
grobner basis makes multivariate division unique


syzygy are relations between generators right?
what if we form a vector of
1 g_1,  x g_1, x^2 g_1

Then we can express them linearly.

Brute forcing.
Integers = guess and check
polynomial = evaulate, integer factorize

multivariate - evaluate 1 variable. Almost feels like a csp method.
We'd have no clue how.
Unit propagation - univariate polynomials are real nice. (relatively speaking)
use algerbaic numbers. 
Especially helpful over finite fields. Then it really is kind of a csp.


f(x) on the solutions


linear relations ->
Convex relations or
polynomial relations. algerbaic constraints
Use grobner basis to project out inner variables

what about a derivative operator. That would reduce the degree
partial + x

gen otimes x otimes y
can generate division. That is part of grobner basis.
generate division of all lesser bounds.

we can just repeate stuff upward you know.
x *  (same equation) = 0
if we include 1 as a generator, we get the raw boys.
Then we can include all division.

or we can add derived new terms to the bottom I guess.

some particular monomial ordering


descartes rule of signs
signature of matrix (schur stuff for that too)

a sefl referential structire
f (Rep f)
Combining structings
f :*: g (Rep f + Rep g)










coinductive delay monad
type Delay = Next (() -> a)
But we perhaps do it before every recursive call.
Gives us the ability to tranafer control around


There is an existential in there

The Le form of lens
Iso to an exitenstial type

Since we build our combinators from the ground op, we don't HAVE to use opaque functions
We can look into the closure guts

Hmm. Yeah. They get to stuff shit in there
They have an expanding environment.

data Lens s t a b = Lens :: s -> (a)

data Lens' s a = forall q. Lens'
    { split   :: s -> (a, q)
    , unsplit :: (a, q) -> s
    }
We can save auxiliary data in q. (The sharing)

Hmm
Their type is more like
forall q. (q, s -> (a , a -> (s,q))
Oh. And the second arrow is diffferentiable. Is that a primitive lens or this new thing?
Lens s a = forall q. (q, s -> (a , Lens a (s,q))

The problem of no sharing is that you can't record every application 


data Lens' s a 

weight1 -> (l1 -> l2)
weight2 -> (l2 -> l3)

The differentiable curry
https://openreview.net/pdf?id=ryxuz9SzDB



target tracking is a natural arena for annihilation and creation operators.
visual tracking, ojbect tracking
movement: a^dag * a
Fock space.
() + 1 + 2 part

Quantum Field Theory and the particle tracking problem

Dynamics = p * a + p * a^dag + a^dag a
(solvable via bogoliuboc transfromation. Isn't that cute?)
Observations =

Representing Quantum Fields in Haskell

Dynamics : straightforwardly related to QFT

Observation : 

The free monad 

AntiSymFock f

Distriguishable space
Free f = a + f a + (f f a) + (f f f a)

== FreeVec (f Bool) C
adag :: Rep f -> f Bool c -> 
cdag :: Rep f ->

(sum (\i -> (adag i) * (a i))) vac

SymFock f = FreeVec (f Int) C




LLL. reconstruct best algebraic number using integer programming reconsturction

It's actually exactly least squares. huh
[1 x x^2 x^3 x^4]



Parametrized lenses
Keep weights as parameters
compose :: L a p b -> L b p' c -> L a [b,b'] c

2-ctategory. We can consider 2-morphisms also

L a p b -> L a p' b


liquid crystal simulation
That might be kind of cool and interesting
Can I get the minimum Energy
x**2 + y**2 == 1 (*  XY Model  *)
E = sum ((x - x')**2 + (y - y')**2) 
min E

what if vx, vy are functions of x y.

x ^2 + y^2 + z^2 == 1 #put it on the unit sphere
vx^2 + vy^2 == 1

work in the residual ring of that



Larch
JML
Spec#
Z3

de Millo Lipton and Perlis


(z-z)

describe edge wave shape implcitly
phi(z) = 0





What am I up to?
* Envelopes - nice
xy model
* indexed lens - nice
* coq learn x in y - nice
2 Vect
the simplest vector
Quantum circuits
tensors
Neural flapy bird
polytope/linear relations


class ICategory k where
  compose :: k b i c -> k a i' c -> k a (i' ++ i) c
  id :: k a '[] a

  or
  compsoe :: k b f c r -> k a g b r -> k a (g :+: f) b r

Reminds me of my 2d grids.

k i b c -> k i' a b -> k (i' + i) a c 
(k i) is almost a category then
(k i)  = p
(k i') = q
Then Procompose  kind of does the trick


flomble :: k i a b -> k () (i,a) b
flimble :: k i a b -> k () a (b, i)
bend and bow

type Morph2 k a b i i'= k i a b -> k i' a b

forward dynamics
s -> s'

Probabilistic dynamics
P s -> P s'

backwards dynamics
(s' -> Bool) -> (s -> Bool)  -- (allows nondeterminstic)
P s' -> P s (backwards inference)


Backwards iteration. Includes dynamics and reward function
V s' -> V s



type System s a s' = (s,a) -> s'
type Controller = s -> a -- controllers can also depend on histroy, but a feedbakc controller is common
type Controller = s -> P a -- probabilistic controller.
type Step s s' = s -> s'

step = system . (snd controller) . fan 

type Value s = s -> Double
type Reward s = s -> Double
type Value s = s -> Double -> Double
type Reward s s' = s -> s' -> Double

a value function transformer
type Value s s' = (s' -> Double) -> (s -> Double)  = Value s' -> Value s

value v' s = (reward s) + (v' (step s)) 


s [w] s'


value = step and sum reward



type Observations

(s, a) -> s'

However (s,w) -> a is a parametrized control function

Functions are data. A pointer and closure object. deapply


s -> f \otimes g
f \otimes g -> s

lenses for quantum computers

also you can take a density matrix and make a larger space wehre it is pure.?

iterative LQR

1. 
(s,a) -> s' is differentiable (linear dynamics)
2. 
(s,w) -> a is a affine feedback controller 
3.
Reward s = Matrix s. quadratic form OR second differentiable function
3.
V s = Matrix s
4.
V s s' is schur complement procedure
5.


LQR
1. 
(s,a) -> s' is affine
2. (s,w) -> a is affine





Polytope relations

global ctx

() /\ () = conjunctin
() \/ () = union

compose


not () = \/ \/ \/ \/ \/ \/ \/ # at least one is wrong

kalman filters.
sum (x - o)**2 + eps**2 (error in dynamics vs error in observation) + (x0 - x_prev)**2
x = x + eps

ok. What about just the neural nets

data AD w a b = AD (w -> a -> b)

newtype AD w a b = AD (w,a) b

compose :: AD w a b -> AD w b c -> AD (w,w') a b

type Morph2 



data AD = AD w -> a -> b
         Compose :: ++


Heap and seperation logic

lift :: (w,a) (w',b) -> ((q,w),a) ((q,w),b)
lift f = 
assoc . (par id f)



weakest precondtion in the presence of arrays
a[k] = 3 does not work the same as x = 3
We update the entire array


pycon - by december submit - I get a twofer for z3py tutorial
ML conferences? We could submit our formal bird maybe.


Polyhedral relations

PRel a b = PRel (?)

2Vect is good for probablities of paths
Anyons are contrcted by paths

Anyons

Berry Phase
SSH model

Quantum ElectroStatics
Random potentials and QFT
vector spaces  I have loved


Gauss-Seidel Lens
Kalman filter
Laplace equations with RelLens

type LinRel2D l r u d = LinRel (Either l u) (Either r d)
{-
For general laplace equations 
R \partial \phi = I
\phi = V
-}       /
         \
         /
        |
-/\/\/----/\/\/\-
        |
        /
        \
        /
        |

flow
V-----V
|     |
|     |
V-----V

Nystrom method


stencil :: LinRel IV IV IV IV
stencil = LinRel [1,0,1,0,1,0,1,0, -- current conservation

stencil = (hpar r10 r10) <<< short <<< (hpar r10 r10) where r10 = resistor 10   

-- oooh I used hcompose 
hcompose hcompose 
stencil2 = hcompose stencil stencil
stencil4 = hcompose stencil2 stencil2
stencil16 = hcompose stencil4 stencil4

vcompose stencil16 stencil16

class Grid n m
    type L  
    type R 
    type U 
    type D 
    gridrel :: LinRel A B
instance Grid n m => Grid n (S m) where
    gridrel = hcompose g g where g = gredrel @n @m

instance Grid n 0 => Grid (S n) 0 where
    gridrel = vcompose g g where g = gredrel @n @0

instance Grid 0 0 where
    gridrel = stencil

Category Theory and the Laplace Equation

The Laplace equation is ubiquitous in physics and engineering.
It and slught variants of ti describes electrostatics, magnetostatics, steady state heat flow, elastic flex, pressure, velcotiy potentials.
There are a couple reasons for that.
It results from minimizing the square gradient of a field which can make sense from an energy persepctive.
It also results from the combination of a flow conservation law and a linear constitutive relation.
It also gets used even if not appropraute because we know how to mathemtically deal with it.

Yeah
There is a boundary relation connecting that value of a potential
and the derviatvie normal to the bundary assuming that the potential oberys
the PDE in the interior.
A Green's function is the continuous analog of an matrix inverse. It is a function such that
LG = \delta
The boundary is the subtlest thing in differential equations. It is an intimate part of the statement of the problem
But the notation does not make it feel that way.

Neumann Map, Dirichlet Map, Boundary Map?

stencil 

computing homology
range mode kernel



Could I mix a gauss seidel solver and the closed form relational solver?

If we replace the direct sum with tensor product, we are talking about quantum fields.
Quantum fields do not obey linear relations however?
Between 1 time to the next they do

We often care about the spectrum. The spectrum is plasubly described by
H - lambda. So linear relations as rational functions of lambda



what about optimal Transport
min d(x,x_t+1) p(x,x_t+1) + regularizer

instead of marginal p = phi1
          marginal p = phi2
we can constrain the individual and joint? probabilities.
s.t.
E[x^2] = x^2
E[]

I may have mixed metaphors
integral p = 1
p >= 0
p = SOS
Yeah. Sum of Squares and optimal transport make sense.


Question is, given video of a bunch of particle tracks
or an image morphing in a fixed velocity field
reconstruct the veclity field or the eq o motion of the particles


data Ordering' = EQ | GTE | LTE | LT | GT
-- Or let the ordering be implicit via not
data PolyRel a b = 
  Lin (Vector Double) Ordering Double 
| Not (PolyRel a b)
| Meet [PolyRel a b]
| Join [PolyRel a b]
| Compose :: BEnum b => PolyRel b c -> PolyRel a b -> PolyRel a c
|

data Query a b = Query (PolyRel a b) Ordering (PolyRel a b)


data PolyRel a b = 
  Lin (Matrix Double) Ordering (Vector Double)
| Meet [PolyRel a b]
| Join [PolyRel a b]
| Compose :: BEnum b => PolyRel b c -> PolyRel a b -> PolyRel a c

{-
Only relational operators need to be in the dsl
Many of these follow from the linear relations, which are a subclass



convert :: LinRel a b -> PolyRel a b

converse = Lin (i ||| -i) EQ (vzero)

| Converse :: PolyRel a b -> PolyRel b a
| Par 
| Fst
| Snd
| 
-}

-- This fragment is linear programs.


interp :: Query -> MIP 


MIP -> SAT / UNSAT 

Polyrel bool doub bool doub
or

BEnum a


reify :: PolyRel a b -> PolyRel a (BUnit, b)

relu :: PolyRel () ()

ite :: PolyRel (BUnit,a,a) a 

hull
halfplane

rdiv :: 

optimize :: PolyRel a a -> PolyRel a b -> PolyRel a b  


boolvars = cvx.Variable(N)
upper = cvx.Parameter(N)
lower = cvx.Parameter(N)

root = BBNode(prob, boolvar, upper, lower)

upper.value =  
lower.value =

#is_bool mask
is_bool = upper.value - lower.value < 1-3



LinRel as a profunctor

LinRel a b

LinRel a c
or
LinOp a c 

Mixed representation. H in x, V in y 
y = Ax + Bl

Composition becomes easy
z = (A2 A1) x + A2 B1 l1 + B2 l2

(A2 . A1) (A2 B1 ||| B2 )

looks like controllability.

AffineOp (Matrix Double) (Vector Double)
compsition = no problem. -> Homogenization is starting to sound pretttty good


topological mechanisms with grobner bases

http://www.argmin.net/2016/05/18/mates-of-costate/
backpropagation = adjoint method
https://en.wikipedia.org/wiki/Iterative_learning_control
iterlative linear control (ILC)
Try to machine learn LQR controllers (Recht's linearization principple)


Direct Q method.
This inner maximization is a bish.
V(s) = max_u r + V(s_t+1)
+ a fitting


buchberger and 
finding a good monomial ordering is a superset of finding a good elimination ordering. You want to minimize fill in and 
Symmetric Positive Definit is nice. Polynomail analog?
Seperate equations into uppper and lower diagonal.
Iterate gauss-jacobi style using 1-d poly solves
sparse preconditioner. Do LU but ignore any terms that create fill in. Analog would be do S-polynomail prcoedure of buchberger
krylov subspace. GMRES becomes exact


Neural weights = controls or lagrange multipliers

We can do ADMM for composition. Which is pretty good.

ADMM = Gauss-Seidel

We have also said Gauss Seidel is a lens.
We can do lagrange multiplier gauss seidel.

\lambda(x-Ax) + rho(x-Ax)^2


Yes, the Kalman filter style passing back the V matrix allows a second order method (probably). But at the expense of flexibility.



Hmm. We could literally use pytorch as an ADMM controller?
Write down the 1 timestep solution.
Make the weights the ADMM lagrange multipliers.


Batching might even be useful

One way to make robust control is to just try all the plausible dynamics
With the constraint that they have to use the same control at the first time step.

Or batching might be useful for a bit of random search. start with randomized weights. Take the best control seqeunce found.

Or unscented filter style to account for inaccruacy in the current state measurement.





costate state relationship
costate is linearized Value function (?)
Qx = p (at final stage)
p = p + Qx


(x, u, p)

the drviatvie of afunction is is linear function
the derivative of a nonlinear relation is a linear relation

polynomial relations
semialgebraic sets (semialgebraic relations)
Natural notion of meet. Fixed degree even


The method of images as a fitting problem
Fit the potential of many point charges such that they stay outside the region. The will eventually devolve into a surface density.

The infinite sequence of images (reflect reflect reflect)

expansion matching problems
(hard to keep stuff straight)

Green's functions.

keldysh formalism and lens.

ising model zoom in
ising model conformal map
How could I do this?
Could use that same image remapping guy I did with will's face
Pick some randomish complex polynomails
Maybe some ln or 1/z or exp

resampling as we're slowly mapping?
really high resolution?

The 2 boson vector is indexed by the unordered pair {a,a}.
The unordered pair is not a very type theory thing.
Converting an ordered pair to unordered is destructive of information
Does an unordered pair have a universal property?
We can only apply symmettrical functions
f(a,b) = f(b,a)


elementary symettric polynomials
random ordering semantics.

There was a construction to make an ordered pair from unordered. In Jech set theory book {{a}, {a,b}} was an ordered pair

a^n / n! is always an integer? no it isn't
by induction
a^n / n = 5 ^ 2 = 25 / 2 = no
a^2 / 2
a * (1 + (a-1)/2)
a + a * (a-1) / 2 

Eq a | Distinct a a 

fermions are indexed by distinct unordered pairs. (kind of. With minus signs)
The very ordered pair.
Ordered a a (where a1 < a2) -- canonical ordering

constructively ordered via storing only the difference
(nat, nat) == (x, x + d)

3 colors ordered intrinsically
data RGB1 = R1 BG2 | B1 G2 | G1 Void
data RGB = R1 GB | GB  -- suppress void case. and unique GB case
data G2 = G -- could supress this one
data BG2 = B2 | G2  

-- we have to intertwine our container and our data type.
-- perhaps this is not unexpected.

and in some sense we want to write that these colors correspond to these
data RGB = R | G | B

downvert :: RGB1 -> (RGB,RGB)
upvert :: (RGB,RGB) -> Maybe (RGB1) -- or orders them
sortRGB :: (RGB,RGB) -> (RGB1)

sortRGB . downvert = id
downvert . sortRGB =/ id 
Is this an adjunction? It's almost an isomorphism


Sorted Finite pair
No I might still have to intertwine container and held type. 

Pair :: x Fin n ->   



Intrinsically sorted floats. GET OUT OF HERE.



Ordered list
but also 
[Nat] is acceptable as a rep
then fold to get which exact element
If there are a finite number of optins, the nat thing doesn't work





data LTE = R RGB | G GB | B B 

so the vector analog would be

An intrincisally expressed fermiion fock space using ordering
data Vec N a = Scalar a | Vec N (Vec N-1 a) | Skip ?

Do you really need an ineuqality to express unorderedness?








Verifying calculations in sympy?
psi(t) an abstract vector that depends on t


Discrete time born series.
Does is even make sense?
The is an algerba of differentials
But not one for finite shifts?
U + U doesn't really make sense
UU maybe but what is the series? trottery


psi = U psi
U = e^i(H + V)dt


Fitting a minimal kullback leibler sitrbituion as renormalization step.

Canonical ensembles



abstraciton function defnition of operatins

gradient of V function

Qx + grad_x V + lam A  b= 0 
   +  V

   Qx + A^T l = 0

min   x Q x + dV (Ax + b) =
dV cdot dx_f under constrain
dV = Q' x_f

c x 


Q + l (Ax + b - x) + 

C = \sum_t x_t Q x_t + u_t R u_t + \lambda_{t+1} ( A x_t + B u _t - x_{t+1} )

equations of motion
\nabla_{\lambda_{t+1}} C = A x_t + B u _t - x_{t+1} = 0
initial conditions
\nabla_{\lambda_0} C = x_i - x_{0} = 0 


backwards iteration of value derivative
\nabla_{\x_t} C = Q x_{t} + A^T \lambda_{t-1} - \lambda_{t} = 0
\lambda_{t} =  A^T \lambda_{t-1} - Q x_{t}

Final condition on value derivative
\nabla_{\x_N} C = Q x_N - \lambda_{N} = 0 

Pick best action.
\nabla_{\u_t} C = R u_{t} - B^T \lambda_t  = 0



x^T Q x ==

dQ^T x s.t. dQ = Qx

2 step
y = A x
xQx + yQy = xQx + dQ y =
x Q x + dQ (A x)
= dQ2 x
dQ2 = Qx + A^T dQ
dQ = Q y = Q (A x)

 x dQ2 s.t. dQ2 = Qx + dQ y


For a finite difference approximaition
A ~ (1+dt) so A is full rank and nearly the identity.

Q function
Q^T [x | u]


shooting method
In the standard formulation of the shooting method, we maintain the controls between the different runs.
We can tie together 1-step MPC controllers into N-step MPC controllers.

ST s (x -> (x' , b -> ST s a)

State w w' a b



compose :: STLens -> STLens -> STLens
compose = do 
           f <-
           g <-
           return (f <<< g)


each stage of the thing is not forward integration
each stage is the solution of a 1-step optimization problem.
  lam () (x - A x)^2

  The lambdas persist. Not the controls.



widening operators




shape analysis
abstract domain of graphs for transition systems and heap states
sets of graphs
A Relational Shape Abstract Domain

Reachability domain. Hmm. R a b is is b reachable from a. Certainly does describe
Or distance domain R a b is bound on distance.





This bryson and ho is nuts
Evasion pursuit games makes perfect sense
missile defense
worst case
declan's cat and mouse game



integer programming and galois connections
abstraction conretization function = relaxation functions?


I some sense H-Rep is analog of (a -> b -> Bool)
V-rep [(a,b)]

We play this game of taking a 1D domain, lift it to 2 arguments
There is a slgihtly larger space of possibilities there.

quantifier elimination wth linear equalities
lattice equalities?
how to deal with not , disjunction?

pursuit evasion games
differential games


CEGAR and pursuit evasion
Given a fixed strategry we can laternate optimizations
Givne a closed loop strategy we can alternate optimization

robust control
alternating quantifiers
cegar



vertex pursuit
homocoidal chauffeur
lady lake problem


The simplest error bounds questions

linear ODE

1 variable linear recurrence
different time steps

we want to compute a tube for the ODE

numerical integration is perhaps easier.


dot(f-f)
dy/dt = y

min max |~dy/dt - ~y|
~y - y >= 0 forall x

but we don't have y.
min \int y or min max y


min y(x_f) 
~dy - dy >= 0 forall t.
+ \lam(x + x_0)(x - x_f) 

and vice versa

max ~y(x_f)


dy = xsll,
this gives a verified bound for integrtion

suppose
dy = y

y(x_f) - y(x_f) = int ~dy - dy
min 

min ~y(x_f) - y(x_f)
~dy - dy >= 0

~dy - y >= 0


~y >= y forall x implies
~dy >= dy forall x
and vice versa

min y_u(x_f) - y_l(x_f) -- minimize bound on y(x_f)

y_u >= y >= y_l   forall x 
dy_u >= dy >= dy_l   forall x
dy = y
dy_u >= y >= dy_l   forall x

remove y via fourier motzkin. I can't directly discuss y.
differentiation is linear. but. Just because y >= f we can't conlcude dy >= df
hwoever, dy >= df and y(0) >= f(0) we can concluder y(x) >= f(x) forall x
because integraiton is positive.

\int p(x) is positive also. if p(x) is always positive. 

y_u >= y >= y_l
defivition of derivative
y(x+dx) - y(x) = dy
y_u(x+dx) - y_l(x) >= y(x+dx) - y(x)
not infinitesimal though

This is reminsicent of how that hamming book bounded errors. Everything is polynomials. use integration. bound integrals with absolute values of max




min y_u - y_l
y_u  >= dy_l
dy_l >=  y_l
y_u(x_0) >= y(x_0) >= y_l(x_0)

Did what I just did actually make sense?

And then from a solution of this, can I 


min max (y_u - y_l)

min max |~y - ~dy| -- minmax redisual

=>

min t
st.
-t <= ~y - ~dy <= t

=>
This solution is pretty good, but has no direct connection to the actual equation in question.


The cornerstone of approximations is Taylor's theorem.
Taylor's theorem depends on the intermediate value theorem.
It is non constructive? in that we don't know the position where
the appropriate value lies.


homotopy method. n >=3 games
relational feedback - don't use u = Kx. K compounds then
Apply the R matrix method to kalman filtering.



approximat

Consider the finite difference method
We can make an LP consiting of the solution of the finite difference method
And the 

#y' = y

N = 10
dt = 1.0 / N
y = cvx.Variable(N)
y_u = cvx.Variable(N//2)
y_l = cvx.Variable(N//2)
c = []
c += [y[0] == 1]
for i in range(N//2):
   c += [y[i+1] = y[i] + dt*y[i]]
   c += [y[i+2] = y[i+1] + dt*y[i+1]]

   c += [ y_u[i] >= y[i]  ]
   c += [ y_u[i] + y_u[i+1] >= 2 * y[i+1]  ]

   c += [y[i] >= y_l[i]  ]
   c += [ 2 * y[i+1] >= y_l[i] + y_l[i+1]   ]

obj = cvx.Minimize(y_u[-1])
prob = cvx.Problem(obj, c)
prob.solve()

plt.plot(y.value[::2])
plt.plot(y_u.value)
plt.plot(y_l.value)

If you have an approximate solution
bound the differential equation of the error
e = y - ~y
de = dy - ~dy

e - de = ~y - ~dy -- an inhomgenous equation

The "best" solution is one that under approximates
Galois connection idea - like division and subtraction on Nat
Polynomial tubes





derive a p(x) from this?



~y 

~dy - ~y >= 0 


min max |
~y - y <= 0 forall x

lim n -> inf

~y1 = ~y2 = y ?

Similar in many respects to just comparing different finit dt


mean value theorem for polynomials (only under algebraic numbers)

support f(x1)<= 0
and f(x2) >= 0
then there exists a root in between?
Yes in algebaric numbers, but that is almost the definition of algebraic numebrs 


What is the meaning of the dervative of a polynomial?
f'(z) = f(a) - f(b)
a <= z <= b 


[0,1,2,3,4] -> [,23,3,12]

type Poly = Nat -> Q
type Poly = [(Nat, Q)]
type Poly = Z | [positiveQ]
type Poly = Ring a => a -> a -- but we don't get an induction principle even though I know it is true that all 
diff f = \n -> (n + 1) * f (n + 1) 

eval :: Poly -> (Q -> Q)

put number of coefficients into the type
type Poly N = Fin N -> Q

what characterizes polynomials ve all other functions?
type Poly N = a -> a
5 unique evaluations <=> polynomails of size n

if 2 polynomaisl agree on 5 unique positions = equal

5 paramter function class with no degeneracy
Ok, maybe that is sufficient to define an interesting class of functions






Relations and a Boy who loves them
Abstract:
Fun with Relations
Relations are a canonical example for many categorical concepts and a unifying abstraction for database theory, functional programming, and linear algebra, and more. Relations possess a high level algebraic description useful for the discovery of new facts and computationally efficient forms. 
Join us for a talk on relations, their algebra, and some of the things you can do with them.




<= y <=
<= y <=


polynomials and be simply differentiated and integrated
They can be factored into squares terms

finite difference method

 y <= ~y

 Polynomails can be summed in closed form. falling factorials are better

 find (minimum) polynomial s.t.
 min ~y(xf) = \int ~dy
 ~dy  >=  dy

 do closed form integration 

k.

finite difference equation
dy = y
solutoin = 2^x


dy >= y

~dy >= dy
dy = y


~dy >= y -- this is equivlanet to the above
~y >= y -- dervied from above
y = dy
~y0 >= y0
I don't even understand how I could eliminate in principle?
the fuq

y+ - y = y -- we can flip 'em using this

min y
dy >= y


min max (y_u - y_l)

bounds on integrals of polynomials
bounds on integrals of rationals?
As long of you don't go over a pole, rationals stay negative or positive

<= \int 1/x <=
ln(2)
ln(e) = 1

Linear differential equations only?
Higher order and Matrix systems

minK y
dy >K= Ay

  _
| _|

COuld build a bounding box with 4 queries for SHO.
tubes in different directions

sin(1)


max over t and components.
Or can weight directions differently

min max |dy - Ay| forall t.
max (w .*  (dy - Ay) )


-t <= dy - Ay <= t



|dy + Ay| <= |dy| + |Ay|

Hence we can derive a bound
|dy| <= |A|

This is gonna be a weaker bound though.


consider just a straight line approximation
dy >= y

mt + b


min m + b = y(1)
m >= mt + b forall t ===> m >= b && m >= m + b -- can't do it?
and y(0) = b >= 1

impossible. ?

min m + b
dy(1) = m >= y(1) = m + b
y(0) >= 1


recurrences in the master theorem

ceil(ln n) is sensible as a galois connection



linear odes could kind of be solved in closed form anyhow

inhomogenous too though
We'll allow terms like (1+x^2) and stuff
So we're bounding exponentials, bessel, legendre and such.



reducation = 




Fitting sine

What about using 1-step perfect V(s) solving

V(s)
And we can do back and forward passes.
And simulated runs to figure out where to 

Relu
       /
      /
_____/



Picard iteration
Never heard it called that.
Born series, neumann series.

The SOS tube domain
forall t. l(t) <= x(t) <= u(t)
l <= u

x_0 + \int f(x1) = x2

forall x l <= x <= u
the contraction is also in l u
We would wnat to seek a derivation
A derivation is a dual problem

l(x) * (x2 - (x0 + int x1)) + l2(x) * (x - l) + l3 * (u - x)
= x2  - u
= l - x2



<= x <=



show/force tube x2 is inside x1
I'm not sure that this has much to recommend it over the taylor model method

Iterative method for PDE
Relaxation method for laplace

Write as 
min |del phi|^2
replace one phi?
Make average of phi+ phi-?


Alg = forall a. (f a) a
Cata :: (SRel (f a) a) -> SRel (Fix f) a



SMT + relations
The complement defininition might be really useful


type SRel a b = a -> b -> SBool
type SRel a b = a -> b -> Symbolic SBool



free_ :: SymVal a => Symbolic (SBV a)


tyep SRel a b  = SBV {
  invar :: a
  outvar :: b
  }

compose :: Symval b => SRel -> SRel -> (SRel b c)
compose p q = \a c -> do 
                  b <- free_
                  p' <- p a b
                  q' <- q b c
                  return p' && q'


complement p = \a b -> fmap snot (p a b)
converse p a b = p b a

rSub :: SRel a b -> SRel a b -> Bool
rSub p q = prove $ \a b ->
     p' <- p a b
     q' <- q a b
     q' implies p'



We're sniffing john wiegley's work on compiling to a category of SMT





PolyCone = [Vector Double]
-- a cdot x >= 0

data DD = DD { hrep :: , vrep ::    }

halfplane h = DD [h] [basis] 

dual (DD h v) = DD v h

addh h (DD h v) = 
        go h (v:vs) (jp , j0 , jm) = 
        dot h v > 0 = go h vs (v : jp, j0, jm)
        dot h v == 0 = go h vs (jp, v : jp, jm)
        dot h v < 0 = go h vs ()


partition h vs = let vh = dot h v in
                 if h > 0 then 
                 else if h == 0 then
                 else if h < 0 then

vs = jp ++ j0 ++ [ (dot h vp) * vm - (dot h vm) * vp |  vp <- jp, vm  <- jm ]


conjunction (DD h v) (DD h v) = DD v



interior point based projection?
taylor series of barrier function?
chebyshev center expansion at


stat mech inspired prejection.
e^ln() = \int x e^{ln} ~ at cheby center eval series.
      ~  -- schur complement as chebycenter
convex functions are those for which a steepesest descent procedure should work very well
x^2 + x^4 = Great

XY model 

f <= u
A linear map takes a convex set to another convex set
l <= f <= u




Jeez.
I dunno
determining equations
transformaiton in prolonged space

Permutation symmettry
1 x y xy xx yy certainly has a representation under permutation symmettry
1 swap => Find common eigenvectors = invariant polynomials
Eigenvectors of eigenvalue 1.
1
x + y
xy
xx + yy -- these two aren't distinguished actually

variant
x - y
xx - yy


om_xy yz xz

1 -1
1 1

y partial_x - x \partial_y => is in fact a matrix on poly space
[]
[]
[]
common eigenvalue 0 are invariants

near-invariants = common nearly 0 eigenvalue

Lie differential method, except we give explicit paramtriazation.
Do the best you can.

invariants

Olver
sympy has it. There is a lot of guess the appropraite symmettry functions
https://www.researchgate.net/publication/233653257_Solving_Differential_Equations_by_Symmetry_Groups

families of curves. convert to differential equations by differentaiting out constants.

orbits
invariants


polynomial functions invaraint under rotations
linear group actions on a fixed set of monomials 
should be linear operators on the space of monomials

Rotations and Spherical harmonics
SPherical harmonics are the angular part of particular polynomials

For lie groups, we only have to consider the generators?


clebsh gordon - Eigenvalue of each of them








Linear airthmetic proof system


data Prop = GTE (Map var Ratio)
data Formula = Conj [Formula] | Exists [var] Formula

data Prop = Prim String
data Formula = Conj [Formula] | Disj [Formula] | Implies Formula Formula | Prop Prop


data CNF = ConjDisj [[(Bool, var)]]
data Conj a = Conj [a]
data Disj a = Disj [a]
data Not a = Not a | Ton a

type CNF a  = Conj (Disj (Not a))

resolve :: P (CNF a) -> i -> j -> var -> Maybe (P (CNF a))
resolve = 


data ProofTree =  

data Formula = Exists (var -> Formula) | Not Formula




evaluator is most convincing.



data FRel a b = Cata (FRel (f a) b) | Raw SRel a b 



Ok. 

(forall a. a -> a) -> (Rel a b -> Rel a b)
convert polymorphic functions
convert 
(forall a. a -> a) -> (Rel a b -> Rel a b -> Bool)



Combo SMT and quickcheck

SMT for spec of program.

quickcheck for automated testing

sorting, sorted predicate and has exactly the same elements 




type C1 a b = a -> b
type C2 a b = (a,a) -> b
type C2' a b = a -> a -> b
type C3 a b = (a,a,a) -> b
type C3' a b = a -> a -> a -> b
type CO a b = [a] -> b

[a] -> b == Either () (a,[a]) -> b = Either (() -> b)  (a , [a]) -> b)
= Either b (a -> [a] -> b) = Either b (a -> ([a] -> b))




forall s. ([a] -> s) -> ([b] -> s) == [b] -> [a]



type C12 = Either a (a,a) -> b
type C12' = (a, Maybe a) -> b
type C12'' = a -> Maybe a -> b
type C12''' = a -> Either b (a -> b)

forall a. (a -> a)  == ()
fix x^x -- 1 is fixed point

(a -> a) -> a -> a == (a -> a, a) -> a
a^(a^a * a)

does forall have an intepretation as nat?
type P a = a * a + a -- sure, functions on nat (polynomials)
Fix f = fixed point (* not clear if a well defined concept *)
forall a. -- initial something. universal something.
forall is an infinite product?

forall a. a -> a = () -> () and Two -> Two and ...
but, not arbitrary Two -> Two

(a -> s) -> (b -> s) -> s = a + b = (Either a b -> s) -> s = s ^ (s ^ (a + b))
forall s. (a -> s) -> s
kroncker delta. not dirac delta

[i == j]

limit s -> 0
limit s -> 1

s ^ (s ^ )
limit s / limit 
lim
max 
min
sum
prod
subst 
average
expectation
intersection
conjunction
disjuncion 
union
forall a. a == 0 -- this is an interesting point. Any polynomial of type parameter will get evaluate
at 0.
forall s. s * s * a + b * s + c == c

(a -> s) -> (b -> s)

Game semantics - surreal numbers or nimbers
a * b =/ b * a if inifinite. Makes sense
https://stackoverflow.com/questions/25344032/forall-as-an-intersection-over-those-sets

Ord (a,b)
   compare (x,y) (a,b) = 



A pair of numbers? (x, y) One for psotive occurences one for negative.
(a -> s) -> (b -> s)
Now we recursive compute
(a,0) -> (b, 0)

(a,1)
(a -> s) -> s

Surreal numbers? exists forall = game semnatics, surreal is combinatorial games.
The arrow is the dividing line

 a -> b

 {    |     }

forall a. has to work for void too
forall a. a => Void
Void -> Void also has one arrow.

Surreal haskell
Void is empty set
type Zero = Void -> Void = 0 -- feels like a 1?
type One = Void -> Zero = Void -> Void -> Void -- feels like a two?
type NOne = Zero -> Void -- (Void -> Void) -> Void -- feels like none maybe. Void-> Void ~ 1, () -> Void. 
type Two = Void -> One


a - zero
a -> a -- one
a -> (a -> a) -- two
-- and so on

forall b b. -> forall a. a
-- possible to write, impossible to call


(a <- a) negone

exists b. b -> forll a. a

Impossible to call functions
Impossible to write functions.
Possible to call and write
caller wins, writer wins.
Void -> Void -> Void. Caller extra loses.
() -> Void, Void writer extra loses.

Void - impossible to wrtie
Void -> Void - imposible to call
forall a. a -> a possible to write and call.
 
game semantics


Relational intepreter

Void = empty relation
Two = Linear ordering two
() = identity relation

A relational Mapping.

type Rel a b = [(a,b)]
Product (Rel a b) (Rel c d) = Rel (a,b) (c,d) 

par :: Rel a b -> Rel c d -> Re (a,b) (c,d)
reynolds :: Rel a b -> Rel c d -> Rel (a -> c) (b -> d)
reynolds as bs = [ (f,g) | f <- enumAll, g <- enumAll, all (\(a,b) -> (f a, g b) elem bs ) as ]
id :: BEnum a => Rel a a



instance BEnum a, BEnum b => BEnum (a -> b) where
   enumAll = helper enumAll
      helper (x:xs) = [ \y -> if x == y then a else g y |  a <- enumAll , g <- helper xs   ]
      helper [x] = [ \y -> a | a <- enumAll]
      helper [] = []
-- redundant a -> b
instance BEnum a, BEnum b, BEnum (a -> b) => BEnum (Rel a b) where
   enumAll = [ [  (a,b) | a <- enumAll, b <- enumAll, f a b] | f <- enumAll]


Rel (Rel a b) (Rel c d)

type Relator = Rel a b -> Rel (f a) (f' b)

(Rel a b -> Rel c d) -> Rel (ForAll ) (ForAll ) 
(Rel (g a) b -> Rel a (f b))
(Rel (g a) b <->  Rel a (f b))


class Poly reltype a b
    poly :: Rel a b 

instance Poly Bool Bool Bool
   poly = id @Bool @Bool

instance Poly (a -> b)
   poly = reynolds (poly @a) (poly @b)
instance Poly (f a) a' a'', Poly (f b) b' b'') => Poly (ForAll f) (forall a. f a) (forall b. f b) where
   poly = 

(forall a. a a' a'' => a (f a') (f a''))

[(a,b) |  a <- as  ]

data ForAll f where
    Forall :: (forall a. f a) -> Forall f

-- relation map?
(a,b) -> (f a, f b)
Rel a b -> Rel c d
(a,b) -> (c,d)

(forall a a'. Rel a a' -> Rel (f a) (f' a')) ->  Rel (Forall f) (Forall f')

(forall a. BEnum a => BEnum f a)

id <= (reynolds (param) yadaya)
?


data Ty = Arr Ty Ty | Forall (Ty -> Ty) | Unit | Void
interp :: Ty -> Rel a b 


-- type EnumAll = '[Void, Unit, Two, Three, Four, Five, Six, Seven, Eight, Nine]
-- 
BEnum x => All (x : xs)
    all f = [ f a | a <- enumAll] && all @xs f 

type family Iter xs ys where
   Iter (x : xs) (y : ys) = (x,y) : Either x y : (x -> y) <> Iter (x:xs) ys <> Iter xs (y : ys)

I1 = '[Void, Unit]
I2 = I1 : (Iter I1 I1)
I3 = Iter I2 I2

  (In a EnumAll, BEnum g a) => BEnum (Forall g)

~F~ is a the relation function
f is type function
poly :: (Rel a b -> Rel (f a) (f' b)) -> Rel (Forall f) (Forall f')
poly f = [ (ForAll g, ForAll g') |  all  g @a   ]

  [ |  g <- enumAll @(ForAll f) , g'<- enumAll @(ForAll f)  ]

type family ForAll f where
data Exists f =  f Void + f One + f Two + f Three + f Four
data Exists f = E0 (f Void) | E1 (f One) | E2 (f Two) | E3 (f Three) | ... 
data ForAll f = ( f Void , f One , f Two , f Three , f Four)
data ForAll f = F { f0 :: (f Void), f1 :: (f One), f2 :: (f Two) (f Three) (f Four) (f Five) ...}


( ->    )
(Exists )

Rel a b -> Rel (f a) (f' b)
poly f = [     | (a,b) <- EnumAll, f <- enumAll @(ForAll f)   enumAll ]

forall A A'. Rel A A'  

class Specialize (Forall f) a where
    at :: (ForAll f) -> f a

instance Specialize (ForAll f) Void where
   at = f0
instance Specialize (ForAll f) One where
   at = f1
instance Specialize (ForAll f) Two where
   at = f2

extro :: Exists f -> (forall a. f a -> b) -> b 
extro (E0 x) f = f x
extro (E1 x) f = f x
extro (E2 x) f = f x
...

frintro :: (forall a. f a) -> ForAll a
frintro f = F f f f f f f f f f f f f f f

data Exists f where -- true exists
   Exists :: forall a. f a -> Exists f
data ForAll f where -- Those parenthesis make all the difference.
   ForAll :: (forall a. f a) -> ForAll f
-- If the efficicency loss of a newtype is bugging you, hoooo boy are we in for a ride.




-- unsafe opposite? Thought I might want undefined but maybe not. Maybe a typeclass is good enough

frextro :: ForAll f -> (forall a. f a)

class ForAll a where
   extro :: ForAll f -> f a

class Exists a where
   exintro :: f a -> Exists f

class FinType a where   -- TyBEnum
   exintro :: f a -> Exists f
   froutro :: ForAll f -> f a -- this is the specialize above

instance BEnum ForAll 


Free Theorems in Stupid World



type ForAll f = ForallN 10 f

type family ForAll N f where
    ForAll 0 f = ()
    ForAll n f = (f n, ForAll (n-1) f)  
type family 

There is pleasing simplicity to the written out ForAll Exists Though

A compiler that specializes at compile time kind of 
builds these things with defunctionalization? Another reynolds classic


so Forall f can contain stuff that was not built via the safe constructor
frinto :: (forall a. f a) -> ForAll a


It's kind of a typeclass?
But a closed type class.

data Class f = {
   void :: f Void
   unit :: f Unit
   two  :: f One
   three ::
}

data Class c = {   void ::  }

data Eq a = Eq { eq :: a -> a -> Bool }


data BEnum a = BEnum {enumAll :: [a]}
data BEnumC = BEnum { void :: }
reflect :: BEnumC
reflect = BEnumC enumAll enumAll enumAll enumAll enumAll enumAll enumAll enumAll 

EnumAll = '[ , , , , ,]

data ForAll' xs f where
   ForAll :: ForAll' xs f -> f x -> ForAll' (x : xs) f
   Nil :: Forall' [] f
data Exists' xs f where
   Here :: f a -> Exists (a : xs) f
   There :: Exists xs f -> Exists (x : xs) f



type ForAll = ForAll EnumAll
type Exists = Exists EnumAll

-- smart constructor.
class smartForall xs 
instance smartforall xs => smartForall (x : xs) where
    build f = Forall (build f) (f @x)
instance smartforall [] where
    build _ = Nil    

This is becoming an HList. Well. Similar anyhow. The application of f is a little funky
That's good though. If stuff becomes obiouvs and familiar, that was the goal

reflecting parametricity





an euivalnce
a -> a -> Bool -- decidable equivalnce
partially applying this function gives something that is equal to the type in some sense
the canonizer is a little more productive




newtype Diag a = Diag (a -> a)
ForAll Diag

or is_canon

smart ocnsutrctor returns maybe. Only if you already have it in canonical form can you build

typeclasses are a kind of rank n polymorphism

quotient-types
smart constructors with canonizer
data Q n = { canon :: n -> n, val :: n } 
when you pattern match you can assume in canonical form.
canonical means yRx <-> canon x = canon y

quotient types CAN divide the size of a type
ForAll 1 2 3 4 5 5 6  == 1 * 2 * 3 * 4 * 5 ...
But, it is the quotient of this object that is the parametric type built by smart constructors

ForAll 0 1 2 3 4 == 0 * 1 * 2 * 3 * 4 ...
ForAll (0 ^ 0) * (1 ^ 1) * (2 ^ 2) ... - something or 
forall a. = product_quotient
forall a. = (0 ^ 0) * (1 ^ 1) * ()

product over only values that obey the parametricity theorem.
Which is indeed putting the cart weirdly before the horse. but ok
(0 ^ 0) == how many?
(  ^  )

it's like sum vs straight sum
or sum with weird conditions
or product with weird conditions

For a finite type
2. how many values obey the parametricity property whatever the hell it is.
forall a. a

poly :: (Rel a b -> Rel (f a) (f' b)) -> Rel (Forall f) (Forall f')
poly f = [   (g,g')   | (g,g') <- enumAll   , all  (\rel -> (frextro g  , frextro g') `elem` (f rel)  ) allRel]

-- need to enumerate allRel
 [Rel Void Void] , [Rel Void One], [Rel One Void] , Rel One One,  


canon :: X -> Y
canon ~ takes X and divids by Y = X/Y

-- finite sums are calculabe
sum :: [a] -> (a -> a) -> a
-- finite type sums
-- finite type products.
prod :: [a] -> (a -> a) -> a

prod :: [(a,a)] -> (?) -> a

prod :: [a] -> [(a,a)] -> a

exists becomes point free
prod pointfree?


data Prop = PTrue | PFalse

(a -> Prop)



parametricity,

It is surprising how intuitive parametrically polymorphic functions are and yet how confusing it is to precisely state the properties they enjoy

Something having a parametrically polymorphic type means it does "the same thing" regardless of which type you instantiate it with.
if I have a value 
f :: forall a. a -> a
somehow 
f @Int is doing "the same thing" as f @Bool.

But it can't really mean that x @Int is equal to x @Bool can it?

A useful conceptual model for Haskell is that functions are like mathematical functions.
Functions are equal if for equal inputs, they produce equal outputs.

Equals is a soft informal notion. There are many subtly distinct inpretations of what equals means. Even in computer languages, does equals mean contains the same data? Does equals mean that the two variables are literally in the same memory location? 
Can something only be equal to itself? Is isomorphic equal?
Is 1/2 equal to 2/4? They aren't literally the same expression though?

"fst (1,True)" = "1" = "0 + 1"

Whether we want to consider these things equal depends on the perspective we wish to take. If we care about byte equality of the test file holding the code, no these strings aren't equal. If we care about equality in terms of run time execution, these might or might not not be equal depending on how they compile. Is the compiler smart enough to simplify these expressions ahead of time? If we care about just the final result after evaluation, then yes they are equal. 

Quiz time. Are these functions equal?

id1 :: Bool -> Bool
id1 True = True
id1 False = False

id2 :: Bool -> Bool
id2 x = x

id3 :: forall a. a -> a
id3 x = x

Really think about it. Why or why not? If you don't find this confusing, you're not thinking hard enough or you've thought way too hard.



Let's say we want to use equal to mean (==) :: a -> a -> Bool
But saying f @Int and f @Bool are equal doesn't even really type check, does it? It doesn't even make all that sense to ask if a Bool and an Int are equal so how can we compare the inputs or outputs?

For finite types, we can really consider function values to be tables.
|  True  | T/F |
|  False | T/F |

In what sense are two tables of different type equal? It seems crazy.


So there is something a little nebulous and subtle about this notion of somehow being able to consider de parametrized values as being equal.

Let's go down a helpful road.
One generalization of what equality is an equivalence relation, a subset of pairs of tuples such that it is reflexive (R a a), symmetric (R a b implies R b a) and transitive (R a b and R b c implies R a c).

There is no way that a heterogenous relation can be considered reflexive, symmettric, or transitive.

But we could still have values that are related

In world 1, where I'm a blond, I applied f to an Int
In world 2, where I'm a brunette, I applied f to a Bool

When in world 1, I apply f to 3, I apply f to True in world 2.

What should the polymorphic function do?

Well it should take related inputs to related outputs.

forall a. [a]

in world 1, I relate  True with 3


I don't get it.


There are two hats you can put on sqrt(2).
In analysis, we think that this is just a number. Its 1.4something
In algebra, there is a sense in which sqrt(2) is a completely formal symbol.
You might as well replace it by smileyface. 
But with the condition that whenever you see smileyfac^2 you can replace it with 2
It is in this world where qustions like can a polynomial be solved by radicals even makes sense.


Programming languages also have different hats.

Languages are just sets of strings of certain properties - syntax
Languages are just tree data structures with certain properties - syntax
Languages are dynamic systems - Operational semantics
Languages are mathematical objects - denotational Semantics

The actual syntax of polymorph


The naive categorical model of polymorphism:
Sorry about the naive thing everyone.

The naive model is that there are certain patterns we recognize as common categorical constructions

Personally, my intuition runs the opposite direction. I have some intuitive feel for haskell. I think this largely comes from having gone through the motion of implementing bizarre combinators and seeing how mechanical it is.
Honestly, I think Haskell should have a C-c C-a auto like agda does that hooks into djinn. 

The search is based on the following substitutions:

Isomorphisms:
1. a ~ Id a -- The identity functor can be inserted
2. b ~ Const b a -- The constant functor can be inserted
3. a ~ () -> a   -- Units can be inserted
3. Yoneda Lemma. forall b. Functor f => (a -> b ) -> f b ~ f a
4. Coyoneda exists b. (f b, b -> a) ~ f a 
7. ProYoneda 



Higher Rank
forall f. f a -> f b ~   (a ~ b)
{- This is leinitz equality. It is also the eliminator of Refl -}
forall f. Functor f => f a -> f b ~ (a -> b)
forall f. Contra f ""  ~ (b -> a)
forall p. Profunctor p. p a b -> p s t  ~  ( a -> b , b -> a )


3. (a -> b) ~ k a b   (use sparingly. Sometimes -> is morphism, sometimes meta-arrow)

Properties:
5. Natural transfromations forall a. f a -> g a. fmap f . h == h . fmap f 
6. Limit forall a. f a.    
8. Adjunction
9. 




are views isomorphisms?

forall p. p a b -> p s t
two parallel equalities.
Can one do something interesting with that?
Kind of seems like it might have a horizontal composition?
Converse p a b = p b a -- makes more sense
you can braid converse. Doesn't seem like it would get hung up

Higher order equality
forall (forall f. f a -> f b) -> (forall g. g a -> g b)

newtype Eq a b = Eq (forall f. f a -> f b)
p a b -> p s t
p a b c d -> p s t u v 
and so on


two parallel reversible lines

profunctor gives directionality to the lines.

HIT has a constructor for paths
s -> (forall f. f  s' -> f s) -> s  


Giving power actually increases the things inside
Is there anything between functor and nothing?

univalence
univ :: Iso a b -> a :=: b
univ = undefined

class Univ a b
  univ :: Iso a b -> a :=: b
instance Univ a a where
  univ _ = refl

what is space interpetation of parametrciity? Uniformly parametrized space.
symmettry? Parametrcity ~ Noether something something




No concrete types.



In some sense, it feels like an object of type forall a. f a contains values for every possible choice of a.
In this sense, it feels similar to
forall a. f a ~ (f Void, f (), f Bool, f Int, f (Bool -> Bool, ... )
and then I can extract the appropriate value with
@a  ~  getInt

sum (x:xs) f = (f x) + (sum xs f)
sum [] f = 0 
-- guarded sum.
dot (a,b)::xs f | a == b = (f a b) * xs
                | otherwise = dot xs f


When I said that it is bizarre how intuitive that a -> a is the identity function, he said it is obvious because it is the naturality conditon. This is both a good point and a bad point.

There are a number of patterns that have categorical intepretation.

type f ~> g = forall a. f a -> g a - natural transformation
(fmap f) . n == n . (fmap f) -- equation

forall b. (a -> b) -> f b  -- yoneda lemma
-- equations (a -> b) -> f b ~~ f a

forall a. f a -- limit?


forall a. p a a -- coend?




data constructors patterns
forall s. (a -> s) -> (b -> s) -> s ~~ Either a b
via yoneda. ? 


forall s. (a -> b -> s) -> s ~~ (a,b) -- curry and yoneda
((a, b) -> s) -> s == f = Id == Id (a,b)

forall s. (a -> s) -> s == a by yoneda
forall s. s -> s == ( () -> s ) -> s == ()
forall s. s ==?  Limit Id?  


But also via universal properties?
data constructors are unique
Tuple :: a -> b -> Tuple a b
()


type Cone a g = forall b. Const a b -> g b -- natural transformation between constant functor and g

yoneda embedding
a -> b 
(b -> Bool) -> (a -> Bool)  -- there is a way to take a -. b and turn it into this
-- but not vice versa
set to set mappings are not the same as element to element mappings
unless "parametric"

unless profunctorial?

test: take (b -> Bool) -> (a -> Bool). give indicator functions on b.
or give all sets on b. give conjunction on a?















The name of the game then appears to be to take the type you are interested in, and massage it via isomorhisms until it matches one of these forms.
For example a -> a  ~~ Id a -> Id a, then it 

These constructions implicitly are assuming that the containers have functor and profunctor instances whenever necessary




dot [] f = 1

-- delta function form
dot x:xs =  (f a b) ^ (delta a b) * (dot xs f)
Sum ts f 

Prod


A useful set of possible worlds is one in which first you applied a produced the value by applying another function


What precisely does "the same thing" mean though?

Sometimes I like to think of a parametric type as an opaque (not dereferencable) pointer type. That's not a crazy way to implement it, although there are others.




realizability
categorical bounded model checking of petri nets
categorical games via bilevel optimization?

reachset tubes


L x = y
contractive linear operator
x = L x + b

ok. But suppose y is parameters describing region
x <= A y # A is rectangular

x `sub` L x + b

or suppose 
x1 x2 x3  00 0 000 0 0 0
but L takes us out of the 123 subspace.
Taylor model then constricts

polytope
 Al <= x <= Au -- no reason A and b have to be the same
L x + b

component wise inequality seems bad
L _is_ contractive, which means eigenvalues [1,-1]
 
 Seems like I'd want a more basis invaraint set.
 x * x^T <= \sum lA ?
 -- a sphere in 1-d?
ellipse
(x - b)A(x - b) <= eps
SOCP? Yes. Ok. I bite. Well. maybe

Maybe we're doing SOS on quadratic forms.

(Ax + b)^2 <= cx + e -- cone
is inside other cone
->
(Ax + b)^2 

inside cone is always neagtive with outside cone is nagetive
, but only when outside cone is 

l >= 0
cout + l*cin <= 0


xAx <= 1 -- is cout
(Lx + b)A(Lx + b) <= 1

Or.
Maximize cone 2 constraint under cone1 constraint.



when   +  == 




(L x + b) * (x L + b) <= A
x x <= L-1 A L-1 - bb 
more restrictive ----~> L-1 A L-1 <= A

Or that we could derive one from the other.
 (x x - A) = 
Or the power method.


ls power
solar power
40W megawatt
s calif 18 months

battery storage 130MW

finance backgroudn
credit analyst in new york city
venezulan oil project
wife senda?
solar rnewable energy

boss pioneered electrical
texas hawaii utiliy scale battery

well capitalized - we hsould have charged more
parallel team
backtest function

efficiency factors and caharging
weird mehcanics of market
high volatitllty in real time energyby hour
two hours before the hour
bids
1 hour duration - full power
inverter out cpaacity
1C batteries
solid RY
High Power, not high cpaacity

83% inefficiency
slower to charge than 
Califronia indenpdent system operator

regoinal transmission operator

FRCAST
Team comparison

hdf
sql query
1gb

comparisonopen access
forecast
actual dataweather
outage
renewable forecasts
prices
ancillary services
1 day before

demand forecast
fuel prices
outage
renewable forecast
path 15

south cali is different from north application

gas power plants have intepretation
most spikes in evening

hour of day matters

month of year

wear on batteries
5 minutes

straightnowicki is peretty good

midnight hour
peakers

40MW gas turbine
 minimal turns off


power industry
fall scheudled maintanenace

static strategy
bewlo 60 above 90

leaving them charged is bad for th battery

where od actual 
prices come form

mojave desert gas

methane leak 
in claifrnoai
alsi ocnayon gas leak

sand diego
trading hub sp15












some obvious :
 renewable energy - cloudy energy thunderstorm
 larger than usual outage

 kiso ? 








250MW 

battery in south australia 130MW
450MW






bap ../resources/sample_binaries/return_argc/main --pass=wp --wp-compare=false --wp-function=main --wp-check-calls=false --wp-inline= --wp-postcond="(assert (= RAX #x0000000000000000))"  --wp-precond="(assert true)"

plus :: a -> a -> a
[(,),]

	RCX  |->  #x0000000000000000
	RAX  |->  #x0000000000000000
	PF   |->  #b0
	OF   |->  #b0
	CF   |->  #b0
	AF   |->  #b0
	mem_orig |-> [
		#x00000000005d9b08 |-> #x01 ;
		else |-> #x00]
	mem_mod |-> [
		#x00000000005d9b08 |-> #x01 ;
		#x000000000000000b |-> #x80 ;
		else |-> #x00]


data Prod xs f where -- same as forall
  Prod
  Done



data Sum




Sorted a = 
Inductive sorted a := {  x : list a ;  pf : sort x = x }

smart_con x = {  sort x  ; pf : sort_idem  }
smartdecon f { x ; pf} = match  

sorted = 
recsort :: (a -> Sorted  ->  b) -> [] -> 


data Reflect a
   dict :: k a => k b



but the stuff that is is parametric.
What if we threw template haskell in the mix.
Code (f a) 
Then we have a specializer?







 Enum (f Void), Enum (f One) ...    => BEnum  (Exists f)
 Enum (f Void), Enum (f One) ...    => BEnum  (ForAll f)



   Specialize (f) where


-- A BOLD CHOICE INDEED

Or the exiestential one


Rel (f Void or f One or f Two or f Three) ()


BEnum a, Eq b => Eq (a -> b)

ForAll f = '(f Void, f One, f Two, f Three)

f Void + (f One) + (f Two) + (f Three) + (f Four) + 

value semantics
(a -> b) -> a -> b

set semantics
(a -> b) -> Set a -> Set b

relational semnatics
  (a -> b) -> Rel a a' -> Rel b b'  -- but  it is possible to apply (a -> b) to both a and a'


relational semantics
(a -> b) -> Rel a b

x -> Rel x x


(forall a. a -> a) -> (Rel a a')


using up the context?

linear logic?



forall a. a 
/\a. a - zero
/\a. a -> a = 1


Void is 0
? -> Void
Void -> (Void -> Void) = Two I could buy that



forall a -> a == 1  ... 0 ^ 0 = 1, sure..... maybe
forall s. (a -> s) -> s
lim-> 0 s ^ (s ^ a) = a? I highly doubt it limit as a binding form. that's another one.
I guess I usually consider lim to basically be susbtitution.

a -> b = b ^ a
(a -> b) -> c should iterated -> still be expoentntiaion?
what if you could only evaluate once

a "polymorphic" number function should not depend on the value of s


l'hopital's rule


forall a b. (a,b) -> b
y^(x*y) == y = 1  x = anything





forall s. (a -> b -> s) -> s ~ (a,b)

~ (a -> b -> -) -> Id -  ~~  Nat (a -> b -> -) Id 
type P a b s t = (a -> b -> s) -> t
Tuple a b = End (P a b)

forall s. (a -> s) -> (b -> s) -> s ~ Either a b
R a b s t = (a -> s) -> (b -> s) -> t
End (R a b)  ~ Either

? WHy not coend?

data CoEnd p where
    CoEnd :: p s s -> CoEnd p

data Exists f where
    Exists :: forall a. f a -> Exists f


Exists ()
exists s. s -> (a -> s, b -> s)
exists s. (a -> s) -> (b -> s) -> s
exists (s, s -> a, s -> b) ~ (a,b)
exists Either (s, s -> a) (s, s -> b)



Rel (Set a) (Set b)
forall b in set1, exists c in set2? -> Bool lattice
False <= True

vs
forall b in set1 all set2

exists b in all a


relation style pro / bi functor

set style functor
forall x in s -> True / False
forall x vs exists. Does that make it covaraint vs contravariant

Sets of sets. 
Set (Set a) -> union 
objects are sets.


vs exists b set1 exists c set2

~ (a,b)


proof relevant relations
set x set -> settttt?

forall x. (a -> x) -> f x

if you shrink, forall remains true
if you grow it can become false
S1 sub S2 ->  p(s2) <= p(s1)

FSet a :: Set a -> Bool
ESet a :: Set a -> Bool

forall :: Set a -> FSet a
forall s = \s2 -> all (\e -> e elem s) s2  

exists :: Set a -> ESet a
exists s = \s2 -> exists (\e -> e elem s) s2


subset x y
forall == partially applied subset
exists = partially applied subset with not? 
Rel = (FSet, ESet) -> Bool
Rel a b = Set a -> Set b -> Bool
Rel a b = FSet a -> ESet b -> Bool

And does this help?


Rel a b = [(a,b)]
Rel a b -> HRel a b  
We can lift rel into this higher relation.
And if HRel obeys the laws of profunctors we can lower?

covraint funcor
forall x. (a -> x) -> f x = f a
exists x. (x -> a , f x) = f a 

contravariant functor
forall x. (x -> a) -> f x =  

exists x (p a x, x -> b) ~ p a b
exists x. (p x a, b -> x ) ??
forall x. (b -> x) -> p a x ~ p a b

Set a -> Set (Set b)

shortest path on graph. -> Nat as path distance
most distant point
closest point
longest path
shortest path

r a a is a graph.
r a a means is reachable
R Set a Set a. Two sets of points

average distance? 
Shortest distance
maximum minimimum distance

if we take subset, minimum distance can only get longer
if we take subset worst distance can only get smaller.

dual quantifiers.

sets in the plane with metric. Blobs
Worst and best distance. Easy to comppppppute?
closest points is easy.
farthest points feels like it ought to be easy...
No image they actually interesect each other. then it sucks.
best dual planes?


what about forall ~ greatest common divisor
min roughly.
min is a underestimate?

gcd_all_n(s -> s) = s^s = 1?
froall s (a -> s) -> s  ~ a


Set () ~ Bool
Set a -> Set b
Set a -> Bool


forall s. (a -> s) -> ConstId b s

ye, i dunno. not that helpful

s^(s^a) ~ a

a^p mod p = a



1 ^ (1 ^ a) ~
0 ^ (0 ^ a)
2 ^ () only has twos

number thoery does weird stuff to expoentiteal

some number theory shit.
Interpety type -> n mod n
Then expoentnial don't get out of control
over all bases?




I'm using brain pathways that I developed as part of convex set stuff
What was that about? It was like a binary operation on convex sets or something
Shortest path thing is geometry. Graph paths do obey triangle inequalities


s -> (a,b) ~ (s -> a, s -> b)
(a + b -> s) ~ (a -> s, b -> s)

kronecker product lens
s -> kron a c
kron a c -> t 



(->) :: Set a -> Set b -> Set (a -> b)?


class Set a 
    interp = benum

class Set

newtype ForAll f = ForAll (forall a. f a)


instance (ctx :: a) (f 0) => Set ctx (Forall f) where
    interp = \s -> 


lens as 


Intuition 1. 
Anywhere you see, function, try relation
Intuition 2.
Anywhere you see set, try relation



(arr   )
 

Types are sets. Specifically, we usually interpret types as sets of the values they can take on

BEnum (a -> b)

ForAll Id == ???

newtype P a = P [a]

power (s:ss) = [ [ s:s' , s']   | s' <- power ss ]
power = Interp @(a -> Bool)
ForAll set = PowerSet a

Arrow as a binary operation on sets.
PArrow {- partial functions -}
NDArrow {-  nondeterminstic functions -}



Preservation is the hard theorem
Abstraction is the theorem for free


Power as an operation on set.

Arrow as a binary operation on relations
Power as an operation on 


if you shrink exists it can go false
if you grow, it can't
S1 sub S2


850
catpy
yoneda embedding of dsls into functional form?

def yoneda(f):
    return lambda g : compose(f,g)
class Category():
    def I(n):



# no, graph generators
def id(n):
    def res():
        a = [simplenode() ]
        b = [simplenode() ]
        edges = zip(a,b)
        graph(a + b, edges)
        return a, b, g
    return res
def compose()


def pair()


FreeCategory
FreeMonoidalCategory
FreeCartesianCategory

SymPyCat

Compose(Id, )

rewrite() -> all the laws.
Can sympy do a small amount of search?
Or maintain a population, so simulated annealing or something


do category theory with z3py





FreeSympy

def proj1
def proj2


class Graph():
    self.G = graph()



#dependency injection stye / finally tagless / module?


def my_program(C):


parsing
parsing is a reconstruction problem
One can write a collapsing function easily enough (serialization)
(desrialization)
The collpasing function is allowed to fill in arbitrary bullshit too


Monadic parser combinators use the composition of Search (via list) and state
(state of unread input)

BogoParse. Enumerate all parse trees, see which ones fit

parse validation - Given parse and input, is it easy to see if it works?



maybe z3 is missing inductive relations

Function("r", r  )


inductiverel(    )




modules
short exact seqeucne - analog of V-Rep H-Rep
two linear maps
The exactness property is a generalization of an orthogonality property

fixful form of yoneda.

Sharefix

It is a thing, sometimes useful, to factor out the recrusion from a function definition

fact 0 = 1
fact n = n * fact (n - 1)

facf f 0 = 1
facf f n = n * f (n - 1)

fact = fix facf

fix :: (a -> a) -> a
fix f = f (fix f) -- ?


Why can this be useful? Well, when you're worried about non-termination, paying attention to the explcit recursion is useful. 
Another reason is that you can memo-ize your functions if there are usefully shared subproblems

fib 0 = 1
fib 1 = 1
fib n = fib (n - 1) + fib (n - 2)

fibf f 0 = 1
fibf f 1 = 1
fibf f n = f (n - 1) + f (n - 2)

fib = fix fibf


memo :: (a -> a) -> a
memo f = helper Map.empty f
    where helper d x f | x in d = lookup d x
                       | otherwise 

Very similarly, it can be useful to factor out the recrusive bit from a recursively defined algerbaic data type

data List a = Cons a (List a) | Nil

data Fix f where
    Fix :: f (Fix f) -> Fix f

data ListF a b = Cons a b | Nil
type List a = Fix (ListF a)



Why do we want to do this? One road leads down to recursion schemes and the algerba of programming.
However, a more mundane reason is it let's use factor out sharing

data Memo f = Memo (f Int) (Map.IntMap Int (f Int))

Haskell already allows sharing because of immutabilty. However the sharing can't be easily observed or preserved.



Also union find.
What is the analog of union find for value level fix?
would it let you do unification inference?
is union find some kind of dual to memo?

union f = fresh f x

fixM = (a -> m a) -> m a
fixM f = fixM f >>= f

       Map.lookup

V2' ~ Compose V2 
However then we can specialize
data V2' f a = V2Same (f a) | V2Diff (f a) (f a)

V2' V2' V2' V2' V2' Id b

Functor f => Functor (V2' f) where
   fmap (V2Same x) = V2Same (fmap x)

instance Applicative f => Applicative (V2' f)
   pure = V2Same (pure)
   (V2Same f) <*> (V2Same x) = V2Same (f <*> x)
   (V2Same f) <*> (V2Diff x y) = V2Diff (f <*> x) (f <*> y)
   (V2Diff f g) <*> (V2Same x) = V2Diff (f <*> x) (g <*> x)  
   (V2Diff f g) <*> (V2Diff x y ) = V2Diff (f <*> x) (g <*> y)

-- monad transformer?
instance Monad f => Monad (V2' f) where


Heterogenous sharing
Compose f g is one layer of Fix.
Compose f f -> C f (C f f) -> C and so on.

data MemoComp f g = MemoComp (f Int) (Map Int (g a)) 

plus perhaps other consturctors for common cases
-- all of the g correspond
data MemoComp f g = All (f ()) (g a) | Memo 

For BDD we have to follow this chain, which sucks.
It's a Vec n rpbolem

-- This is also iteratee.
type MultiFun a b = More (a -> MultiFun a b) | Done b
==
data MultiFunF a b c = More (a -> c) | Done b
type MultiFun a b = Fix (MultiFunF a b) 

MultiFun ~ [a] -> b



data MemoComp   f () ( Map (Rep f) (g a))
[  (g a), [Rep f ]  ] -- all indices.





modules
Quotients
Hrep, Vrep = you can build the wuotitent of two linear subspaces.
v / w ~ stacking a vrep and hrep?
v / w is isomorphic to v where the dot onto any element of w is zero.
That picks a cnannical representative of the equivalence class.
So, yeah. It is v's hrep stacked with w vrep as a hrep

If you have v vrep, then just project out the w compoenent using it's vrep

v / w = v - w w^T v

Using w's hrep, not clear.


Relational division.
I suspect we'll get projection coefficients from dotting the mathcing bits.

The sum is the join. It is conjoining generators

Of course, we could also choose to use a free quotient representation
like fractions



Ye. The all in one policy training where we use a linear parametrization
It can't depend on the state of the battery, because the battery is not just fixed data, it is computed.
And yet, it is computed based on the previous input (basically), so it is an implicit function of the input.
So it's still not really a feedback controller

It's an open loop plan as a function of initial conditions then at least.

Resample from seen positions. Recurse.
Ultimately only use the shortest time step part.
Unless maybe you have sitty observations, in which case, it nicely tells you what to do open loop for a while. Hmm. 

The initial condition is fixed. We could try a bunch of initial conditions
Then have it open loop as a function of guessed dynamics.

If we have time translation invaraince, we could then use

Not clear this is any better than a recursive training loop.
Give it a bunch of initial conditions. First time step only 
Allow trajectory to optimal open loop afetrward.

We could have the thing have a function trying to explicitly predict where it will be. And we flip that in on iteration.





ConvexCat
We could have lazy set mechanisms
Or ADMM mechanisms

Passing back the optimum function


inx, action, outx, cost

--[  ]--



open dynamical systesm, trajectories and hierarchical planning
Grothendieck construction
open dynamical systems are functions of both their state and parameters.
We want to include both stochastic, Nondet, and det dynamics.



meeting notes
1 I need to just use categorical notation
2. adjunctiona
3. presheaves. imolication in kripke models.
((a -> b), f b) -> g b

Yoneda and set
Presheaf let's you abose yoneda
so does profunctor I guess

Presheaves are a CCC
type Unit a = Const () a
type Prod f g a = Prod (f a) (g a)
type Compose f g a = Compose (f (g a))
type Arr f g a =? forall b. f b -> g b  - no slot for a? feels wrong
type Arr f g a = forall b. (a -> b, f b) -> g b
(a -> b, b -> c) -> (b -> d)
~ (c -> d, a) or just a raw const d NOOOOOOOOOOOOOOOO
c -> d we can do it.
(a -> b, Const c b) -> Const b d
I thing this is basically c -> d





jsonette. 
inheritance. instantiate b < a => (b -> a -> a) -> b -> a, and a compose looking this
open recursion schemes.
These are object oriented ideas


a product of two objects is a third object and 2 morphisms p1 p2 such that forall f g, exists <f,g> such that p1 . <f,g> = f and vice versa

triple products exist?

And adjunction F -| G is two functors such that
Hom(F a, b) ~ Hom(a, G b) 


Adjoints + Yoneda + understanding the set
del
Hom_C( a , x(b, c) ) == Hom_cxc( dup a,  (b,c) ) ~ adjoint
                    ~~ Hom_c(a,b) , Hom_c(a,c) -- via definition of product cateogry
                    ~~ 
right, cross is a functor if (b,c) is an object

type Product k k' a b = Product (k (Fst a) (Fst b)), (k' (Snd a) (Snd b)) 

data Product k k' a b where
     Product :: k a1 b1 -> k' a2 b2 -> Product k k' '(a1,a2) '(b1,b2)

type Family Doub -- ok doesn't need to be a type family
    Double a = (a,a) 
type Doub a = '(a,a)
doub :: k a b -> Product k k (Doub a) (Doub a)
type Family Cross
    Cross '(a,a) = (a,a)
-- if cartians we can do it
-- Cartesian k =>

-- or this can be the definition of cartesian
to :: forall a b. k a (Cross b) -> (Product k k) (Doub a) b
from :: forall a b. Product k k (Doub a) b -> k a (Cross b)



'(a,a) as external product, (a,a) as internal

Define a functor Cross such that it is right adjoint to Doub.
That is an acceptable presentation



Yoneda ::
to :: Category k => k a b -> (k s a -> k s b)
to k = k . a
from :: (forall s. k s a -> k s b ) -> k a b
from f = f id


Nat(Hom(a,-), Hom(b,-)) ~~ Hom(b,a)
Nat(Hom(-,a), Hom(b,-)) ~~ Hom(a,b)

Nat( Hom(-,a) , F -) ~ F a
why is this true?
the set of natty trans, is indexed by object


k (f :: a -> *) (g :: a -> *)
where f and g are contravaiant functors

a categroy of presheaves

PreSheaveCCC a = a -> FreeCCC ?
data PreSheafCCC a where
   Id
   Comp
   Fst
   Snd
   Pure :: (a -> PreSeasfCCC?) -> PreSheafCCC 

Functor f where
   fmap :: k a b -> k' (f a) (f b) 
Contra f where
   cmap :: k a b -> k' (f b) (f a)


type Nat k f g = forall a. k (f a) (g a)
Nat (->) (k - a) g = forall b. k b a -> g b


if g is a functor into "set"/->
Functor g k (->) where
   fmap :: k a b -> (g a -> g b)

Functor g where
   type dom = a -> a -> *
   type cod = b -> b -> *
   fmap :: Obj a, Obj b  => 


g a is supposed to live in set

yoneda lemms:
(forall b. (k b a) -> g b) ~~ (g a)


yoneda lemma seems to cross value boundaries


yoneda embedding
k a b -> k s a -> k s b


Iso (a,b) c ~~ (Iso a c), (Iso b c) ??? No.
a -> b,c ~~ a -> b), 9a -> b)

Iso ((a,b) -> c) (a -> c, b -> c)

double

-- do I need to be explicit with 
class Category k where
   type Obj o :: Constraint
   compose :: Objn k '[a, b, c] => k a b -> k b c -> k a c
   id :: Obj a => k a a


class Cartesian k where
 type Prod '(a,b) :: * -- maybe data?
 adjoint :: Iso (k c (Prod ab)) (Product k k (c,c) ab)

class Cartesian k => Closed k where
 type Exp :: 
 adjoint :: ( )  ~~  ()






newtype Nat k f g = Nat (forall a. k (f a) (f b) -> k (g a) (g b))

yonedaembed 


35904
40843

id @(k a b) 
=@ @(k    )
=@  



(=@) :: forall c. Iso a b -> Iso b c -> Iso a c

coerceiso = { to = coerce, from =  coerce }

yoneda ::  Nat  ~~

=@ @(k a b) iso

Presheaves internal arrows have topological intuition



Presheaves are predicates
predicates with structure.
predicates respect inclusion.
respect inclusion.

a functor for sets under inclusion -> Sets of predicate functions

morphism mapping
inlcusion -> Functions between predicate function

a set gives 

Ordering relations



a, p -> adot
a -> p

iso
Profunctor p => p a b -> p c d

Internal lens?
like => for presheaf?


dialectica vs skolem
skolen - pick a function
dialectica - pick a relation

a(x,y)
pick a relation

alternating quantifier:
R . (Q / X) . (P / Z)
How do you skolemize this?
Go higher order.
exsist s. f s
(forall s. f s -> z) -> z 

forall x. (exists y. (p x y)
forall x. (forall s. (p x s) -> r) -> r

forall x exists y forall u exist v

forall x exists y forall u exists v =>

-- double skolem
exists y(x) v(x, u) forall x u  

-- but is double skolem dialectica? I kind of doubt it


forall x , exists y v(u) exists u

exists x forall y exists u forall v

exists x, u(y,x) forall y,  

exists Lens, forall i o.

forall exists

forall x in A exists x in B 




2-Vect from this new Nullspace persepctive
As embeddings in bigger spaces.

dialectirca 
Turns  RankNTypes into a single exists forall

exists x .forall y. p x y

data DiaLect p = Dialect :: (forall y. p x y) -> Dia p

newtype Ignore z x y  = Ignore z -- Atomic
newtype Forall f x y = Forall (f y)
newtype Exists f x y = Exists (f x)

data Atomic z x y where
     Atomic :: z -> Atomic z x y

data A x y where
    A :: y -> A x y
data E x y where
    E :: x -> A x y




(Int,Bool) ~~ Dia (Conj (Atomic Int) (Atomic Bool))
Either Int Bool ~~ Dia (Disj (Atomic Int) (Atomic Bool))



Replacing higher rank types with higher kind types

forall a. (a -> a) ~~ Dia (ForAll (Impl (  A )  ( A )))
forall s. (s -> s) -> s  ~~  ForAll (Impl (Impl A A) A)  


data Typ a where
   Tup :: Typ a -> Typ b -> Typ (a,b)

data Conj p q x y where
    Conj :: p x y -> q u v -> Conj p q '(x,u) '(y,v)
data Disj p q x y where
   Left :: p x y  -> Disj p q '(x,u) '(y,v)  
   Right :: q u v -> Disj p q '(x,u) '(y,v)
-- Perhaps the chruch from (p x y -> z) -> (q u v -> z) -> 
-- z -> (z -> )
data Exists p x y where -- = p
     Exists :: p '(x,z) y -> Exists '(x,z) y
data ForAll p x y where
     ForAll :: p '(x z, q) '(z,y) -> ForAll '(x, q) '(z,y)


data UnProxy1 p x y where
     UnProxy1 :: p x y -> UnProxy1 p (Proxy x) y

coerce?

cps all parametrs
a <==> (a -> *) -> *

data DoIt p cx cy 


run
if i wanted t ot do this point free, i'd need  typelevel lenses



-- and of course Implies must be the complicated one.
-- since implication screws with forall.
data Implies p q x y where
     Implies :: (p x (y x v) -> q (u x) v) -> Implies p q '(u,y) '(v,x)


Since we don't have type level functions, are we screwed?
Especially since the things we can write are data types
which always have to end


Exists p x y 
   Exists :: p z x y -> Exists p '(z,x) y
   ForAll :: p (x z) '(y , z) -> 



      ForAll :: f z x y -> (z -> x) -> ForAll x (z,)
      ForAll :: f (z -> x) (y,z) -> ForAll x y
      


-- I guess exists is witness to that the first parameter has form '()



 -> (x -> u)


game semantics - logic is a dsl for describinig games
we can takl bout equivalent games in sense like 



data Dia p x y where
    ForAll ::
    Exists :: 
    Arr ::
    Disj ::
    Conj ::




data Exists f x y where
      Exists :: f (x,z) y -> z -> Exists f x y
      

data Implies p q x y where
   Implies ::  (p x (x -> v -> y) -> q (x -> u) v) -> Implies p q (x,u) (y,v) 
   Implies :: (p x v -> q u w) -> (Lens s v u w) -> Implies (x,u) (y,v)
   Implies :: (p (y x v) u -> q   )  -> Implies p q '(u,y) '(v,x)




(forall x. x -> x) -> y

y = Const y () ()



forall x. exists y. p x y
exists y. (x -> y) , forall x. p x y
probably not. Maybe if p has some functoiral power

-- How about this? Value level logic
data Form a where
   Exists :: (e -> Form es) -> Form (e : es) as
   ForAll :: (a -> Form xs ) -> Form (a : xs)
   Drop :: Form xs -> Form x : xs
   Disj :: Form -> Form -> Form
   Conj :: Form -> Form -> Form
   Impl :: Form -> Form -> Form
   Atom :: Bool

interp (ForAll f) = forall f (\x -> interp (f x))
forall f = all f enumAll
exists f = exsits f enumAll
dialect
Lens s t a b

dialectinterp :: Form xs -> Lens s t a b ???
dialetcinterp (ForAll f) = \x y -> 
f implies g

this is like that 

instance BEnum s t a b => BEnum (Lens s t a b) where
   



data Exists f where
   Exists :: f x -> Exists f
data Forall f where
   ForAll :: (forall a. f a) -> ForAll f

data Exists1 f y where
   Exists1 :: f x y -> Exists1 f y

data ForAll f y where
   Exists1 :: (forall x. f x y) -> Exists1 f y

Forall (Exists1 p) ~ Exists (ForAll )

So these things shouldn't be functions?

forall x. exists y. p x y
exists f . forall x. p x (f x)
Exists ForAll1 Flip Compose p))

newtype Flup p x f = Flup (p x (f x))
Exists (ForAll1 (Flup p)) ~ ForAll1 (Exists p)

to :: Exists (ForAll1 (Flup p))
to (Exists (ForAll1 (Flup x))) = ForAll (Exists x)


forall x, exists y.  (p x y)  <-> exists f = \y -> p x y  


from :: 

where exists 

4. (âz Ï(z))D = âX âz, y ÏD(X(z), y, z).
5. (âz Ï(z))D = âz, x ây ÏD(x, y, z).
6. (Ï â Ï)
D = âU, Y âx, v (ÏD(x, Y (x, v)) â ÏD(U(x), v)).


exists c. forall a b. {to :: a -> (c,b) , :: from (c,b) -> a }


tuple
( a -> b -> s) -> s

either (a -> )


list 
s -> ()


(forall s. s -> s) -> 

getting rid of existentials is double negation translation
kind of. Maybe



Lens?


Ok. Carefully how does double negation work?
you embded classical logic into intuitionisitc.
From 



contact emily pillmore

      <-            <-   zdot, 1 <-   1
(x,y,z,t) -> (y,z,t) -> (z,t) -> t

params.
COnsider leap frogging




combinator 
tok -> option b -> Machine st tok b



data Fan x y
data 

data FreeCat a b where
    


'Par
'Fst
'Fst
Pure :: f a b -> FreeCat f a b

type family Apply f x where
   Apply 'Fst '(x,y) = x
   Apply 'Snd '(x,y) = y
   Apply ('Fan f g) x = '(Apply f x, Apply g x)
   Apply 'Inj1 x = 'Left x
   Apply 'Inj2 x = 'Right x
   Apply ('Split f g) ('Left x) = Apply f x
   Apply ('Split f g) ('Right x) = Apply g x 
   Apply 'Apply '(f,x) = Apply f x
   Apply ('Curry f) x = 'Part f x
   Apply ('Part f x) y = Apply f '(x,y)
   Apply ('Uncurry f) '(x,y) = Apply (Apply f x) y
   Apply ('Par f g) '(x,y) = '(Apply f x, Apply g y)
   Apply ('Unit b) '() = b
   Apply ('Cata f) ('Fix x) = f 
   


data Nat f g x where
   Natf :: f a -> Nat f g 1
   Natg :: g a -> Nat f g 0

forall x. Nat f g x


data State a where
    Tup :: State a -> State b -> State (a,b)
    Pure :: a -> State a
    Vector :: Vector Double -> State (Vector Double)

Lens where
   Lens :: (a -> (State c, b)) -> ( b, State c -> a) -> Lens a b

Not a pure existtential, but a GADTized one that we can investiagate

Why do I need Tup?
I need tup so that I can rediscover the type later. if I want to probe it


compose (Lens to from) (Lens to from) = Lens (\x ->  let (s, y) = f x
          let (s', z) = g y
          in (Tup s s', z)

          ( \case (Tup(  , )))


We avoid explicit state passing by using closures.

We can return to explciit state passing by 


 {st : Type} `{ReadWrite json_tok st} : Machine st json_tok json_val 



(forall a. a -> a) -> ()



newtype L a = L (a -> a)
newtype ForAll f = ForAll (forall a. f a)
Forall L


forall a.  a -> a
 a -> a
 a -> a
 a -> a 
 

(forall a. a -> a) -> a
forall f. exists. g (f a -> g a) -> ()



data Typ a where
   Typ :: a -> b -> Typ (a,b)
   Arr :: (a -> b) -> Type (a -> b)

I should make a bp about mip_table


Sometimes during MIP modelling it is useful to have discrete functions available for the binary variables.
Some of them one knows how to write
cvx.sum(x) <= 1 means that at most 1 variables is true
cvx.sum(x) <= n at most n variables
cvx.sum(x) == 1 exactly n variabes are true

x <= y  is   (x implies y)
x + y <= 1    (x nand y))
x + y >= 0.5  (x )    


But sometimes they take a lot of fiddling and it is pretty confusing.


If you have a truth table of a function it is striaghtforward to directly translation this into a mip. The mip is described as the convex hull of the columns of the table.
We can then choose to binarize either the lmabda parameters or the final paramters

table = np.array(  [[0, 1, 0], [1,0,0], [0,0,0], [1,1,1] ])
l = cvx.Variable( rows , boolean = true)
x = cvx.Variable( N , boolean = true)
constraints += [cvx.sum(lambda) == 1]
constraints += [l @ table  == x ]


Iterative laplace equation with bounds.
Lipschitz constants are also.

Bounds on value and derivatives. They are interconnected of course

forall a b
\int a b dx `sub` dphi(x) - phi(b) and vice versa?
  phi(a) - phi(b) `sub` \int a b dphi(x)    



categroy theory as syntehtic homotopy
maht
I = 
a -> <- b is a mockery of the real line, like cubical types.

functors are then parametrized paths? Well you don't really get the intermediate points

a box  is  IxI?

Maybe a box isn't I x I

A square is IxI ? What is the face then? The face is a natural transformation F => G

ul
lu

A sphere would have 2 natural transformations?
Or use a cube?

bool -> bool


'True -> 'False
'False -> 'True
'True -> 'True

data PrimPath a b where
  Left :: PrimPath 'True 'False
  Right :: PrimPath 'False 'True
  IdTrue :: PrimPath 'True 'True
  IdFalse :: PrimPath 'False 'False

-- free paths

data Gen k a b where
  Nil :: Gen k a a
  Cons :: k b c -> Gen k a b -> Gen k a c

instance Category PrimPath where
  Left . Right = IdRight
  Right . Left = IdLeft
  IdRight . x = x
  x . IdRight = x



Gen k a b -> k a b

data Product k k' a b where
type k ** k' = Product k k'


sum


data Line a b where
   Right :: Line a ('S a)
   Left :: Line ('S a) a
   Id :: Line a a

type Plane = Line ** Line

data Square a b where
   Square :: Iso (Square '('False,'False) ('Up ,'Up)) (Square a b) -> Square a b -- ?
   Up :: Square '(a,'False) '(a, 'True)
   Down :: 
   Left 
   Right


data SquareEq
   P1 :: Square ('Cons 'Up 'Right) ('Cons 'Left 'Up)
   P2 :: Square
   P3 ::  


\t -> V2 (cos t) (sin t)
\t -> V2 1 0
compose f g = \t -> if t <= .5 then f (2 * t) else g (2 * t - 1)

Homotopy x = Double -> x

data Homotopy x = Homotopy {x0 :: x, x1 :: x, f :: Double -> x}
-- runtime check that points match
compose2 :: Homotopy (Homotopy x)
compose2 f g = \s -> (\t -> (f t) 

{ f : Double -> x | f 0 = x0 /\ f 1 = x1  }



type Homology k = [(k, b)]

gluing? on a point. Cup prouct?
data Glue p k k' a b where
    Glue :: k p b -> Glue p k k' a p -> Glue p k k' p a b
    Lift1 :: 
    Lift2 :: 


Circle - 1 point

data CircPath a b where
   Forward :: CircPath '() '()
   Backward :: CircPath '() '()
   Id :: CircPath a a

-- Gen can give a free compose and free Id
type Circ a b = Gen CircPath a b

data Equiv 


chloe : compare subs on all substrackign statistcis on stuff


example of finding something real
get countermondel

compiler bug in gcc
some patch that introduced in anything
compiler fuzzer thing - csmith



Bayes rules

Gaussian compostion

Schur complements
-- It HAS to be positive semidefinite. Or else integrating a gaussin is ridiculous. Uh, except for imaginary time...
-- It'd be nice / vital to include delta functions.

A * B = Add? Together

Gaussian composition with offset


phi^4 theory monte carlo

p(z|y)p(y|x)

storing sigma makes sense
1/sigma2 + 1/sigma2 -- :(

storing partially inverted?
Inf | Zero | Sig matrix

l-hopital's rule + AD?

(x-y)^2/inf is delta

reformulate as relations?
p(x,y) * p(y,z) / p(y)  

Relations don't have a scale parameter.

put in a constant sigma scale?


give talk
move stuff to appendix 
* 


graded rings and filtrations
taylor series
exact co-chain - taylor series truncation gives best lower taylor series
Likewise for coureir series.
Limit of chain = limit of function (formal power series)
Maximal ideals and prime ideals
schemes
R[x,y]/(x^2 + y^2 - 1) / I / I^2
Wait, am I convinced what I was doing is wrong?
This I squared notion is nice.

gives first derivative

nulstullensatz is conenctedto Ideal <=> variety conversion theorems
This loses multpilciity
Somerhow schemes don't


Jets maybe?
Germs
Commutiative algerba
filtration


What is the concrete representation of these things.
mod (x^2, y^2 ,etc)

Loclization - adding in all fractions except those zero.
The only prime ideal becomes the point itself
primt ideals are points


Gaussian = A + power series outside.

Composition = multiply and integrate.

Pure gaussian
sum of gaussian -- tracks discreteness
Gaussian * series
sum of gaussian * series


Pure gaussian case first.

This sounds kind of fun
e^phi^2 + e^phi^4



Legendre 

It is a linear relational composition.

p(x|y) 




Homology 


Homology =  Vec k (Edges)
type Homology = Vec k (Faces)


boundarymap = 

-- Path as free groupoid over edge generators

data Path k a b where
   Cons :: k b c -> Path a b -> Path a c
   Inv :: Path a b -> Path b a
   Id :: Path a a


data V where
   V0 :: V
   V1 :: V
   V2 :: V

dats Edge (a :: V) (b :: V) where
   E1 :: Edge 'V0 'V1
   ...

data Face (a :: Path Edge) (b :: Path Edge) where
   F1 :: Face ('Comp 'E1 'E2) ('Comp 'E3 'E4)
   F2 :: Face ('E1, 'E2) '('E3, 'E4)
   

-- tough crowd, al these indices blows.


VCompose :: S f u q l r -> S f q d l' r' -> S u d ('Comp l l')) ('Comp r r')
HCompose :: 
Id :: S f 'Id 'Id 'Id 'Id 
PureF :: f u d l r -> S f u d l r



-- faces have 2-composition.

data Surface f p p' where
   Pure :: f p p'
   VCompose :: Surf (Comp k k') (Comp k'' k'') -> Surf (Comp)
      -> Surface (Comp k k' ) ()
   HCompose :: 

Path k a b -> Vec a k


Functions do not intrinically have inverse. We need to carry them around
yoneda embedding for topology/paths
Iso (Iso s a) (Iso s b)

Given a path fro anywhere to a you can make a path from anywhere to b.

Higher Iso


The Free



data Circle = B0

data CircEdge where
   E :: Circ 'B0 'B0

Path Circ


The difference between circ and I is that I has 2 points?

Free Over Void  edge is just a single point

Path Void ==  a single point


Lifting to winding number

Ints

data IntEdge a b where
   E :: IntEdge a (1 + a)

different from NatEdge, which is one id3d


diff :: Path a b -> [(Int, Vertex)]
diff E = [(1,V0), (-1,V1)]
diff :: 





-- lift definition on generator edges to entire thing
(e a b -> v) -> (Path e a v -> v)


codiff :: (V vert -> Double) -> (V e a b -> Double)


FreePath e a b -> Iso (e s a) (e s b)


data HITCirc = Point | Path | Surf


yoneda Id = Iso id id
yoneda (Inv f) = let i = yoneda f in Iso (from i) (to i)
yoneda (Comp f g) = (yoneda f) . (yoneda g) 
yoneda (Pure e) = \e' -> Compose (Pure e) e'

(Pure e) = Compose e f
(Pure e) = \f -> (cmap (yoneda) e) f

data Free f k a b where
  Comp :: -> ->
  Id :: 
  Pure :: f (k a b) -> Free f k a b

  Pure :: ((k a b) -> Bool) -> Free k a b -- as an example

f (k b a)

And if f is contravaraibt
If f was Id, that'd be the old pure
Const means nothing in there.


yoneda :: Contravairant f => Free k a b -> (Free f k s a  ->  Free f k s a)

-- filter down all those inverse.
data FreePath ::
  Pure :: k a b -> 
  Inv :: k a b ->
  Comp

inv :: Free -> Free
inv (Comp f g) = Comp (inv g) (inv f)
inv (Inv f) = Pure f
inv (Pure f) = Inv f
Inv Id = Id

This data type is intriniscally in a canoincal form
no possiblity of Id, associated to right. Inv filtered down
Ok, actually Inv e . Pure e is still ossiblr and it should go away
data Invy = Inv :: | Pure ::

data Path
  Comp :: Invy k a b -> FreePath
  Nil :: FreePath k a a

   

data Surf
  VCompose ('C l u) ('C d r) -> 
  Compsoe  :: b c -> a b -> 

  |_|  and the final -  top cap

  Id :: Surf a a
  Compose ::
  InvU :: k ('C f g) h -> Surf k f ('C Inv f h)

  InvD
  InvL
  Inv 

  HCompose :: Surf f a b -> Surf f c d  -> Surf ('Comp a c) ('Comp b d)
  VCompose :: Surf f b c -> Surf a b -> Surf f a c

  Id1 :: Surf f '(Comp Id e) e' -> Surf f e e'
  Inv :: 
  Inv :: 

There are like a ton of rules

--->  ----->
 ||     ||
---> ----->



Iso (Iso a b) (Iso a b) -> Iso (Iso b c) (Iso b c) -> Iso (Iso a c) (Iso a c)

Iso (Iso a b) (Iso a b) -> Iso (Iso a b) (Iso a b) -> Iso (Iso a b) (Iso a b)


I kind of want to see the path the thing goes through.

data SV a where
  SV0 :: SV 'V0

e1 = Iso (\case SV0 -> SV1) (\case SV1 -> SV0)

FreeCat Iso
   Cons :: 
   Nil ::

Iso m a b  = a -> m b, b -> m a ??
could use writer to record.
Iso k a b = (k a b, k b a)


k ('Comp f g) h -> k f ('Comp (Inv f) h )
k f (Comp g h) -> k f ('Comp (Inv f) h )


Can we use arrow syntax to compile to catgoeires
The tough thing is the arr constructor
(\(a,b) -> ) -> k s a -> k s b 

(a -> b) -> (k s a -> k s a)
(a -> b) -> (k a a -> k a b)
data DecompCat k = Decomp a, Recomp b => k a b


(k s a, k s b) -> k s (a,b)


k s (a,b) -> (k s a, k s b)


Decomp a where


arr f (Fan f g) = 
arr f Fst = 
arr f Snd = rebuild (f (Fst, Snd)) . Snd
arr f (Fan f g) = rebuild (f (arr id  , arr Fst )) . (Fan f g)
arr f ()

data Typ a where
  Tup :: a -> b -> Typ (a,b)
  Pure :: b

 (a -> b) -> (IsTyp a, IsTyp b => k a b) where

(Typ a, Typ b, k a b )
 
 Typ a -> Typ b
 


Maybe constrined arrows






compose as an axiom is basically a constructor


Definition (p :  tok' ) (list tok') : Machine st tok :=

 match p with
    | Read => Read (hd_error x) (lift tail x m)
    | Write f m =>
    | Return x => Return x
    |
    end

we need to use eval_M.


diagram
  :: Arrow arr
  => arr a ()
  -> arr (b, b, b) (a, c, d)
  -> arr (c, d, d) e
  -> arr b (e, d)
  -> arr () (e, e)
  -> arr (e, e, e) e
  -> arr (b, b, b, b) (e, e)
diagram f g h i k l = 
  arr (\(b0_0, b1_0, b2_0, b0_0_1) -> ((b0_0, b1_0, b2_0), (b0_0_1, ())))
  >>> (g *** (i *** k
      >>> arr (\((e0_0, d1_0), (e0_1, e1_1)) -> (e0_0, d1_0, e0_1, e1_1))
      >>> arr (\(a0, a1, a2, a3) -> (a1, a2, a0, a3)))
    >>> arr (\((a0_0, c1_0, d2_0), (a1_1, a2_1, a0_1, a3_1)) -> (a0_0, ((c1_0, d2_0, a1_1), (a2_1, a0_1, a3_1))))
    >>> f *** h *** l)
  >>> arr (\((), (e0_1, e0_2)) -> (e0_1, e0_2))



{"type":"compose","terms":[{"type":"tensor","terms":[{"type":"generator","outputTypes":["a","c","d"],"name":"g","inputTypes":["b","b","b"]},{"type":"compose","terms":[{"type":"tensor","terms":[{"type":"generator","outputTypes":["e","d"],"name":"i","inputTypes":["b"]},{"type":"generator","outputTypes":["e","e"],"name":"+","inputTypes":[]}],"outputTypes":["e","d","e","e"],"inputTypes":["b"]},{"typeParams":["e","d","e","e"],"type":"permutation","permutation":[2,3,1,4],"outputTypes":["d","e","e","e"],"name":"j","inputTypes":["e","d","e","e"]}],"outputTypes":["d","e","e","e"],"inputTypes":["b"]}],"outputTypes":["a","c","d","d","e","e","e"],"inputTypes":["b","b","b","b"]},{"type":"tensor","terms":[{"type":"generator","outputTypes":[],"name":"f","inputTypes":["a"]},{"type":"generator","outputTypes":["e"],"name":"h","inputTypes":["c","d","d"]},{"type":"generator","outputTypes":["e"],"name":"l","inputTypes":["e","e","e"]}],"outputTypes":["e","e"],"inputTypes":["a","c","d","d","e","e","e"]}],"outputTypes":["e","e"],"inputTypes":["b","b","b","b"]}


Complement of vector space - take nullspace, consider as opposite one

complement :: Nullspace -> NullSpace
complement (Null h) = Null (nullspace h)



Quotient of vector spaces
project out of geberator






Describe as combinations of generators (Image representation)
Describe as free + relations (kernel representation)

Converting between the two is necessary to perform operations



sheaves and validated computation

We have local function sets.
But we want to glue them together into a consistent global function set

the sheaf condition, whatever that was, had something to do with "sensor" integration constraint satisfaction

interesting. cox little oshea talk about finite element method.
When we spline, we can glue together regions. we enforce continuity and differentiablility conditions along the junctions between regions.
The notion of quotient as differentation gives us a direct algebraic way to talk about this.
these are natural modules.
each region gets an index in the vector, but they hold rings.


Sum, Product, Compose

class Sum k where
   :: f a -> k f g a
   :: g a -> k f g a

class Sum e where
   :: a -> e a b
   :: b -> e a b

How do you write functor instances over?

instance Functor f 




PreSheaveCCCA*
Peace
A*
LSS-LRTA*
NANCY

NASA
CPPL
PDDL



Encoding trees into sat

For a linear path finding problem, we can just unroll the problem in time
However if we're searching for an expression tree, what do we do?

We can encode the tree by unrolling an arena. We confusing how to deal with different arity constructor though.

Instead, a different approach is to consider a gebneral graph.
Graphs are easy to encode as sat.

We have a fixed set of vertices. Then the edge relationship can be specified by 

It might be kind of cool to consider simplicial complices as a SAT problem.
edge and face relations.

With dimesion, we're increasing the number of variables signinifcally.
N^d

Homotopy - finding a homotopy can be done with path unrolling.


Anyhow, graphs asre easy to encode. We can enforce properites of graphs, like transitivity and other things.
We can enforce the graph is a partial order,
or total order, or equiavalnece, or partial equivalence

To enfore the graph is an apropriately typed syntax tree seems perfectly doable

data MyTree = Tip MyTree MyTree | Leaf

produce a pile aof Tip nodes and Leaf Nodes.

Can we interret such a tree?
count (Tip x y) = (count x) + (coutn y) 
count leaf = 1

implies/if
child_x =>  (count x == count )

This might be equivalent to laying out in some sense


we need to break symmettry between nodes

.< .~   .~  >.

complining pixel arrays to SAT.
not dissimilar from SMT with interval constraint propapagation.

A = np.array([[0,1],
             [-1,0]])

xdot = A x

V = x V x
Vdot = xdot V x + x V xdot = x (AV + VA) x
 
V = cvx.Variable( (2,2) , psd=True)
constraints += [ A @ V + V @ A <= 0]
prob = cvx.Problem(cvx.Minimize(1), constraints)


x = cvx.Variable(2, integer = True)
y = cvx.Variable(2, integer = True)

-- integer transition system

x[1] == y[0]
y[1] == x[0] - 1

-- useful for finding counter examples

wx = cvx.Variable(integer=True)
wy = cvx.Variable(integer=True)

wx * x[1] + wy * y[1] <=    +   lam * (con1) + lam * (con2) +  
x[1] == y[0]
y[1] == x
x >= 0
y >= 0



eventually leaves

x[0] >= 1

cAx + epx + l(x - 1) <= 0
l >= 0
The DUMBEST SOS style certificate is 
all odd terms = 0, all even thems >= 0

term by term this gives
cA + l == 0
eps - l <= 0

optimize (eps / |c| ) 
set eps == 1
 and just optimize c


Local inverse with lenses

Not all functions are invertible. In fact, a lot aren't.
This is often because there either isn't a solution, or there are multiple solutions

A truly invertible function is an 
type Iso a b = (a -> b, b -> a)

which can be lifted to 
type Invertible a b = (a -> b, b -> [a])

Invertible is useful for fiber bundles.
Fiber e b = (e -> b, b -> [e])
The forward map is the projection from the total space to the base space
And the backwards map is the image of the base space in the total space.



this is a subclass of 
type Rel a b = (a -> [b], b -> [a])




These functions should obey laws.
forall x. x `elem` (to x >>= from) 
forall x. x `elem` (from x >>= to) 

It's kind of nice 
type Lens a dela b delb = (a -> (b, delb -> dela)
type PsuedoInverse a b = Lens s t a b 

I mumbled something about this in my original Lens AD post

None of these are arrows. We can't derive an arr for free.

Derivatives determine very local inverse of functions.
That's kind of what we use them for in machine learning. We'lre trying to find the weights that are an inverse function







Finite differences and derivatives.


covering spaces.
homotopy path lifting.





grobner basis algo converges.





Nats, Lists, and FreeMonads.

data Nat = Succ Nat | Zero
data Nat' = Plus Nat' Nat' | One | Zero
data Nat'' = Mul Nat' Nat' | One | Zero | Plus Nat' Nat'
data Nat''' = -- [Nat] -- binary expansion, weakly typed
data Nat''' = [Bool]

data Nat ''' = Zero | One | Two | Three | Plus Nat' Nat'
data Nat''' = Zero | One | Two | Three | Plus4 Nat'


dat Lists a = V0 | V1 a | V2 a | V3 a | Cons (V4 a) (List a)
data BinList' f a = Nil |  More  Option (f a)  BinList f (f a)
type BinList a = BilList' V2 a





appoprixmate differential invaraints


fitting a torus curve

We know what a torus is
Get a bunch of points set poly = 0 on 
poly <= eps inside
poly >= eps outside

fit second order multinomial.


congruence closure of a relation
Thats a good one
 
aRb

(f a) R (f b)


data Closure f r a bwhere
   -- Pure :: r a b -> Closure f r a b
   Closure :: r a b -> Closure f r (f a) (f b)

AST = Fix ASTF

FixClosure
   -- Pure :: r a b -> FixClosure f r a b
   Closure :: Closure f (FixClosure f r) a b -> FixClosure f r (f a) (f b)


Lebnitz equation works under ANY context. Pretty magical.


forall f. FixClosure f (:=:) a b <-> a :=: b 



rerwirte systems and games

P1 (state) -> P2 (state)

dominant strategy ~ confluence?



Thermodynamics and algerbaic geometry
Planning a thermo process?

E(p,t) = 

Laplace

There was that one book I liked

Different potentials
That kitty notebook


If we care about boundary conditions / C offset terms that happen 

partial_x L and L are different operators.
One needs one more layer of boundary conditions
And we can't undo that division. We need to put back in a C.

If we care only about steady state response, then perhaps rational polynomials is okay

In topological materials, this corresponds to perhaps worrying about boundary conditions


validated accuracy of DFT versus fourier series or 

derviing in sympy the fourier optics or 


What to do today:
review chloe's stuff
write tests for lexer
Write composition
Find cbat examples

That'd be a hearty beef

Welp. Oh well


Compiling integer logic to MIP

a  /\ b    = [ compile(a) , compile(b)  ]
a \/ b     =  z = Fresh;  biloosen (Implies (z=1), a) , Implies(z=0, b)]
x <= y  ===> x <= y
Implies(x,y) ===>  loosen( y, lambda z ) 




lambda calc as a rewrite system.
Not so obvious.
Lam(x, ) 

Make pendulum gif
Zach said... something. Duality. He brought up duality.

germs, expoentnail in Top



The category of migen
The category of pytorch
The category of graphcat
nashpy
pyspice
Vect via numpy

catpy
pycat


write compose.
more tests for lexer.
try to get fusion working?
fiddle with silly version



Types unify
values pattern match

can we get good unification out of pattern matching

unify () () |

Unifies.
(SNat a ~ SNat b)

a pattern match is a function
() () () -> ()


reversible computation
a -> m (b, b -> m a)

a -> m b destructively updates memory

but given that memory restored, we we resonctitue a

So this has a flavor of Iso in the sense that

\x -> (y ,\_ -> x) is always allowed
it will store x on the heap
\x -> (x+1, \x' -> x' - 1 )

\x -> (modidfyST (+1 ) x, \x' -> modifyST (-1) x' )
Hypothetically this only stores a pointer to the fixed function
These actually are iso

(\x -> modifyST (), \x -> modifyST ) 

An assembly lens
call/CC is a go to statement.
Then lens is a function call.
env -> (in , output -> env' )

During search however, iso are not what is happening.


35905
40843

ideal diodes put current and voltage into complemtnarity condition with each other
https://ir.uiowa.edu/cgi/viewcontent.cgi?article=6687&context=etd

softer diode is a Relu
you could build a neural net and then implement it passively 
Conceivably you could have a neural net that could do forward inference in GHz
so is contact


https://www.cs.cmu.edu/~sandholm/MIPNash.aaai05.pdf
MIPNash paper











boolean algerba- distributive lattice with complement
lattice - partial orders, subsets, <=, max, min
boolean - set with complement, bools


It can be shown that every finite Boolean algebra is isomorphic 
to the Boolean algebra of all subsets of a finite set.
 Therefore, the number of elements of every finite Boolean algebra 
 is a power of two.
Bools are power set of singleton set



 Hence, one could take subset values of a finite set as truth values
But these will be bitvector logics

rings and bools
going mod x^2 may help
or = x + y - xy? We can fit a polynomial to any truth table
polybori

mod (x^2 - x) basically means we are only defined on the roots of x^2 -x
Which is 0 and 1.
why would you need a system of equations?


https://apps.dtic.mil/dtic/tr/fulltext/u2/a142796.pdf
systlic algerba trasnformations



filter and ideal are dual



data V = V0 | V1 | V2

data EdgeGen a b 

data FreePath k a b where
   Inv :: Iso
   Compose
   Id
   Iso :: (a -> b) -> (b -> a) -> 



(Iso a b, FreePath k a b)
interp Inv = Iso from to
interp EdgeGen
interp Compose

FreePath s a -> FreePath s b

Path2 k 'k 
 Iso a b -> 

recursor :: Profunctor p => (k a a -> p a a) -> p b b 


Zachary Stone13:16
swtich friend code: SW-5442-1940-4134



compreheresnion schema.

It's "just a proof checker"

I instantaite my axiom scehema in partial mode.
Function(nat ,  set, )

Exists( [s], Forall[x],  f(x) =  elem(x,s)  )


Bounded quantifiers - we're good
x \in X === formula
unbounded quantifiers

formula-set correspondence.

induction and comprehension schmea

A schema requires and enumeratin of formulas over which to instantiate it

set comprehension requires second order arithemtic?
In order to have a set in the language


data Form = And | Or | 
   Ex (\x -> Form) | All (x -> Form) 
   | Eq Nat Nat | LT Nat Nat | 
data Nat = Succ Nat | Z | Plus N N | Times N N |   




A lot of this stuff makes more sense from th context of z3.

Basic axioms of arithmetic.

Can't use built in nats. It's too much.

Reification.
z3 Functions are not first class.
We can't pass them into other functions


A function graph is however?
f = Function( Z,Z,Bool) is a relation 
the function axiom is 
Forall([x], Exists( [y], f(x,y) ) defined everywhere
 -- and single valued
Forall([x] , Exists(y1, y2) (f x, y1) f(x,y2) =>  y1 = y2)

Comprehension
Exists(X, ForAll([x,y],  elem( Tuple(x,y), X  ) <==> y == f(x)))
Now X is a reified f.

def comprehend(f):
   X = FreshConst
   ForAll([x,y], elem( Tuple(x,y) , X ) <=> y = f(x))

Comprehension and Schema

find_consts "nat â nat â nat"
datatype 'val msg 
=  Propose 'val
  | Accept 'val

value <Propose 3>

value "plus_nat_inst.plus_nat 3 4"
value "(3 :: nat) + 4"
find_theorems "?a + ?b = ?b + ?a"
find_theorems name:commute
find_theorems name:Lattice
find_theorems "_ + _ = _ + _"
find_consts name:"comm"

lemma "3 + 4 = 7" by auto
find_consts "'a â 'a list â 'a list"
thm List.list.sel(3)
lemma "tl (Cons x xs) = xs" by simp




	




ultrafilter - analog of maximal ideal 
ultrafilter is "point". Yeah. Kind of makes sense.


i'd really like a commutation property to move call_on_read p (parse_fix q n) into a parse_fix ?????

if you can come up with one

maybe parse_fix (fun p_rec => call_on_read p (q p_rec)) n but that's not quite it?


unfold database

pushing call on read through parse fix

extraction - make sure fix has right behavior

debugging - print tokens




peano arithmetic 

bunded complexity formula


There is some similarity between 
I'm getting deja vu

The andrej thing about surprising functionals and 
integration only being able to sample points.

functional differentiation in the abscence of functional integration is easier
Just like vector derivatives in the abscence of... integration over those vectors?
The integral is analog of dot product


Anyway


could get euler-lagrange residual


a differential equation is a set of trjaectories

The rectangular matrix correspodnign to it is in HRep
VRep gives the generator formulation



Gluing homogenous domains.
or gluing solutions on boundaries.
Hmm..

so an object is (psi, dpsi, ddpsi) vector space

and 

phi on interval
psi in interval
ddpsi in intervak

how much does that get us?

forall x, psi(x1) <= psi(x_o) + int\si



power series matching 
a la perturbation theory
a sequence of HReps and projection/injection mappings.

The next hrep has to be consistent with the previous one

h1 <= p . h2



substitutions
heterogenous variable types


(a -> Free f b)


(Var1, Free f Var2) is basically a first order rep
(x,y,z) -> Free f (a,b,c) is a phoas

(x -> Free f (Either a b c), y -> Free f (Either a b c) , z -> 


substitions as a category
mgu is a coequalizer

convex spaces in the distribution monad
generator style
The free convex space is 
[( p , v )] which is pretty similar to 

[( p , [(a,b)])] free convexity over free vectors
a is any field
p is [0, 1] normalized

Probability and convexity are related
The average < v > of vectors always lies in the convex hull of 
their support



finally tagless optimiziations
reify context

if you only needed one pattern match
you're good?
if you need deeper pattern match, you need to be able to
read in that info.

You can partially initialize I guess?

Add Mul  Lit | X


eval x X = x
eval x (Add a b) = (eval x a) + (eval x b)


partial evalutation of addition
eval X = X
eval (Add a b) = if eval a eval b  = Lit then Lit x + y else Add x y
eval Mul = 

instance Expr Int -> Int where
   add a b = \x -> a x + b x
   mul a b = 
   lit a = \_ -> a
   x = \x -> x

instance Expr (Expr') where
   add = Add 

instance Expr (Maybe Int, Expr') where
   add ( Just x , _ ) ( Just y , _) = ( Just (x + y)  , Lit x y) 
   add (Nothing, a ) (_, b) = (Nothing, Add a b)
   lit i = (Just i, Lit i)
   x = (Nothing, X)
   mul () ()

As we're building terms, it is known whether we are statically known or not

data Expr static where
   Add :: Expr s -> Expr s' -> Expr (s && s')
   Mul :: Expr s -> Expr s' -> Expr (s && s')
   Lit :: Int -> Expr 'True
   X :: Expr 'False

eval 

passes going down vs passes coming up, extra data contravaraint vs covariant?
And nested if you need multiple passes.


exp :: Int -> Expr
exp n = if n == 0 then Lit 1 else  Mul X (exp (n-1))

This unrolls because Haskell is a macro language.
It unrolls as runtime.
But we can use runtime as a macro generator if we output code
exp n 



class Exp n where
   exp :: Int -> Int

-- unrolling via typeclass resolution
instance Exp Z where
   exp _ = 1
instance Exp n => Exp (S n) where
   exp x = x * (exp @n)

My impression is that this will all get inlined.
Haskell aggressively inlines class definitions.


Using types as compile time known things is pretty solid

Exp n => SNat n -> Int -> Int? Will this be compile time


Going finally tagless also prevents 


Finally tagless ~ bohm berarducci

If a pattern match is a recursor,
what does a deep pattern match look like?

The neg neg transformation


conduit library
parser iteratees

compose : Machine st tok (Machine st' tok' a) -> Machine (st * st') tok a

(a -> Machine b) -> (b -> Machine c) -> (a -> machine c)


kripke models
terms
alexandrov topology


consider a simplicial complex
(mesh)

There are points, edges, faces. These are all sets.

The faces do not include the edges
a face with all it's edges is closed
a point is closed


a topology is a sub of sets of these things
data Space = P1 | P2 | E1 | E2 | E3 | F1

type Topology = Set (Set Space)

such that 



set enumAll `elem` topology
empty_set `elem` topology
all [ a /\ b `elem` top | a <- top, b <- top ]
all [ a \/ b `elem` top | a <- top, b <- top ]

Since we're in a very finite space, there is no difference between the two conditions

To give us a small infinity, we could imagine a set generated by
data InfLine = Point Int | Line Int


for any point

hausdorff

We have a notion that lines are conncted to the points

[L 0] is open
[P 0, L 0, L -1] is an open set.
[P 0] is not

[L 0, L -1] is also open. It's missing a single point P 0


[ L0, P 0 , L -1] generate the topology
We can build the entire toplogy by iterated /\ and \/

The star
Alexandrov topology

The stars form a topology

That's interesting and direct



sheaf

type Open = Set Gens
objmap :: Open -> ?
incmap :: (Open,Open) -> ?





data thunklist = Cons a ThunkList | Thunk () -> ThunkList | Done


Machine st tok [a]
but we want fusion

Machine st tok 


Inductive MachineStream st tok a :=
  | Yield : a -> MachineStream st tok a -> MachineStream st tok a
  | Cont : Machine st tok (MachineStream st tok a) -> MachineStream st tok a
  | Done : MachineStream st tok a

compose : Machine st tok (MachineStream st tok a) -> Machine st' a b -> 
Machine (st * st') tok b




Machine st tok (Machine st' tok' )

Machine st tok









Sheaf.
presheaf - Covaraint functor to set

Rings get mapped to set. That's algerbraic geometry

It's a function that 


Yea. Kripke models feels like a nice thing for why it's not insane to talke about truth values besides true and false

And the graph of the kripke thing gives a natural categroy

But those lattices aren't obviously set?

Still it's a profunctor.


baby unification. Pure variable equality
a = b, c = d, f = q, a = f
This is more clear that disjoint union data structure is helpful

http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.3615


substitutions forms a category
varsetin
varsetout
{ x:   ,   ,   ,  }
symply.replace( ... )  

can sympy find 





data One a b where
   Id :: One '() '()
 
PreSheaf ~ k a b -> (f b) -> (f a) 

natty trans

forall a b. (k a b) -> ( f b -> f a ) ->



packrat parsers?
LL*
llr k
hammer?

fpga:
stephen taylor
meredith



first order functional programming
Sure. This is the example used
prolog feels this way I think
z3 has this feeling
the substitition mechanism is easier.
No internal binding forms.
Let forms. Not lambda

let x = in
let x = in
let 

let vs lambda
is let applicative?




finally tagless derivative - will this end up being automatic differentiation


data Expr = Add  Expr Expr | Mul Expr Expr | Lit Double | X | Exp Expr


class ExprF a where
  add :: a -> a -> a
  mul :: a -> a -> a


val (Add x y) z = (val x z) + (val y z)
val


diff (Add x y) = Add (diff x) (diff y)
diff (Mul x y) = Add (Mul (diff x) y) (Mul x (diff y))
diff Lit _ = Lit 0
diff X = Lit 1
diff (Exp x) = Mul (diff x) (Exp x)

(*  Of course, we could also use the standard Haskell Num hierarchy *)

class ExprF (Double -> Double) where
  add x y = \z -> (x z) + (y z)
  mul x y = \z -> (x z) * (y z) 
  Lit x = const x
  X = \x -> x

newtype Val = Val (Double -> Double)
newtype Diff = Diff (Double -> Double)

class ExprF Diff where
  add 
  mul (Diff x) (Diff y) = \z ->  * () + () * ()


class Expr 


There is a general purpose strategem

information that needs to be carried down
(a -> )

information that needs to be carried up
(  ,  )

we need to carry back up the value


instance ExprF Diff where

  

differentiation tensors

class TensorSumF f where
   sum_ :: (a -> f a) -> f a






Automatically Differentiating Finally Taglessly

Automatically Tagless Differentiation Finally

Finally Tagllesly Differentiating Automatically Finally


Finally Tagless Differentiation is Automatic



Staging
We'd like to absorb Lit 0
We'd like to do cse collection

data Expr = 
data ProgX = Expr | X
data ProgXY  = Expr | X | Y

eval :: ProgXY -> x -> y -> Expr
eval :: ProgXY -> x -> ProgX -- partial evalutation
eval :: ProgX -> Expr






The elements of an adjacency matrix for a graph count the paths from one vertex to another. The elements of a profunctor are the sets of heteromorphisms from one object to another rather than just the number of them. To compose the adjacency matrices of two bipartite graphs, you multiply their adjacency matrices. To compose two profunctors, you do matrix multiplication, but where multiplication is all pairwise compositions and addition is union.

Exists is a sum

Profunctor = p a b q


(a -> b)  , (c -> d)
k a b , q c d -> 


exists b. (a -> b, b -> c) ~ a -> b


c a c, d ) -> (p a c ->  )


(p a b, q c d ) ->




Traversal for minimx

boolean algebras and powersets
heyting algebra and open sets

functors to set

finite categories
can be used to pick out objects.
can be used to pick out functions
Like graph


V  E

presheaf
f and 
k a b -> (f b -> f a)


Presheaf

graph as a Presheaf
we can have a pattern category that maps into a a data category Set
With some simple finite categories we have familiar structures
I guess they obey some coherence conditions



subobject classfifier - analog of True/False two eleemtns set
subsets are monic morphisms gf = h f => g = h because f is injective

That is a nice closed algerbaicall way of talking about injective

equalizers are analogs of equations

Preshaves have pullbacks
Presheaves have exponentials

But so does Set?


classifying spaces is some context I don't have


Do categories have Vrep Hrep

Hrep is equations
Vrep is generators

Seems like we need both

Categroy morphism is a functor


FUCK CATEGORY THEORY

linear representation of a category. Rectangular matrices
group is a particular one object categroy. we can represent that

a finite category needs a lot of equations


modelling categories by using types as objects makes a lot of sense
but it's also a big enough pain in the ass that I'm pretty sure there must be some situations where it doesn't make that much sense.


While you can always bullshit your way out of any pickle with typelevel shenanigans, I'm not sure it's worth it.

By making our morphisms not carry the objects they go from and to i n their types, we are reduced to an untyped representation that needs to throw errors at runtime. 

Is that so bad?




class Category
  type Obj :: 
  type Morph
  dom :: Morph -> Obj
  cod :: Morph -> Obj
  compose :: Morph -> Morph -> Maybe Morph
  id :: Obj -> Morph


compose :: Morph -> Morph -> Morph -> Bool

data FinSet = FinSetMorph (Set a)  (a -> a) (Set a)


instance Category FinSet where
   compose (F d c a) (F d' c' a) | c == d' =
              | othewise = Nothing



class Category morph where
    type Obj :: *

{- matrices -}
instance Category Matrix where
    type Obj = Int
    compose = 



{- finite substitutions  -}

data Sub = (Set a) (Map a (Term b)  (Set b)
instance Sub




man, but sometimes we WANT to be heterogenous.
Ok, then we need to tag

data Tagged = TInt Int | TBool Bool | TPair Tagged Tagged 
             | 


data Typed = Unit | Void | Pair Typed Type | Plus Typed Typed
data TagVal = One | Plus Tagged Tagged | Left Tagged | Right Tagged

sametype :: Tagged -> Tagged -> Bool
sametype (Left _) (Right _) = True
sametype (Right a) (Right b) = sametype a b
sametype (Left a) (Left b) = sametype a b
sametype (Right _) (Left _) = True
sametype (Pair a b) = sametype a && sametype b
sametype One One = True
sametype Zero Zero = True
sametype _ _ = False

Sub Tagged = FinSub (Set Tagged) (Map Tagged Term Tagged) (Tagged )


(a -> Term b)
This is the kliesli category over term


instance Category (Sub Term) where


instance Functor f where
   type Morph1
   type Morph2
   omap :: (Category Morph1, Category Morph2) => Obj -> Obj
   amap :: Morph1 -> Morph2





datatype Obj = a | b | c
datatype Arrow = f | g | h | k | id of Obj


newtype Dual c = Dual c
instance Categroy c => Dual c where
   comppose = 
   Obj = Obj
   source (Dual k) = target k
   target (Dual k ) = source k

newtype Hom c = Hom (c -> c)




data Product c = Product Obj c c
data Pullback 
product :: Obj -> Obj -> Option 

instance Category c => Cartesian c where
   product :: Obj -> Obj -> Product c


This functional representation of the universal property is crucial to the programming of category theory. It is âSkolemizationâ, turning ââ-statements (unique existence in this case) into functions. 


data Initial c = Initial c 


class Catgeoyr c => Initial c where
   obj :: Obj
   univmorph :: Obj -> c




Domain specific forall

forall :: (a -> Bool) -> Bool -- encodes a exhaustive checking. Can return False fast as sooon as finds coutnerexemapl
exists :: (a -> Bool) -> Bool -- encodes a search procedure. Can return True fast as soon as finds an example

epsilon :: (a -> Bool) -> a




foralllist :: [a] -> 










emergent effects in physics
systems in bulk behave differently then their microscopics

topological effects
projecting into the ground state

projecting as an approximation
broken symmettries

thermodynamic limit

renormalization composition

take two systems, combine, integrate out all energies above cutoff

composition as compose and project.

Lossy functor from hamiltonaians to energy diagrams.



Just streams of data
The bottom style stuff in haskell

I could have a propisitional variable associated with each value

(x = S z w) which could be bottom

Whats the structure here

Heyting algebra - untuitionsitc version of boolean algerba
heyting algebra is algerba of open sets

open sets are semidecidable sets
 Heyting algerba is a lattice
Complete Heyting is something


kripke models
gaining information

So maybe a unsolvable


books
stone spaces
goldblatt topoi
Geometry in Logic
Sheaves and Geometry in logic


Hmm. Could write up the RaaS robot
write up the tank tread paradox



set - indictively defined as 
union
power
pair
{a,b}
{0}
but then the comprehension principle, which refelcts logical formula into the sets



a nominal point and a ball radius

Suppsoe we do take the residual as a metric


(L(phi1 - phi2)^2) 

can we find a 

(L (x - phi )**2 <= eps is ball
xn -> x_n+1 = Axn + b 

ball
 L (A^-1(x - b) - phi)- phi <= eps

is inside first ball.
In other words, inequality 2 implies inequality 1

I feel like L has to be positive semi definite to qualify as a metric

differential inequation

inverse images are 


Always try monics and epics in different slots. See what you get
Effectively are similar to subset

Confusing again that sometimest he catgeory itself is subset




G-Sets
Set^2 - has 2 bools as subobject classifier


brute force search is the master discrete algorithm


bijective search is the master continuous algorithm?



Power series relations
<psi|psi> = 0
(H + gV) (psi + ...) = (E + ...) (psi + ..) 


Series gluing relations


An expansion on a domain

a sequence of best approximations in some local sense
1 <= 1 + x <= 1 + x + x^2 <= ...
a sequence of best approximations in a L2 sense
1 <= 1 + sin <= 1 + ... <=

but also these are vector inclusion maps.
There is a functor at play between the 
inclusion maps and the approximation ordering
Also perhaps the quotient rings of x  and e^iz.


x'' - x = 0
x'' - x = 0

a2 - a0 = 0
a3 - a1 = 0
a4 - a2 = 0
a5 - a3 = 0
...

stabilizes to a 2 dimensional space.


Is this chain exact? We are talking about spaces that inject into each other
A diagrams lik this does seem releavnt.
There is the mapping. There is 
  __ _   _   __    _
 |   |   |    |    |
 -   - - -  - - ----

moving the expansion point of a taylor series is impossible
you'd need to collect up all the terms
unless the coefficients were already exact reals.
Then it is possible.



pull based epsilon
now these a uniform convergfence values
epsout -> (epsin, in -> out)

epsout accuracy -> (epsin , inval -> outval)

This does work.
And you could use fast mpfr for the second part maybe 
because 


outaccuracy -> inaccuracy 
returns a function that can take in ratinal and output another rational
that is accurate to this value.


The connection between auto differentiation and continuity
Auto differentiation finds derivatives
differnetiable <= LipSchit <= Uniform continuous

if we can max(diff), over a bounded domain
x -> (y , dy -> dx)
x -> (y,  domain y -> max derivative val)

domain x -> ( domain y, max derivative value)

standard interval airthmetic
Iin -> Iout
analog of forward differentation. Lift differntial to finite difference
and allow over approximatin
(x, dx) -> (y,dy)





max :: (a -> R) -> R 



Lens :: Rational -> (Rational, eps -> eps)
inval -> (outval, outval -> inval)

Doesn't work because you might have to revise the rational you output




dup :: (eps,eps) -> (eps, x -> (x,x))

dup (eps, eps') = (min eps eps', \x -> (x,x)) 
add eps = (eps / 2 , eps / 2 ), \(x,y) -> x + y)

Really things like these two should be a conversation.
Whichever piece it is easier to approximate.


mul eps = ( sqrt eps, sqrt eps ), \(x,y) -> x * y ) 

-- same diff I guess
sqr eps = (sqrt x, \x -> x * x  )

-- relies prtty heavily on the domain
sin epsy = (  epsy , sin    )
cos epsy = (  )

of course using sin is cheating

Really, we want 
sin epsy = (  epsy, \x -> let n = log epsy in sum (take n sinseries) )

sin epsy = (epsy, \x -> takeuntil (\t -> t <= epsy) sinseries   )

So we need side consistency proofs.

It's all foralls. 
forall epsy1 epsy2, eps1 <= eps2, 
  => forall x. | f (x +-   ) - (f x) | <= eps1
   
epsy -> 0, epsx -> 0 (epsx <= epsy? Seems wasteful but perhaps. )
and 


It's nice that something that doesn't effect the 
result much will not waste time computing

It's nice that it is clear where swtiches over to 
mpfr and such have to happen

a good use case for metaocaml.

Accuracy required is known at compile time
unroll loops, switch methods.





   We have supplied the skolemized delta computing the delta

-- a useful thing

f eps = ( max |df| * eps , f )


Lens eps y x delta


But we're also intermixing the approximation accuracy in there
the f_n depends on epsy.



id = \x -> (x , \x -> x)


compose f g = \epsz ->  (epsy, g') = g epsz
        (epsy, g') = g z
        (epsx, f' . g')


numericists use backwards error analysis too.





Is there a point to this?

We don't always want to know the accuracy of the output
We could just probe incoming intervals until we hit our goals
It's inefficient to do so.
This does seem to match the mathematical definition of continuity more closely

What is the criteria of truncation on I -> I formulation?
we stop when teh error is roughly on the same order as the error induced
by the error in the input.





Part of the reason this makes so little sense to me is its mathy



so an intuitinsitic model finder could be a thing.
Check finite models of 


topology is sets
monics for inclusion / subsets

kripke models


in classical prop logic
not phi  and phi  are more interrelated



data Proof where
   Axiom :: Proof '[] '[]


prove :: Int ->    -> Proof



point free logic


zariski toplogy


ok what 

Lattices made more sense to me for
polyhedra and 

closed sets are planes
closed sets are Ax >= b

open 


inclusion maps as subsets


point free logic
cartesian closed is already enough to do
intuitinstic logic I guess.

second order arithmetic and an internalized set
schema a of comprehension is a reflection principle


I seemed to have some blurb about power sets as use cases of profunctor
if the domain categroy was a power set under inlcusion
And the codomain was true/false

Then restricting the domain can never make true becomes false


There is a monotonicity there which corresponds to the Functor property.



point free agda
Where would one get into trouble?


the diagram of a circuit. is a graph
we do have vector spaces floating over nodes


unification is solvable
Does unification fall into generator relations dichotomy?
That goguen paper that was talking about substitions

knuth bendix i thought was analog of grobner...?



What if we embedded a full typing system into z3


CT = datatype( "Hom"  , Obj,Obj,      )
Function("Type", Morph, CT, Bool )
I guess this is basically Dom Cod


regular logic and linear programming
That's interesting
Exists 


Forall(  formaula of /\ exists -> /\ exists )


I guess you could make a kind of prolog


/\ /\ /\ -> /\ /\ /\
/\ /\ /\ -> /\ /\ /\



search + unification = prolog
search + lp = linlog


prolog + lp might alread do this though.



hmm

convex optimization has a local to global property
the local derivative is a global understimator





considering bottoms
x -> () describes the toplogy on X
The different functions you can make
open up different pieces


_ = undefined
_ = ()

f1 :: 
_ = 
(Just _) = ()
Just (Just _) = ()


true false
bottom 


perhaps this is a fairer assememnt of x -> Bool

There are no set models of polymorphic lambda calculus
because it can encode recursion

from the domain theoretic persepctive

(a,b) is not a * b. it's (a + 1) * (b + 1) ? or it's 4?
no because that is implying a simple recursion for calculating the number

is (a,b) the produt domain theoretically? Probbbbbbably


Huh
under cpo persepctive void is unit

(a , Void) ~~ a

(x,_) -> x
x -> (x, undefined)  ?? we have to open x.
maybe this is assuming strictness


convex functions have a local-global property
The tangent of a convex functions is a global underestimator.



subderviatives and sheaves

The subderivative at a point is a set of tangents.
If it is smooth, it is a single tangent
at a disconituity it can be a set


GAN - generator and discruminator
discrimator ~ kernel map
generatro ~ generator map


Representation by tying the knot. Corresponds to having a point to binding site.
let v = Lam (Var v) in 



hmmm this section is very very similar oto my idea
of powerset and profunctor
powerset of space -> true false lattice


quantifiers and vectors

monotonic functions compsoe
monotonic piecewise linear.



jules asks:
Is a coend possible to express in python
exists s . a -> D (s,a), s -> r -> D t

we could do a cryptographic coend

A little story

The world is convinced i think of the utility of abstract data types.
There are data types that implement hash maps and balanced trees and things that are incredibly useful and performant and a little finicky to get right. If a user can muck around inside the data structure, they have a good chance to destroy some precious invariant required for the correct operation of the data type. The interfaces these data types present are pretty acceptable most of the time and that isn't necessary,  but some languages have facilities to protect the user from herself (is it more politically correct to call this hypothetical pigheaded egotistical user herself or himself?) a little bit.

The origin of this technique I suspect comes naturally.
We are not inclined to look into most library code. It takes a lot of mental energy to try and understand enough of a code base to use it in a way unintended. Beyond that sometimes the code is only available in compiled form, for which it would be an even larger task to try to decrypt it. The abstraction barrier is by obfuscation in this case.





public key cryptography


pickle, then


forall( lambda a :    )
def forall(f):
   def res(*args, **kwargs):
      # or hash the args? requires them to be immutable?
      remember = {  random() : a for a in args}
      y = f(*remember.keys())
      return remember[y]
   return res

@forall
def foo():

requires currying

@exists



It's kind of a special environment to carry around
In addition to the implciit environments being carried

That's kind of fun

f( , , , foralldict )

Categories for contracts.
This is the analog of parametric poly?


def forall(**kwargs)
of
def forall(*args):
   list of keyword arguments to 
   and positional arguments
   if empty, assume all.

@forall(0,2)
def foo(x,y,z):


Could also pass in the lookup.
Only use in return.
def  foo(x,y,z, cb = cb):


   return cb(x)


It's similar to the concept of a handle.
We have a global authority that knows when things are good.
It's similar to the fact that boxed polymorphism requires a uniform pointer representation, but disallowing pointer derferencing


Or maybe some kind of promise. It is only considered fulfilled
after the function finishes and the 
forall scope is quit.

def forall():
   def res
      locked = True
      dict
      f(*dict.keys())
      locked = False


The coend version let's us tag which world we've gone into


promises
and then fill the promises on return
That seems pretty good.




Contracts

@array(  yada yada) ->
@(int => bool)
def foo()


def int(f):
   def res(*args, **kwargs):
      assert(type(args[0]) == int)
      y = f(*args, **kwargs)
      return y
   return res


def arr(a, b, f):
    def res(*args, **kwargs):
      assert(type(args[0]) == int)
      y = f(*args, **kwargs)
      return y
   return res


Yeah, what is the deal with fourier motzkin
It doesn't really deal with logical formula

what is up wiht convex
>= is important to convex.
And yet > is supposed to be the computable one?



Ok. If kripke models 
and heyting models ~ kripke ish
And temporal something. temportal logic
model checking
bisimulation
open maps 


or/and adjoint to duplication

weakening generalization of duplication
You can add superfluous parameters to a predictae


I've seen weakening as something about just like losing shit
a /\ b => a

exists and forall are big ole products and sums


image is related to exsits

if true is in the image of the charectersitci fucntion exists
if false isn't in the image then forall

preimage of points or subsets gives set indexed sets

slice categories as typing
We are considering type to just be some set.
Kind of at the same level of discourse as values

I guess from a term persepctive that makes sense.

We have a fixed set of types

and then we can move to another set of types

a finite set of types
'[(),  Void, Bool]

morphisms that move between universes of types.
'[a,b,c] -> '[x, y, z]

It's HList.
TypeHere
TypeThere
No, it's not. It's just an open sum thing

Here :: x -> Typer (x ': xs)
There :: Typer xs -> Typer (x ': xs)

More and more inclusive universes.
More and more 

If we had a convenient type level set, maybe we'd use that


injection functions are obviously interesting. move us between universes




You can project HList.
'[] -> '[]


I was doing stuff like
Tagged :: (Tagged a, Tagged b) -> Tagged (a,b)
Unit :: Tagged ()

to get universes by structural construction

But a nominal universe will never work like that

TypeUniv '[x,y,z,w,....]

TypeUniv u -> TypeUniv u'

f :: forall b a. TypeUniv b u -> TypeUniv a u

IsType x u where -- a sort of projected (x : u)
Here :: IsType x (x ': xs)
There :: IsType x xs -> IsTpye x (x' :' xs)

These are like Finite Kinds.

IsKind x u ~


It is not a freetheorem that you have to act uniformly on these things
Even in the presence of forall x


forall x. IsKind x u -> IsKind 

a function 
x -> IsKind x u



set theory vs type thoery vs categroy theory

although I've a little bit been saing first order lgoic is the most primitive, not really the case



prop is a truth value

not bool?

unit as a truth value
if you see unit terminate, then it is proven


defunctionalization to first order logic?


Hmm. I've wondered about this in the context of fixed point
To multiply by 2^n, we don't need to change the contents of a register
only it's interpetation.
One could do the same in many cases

subtracting yada yada, just correspond to a transition in intepretation
of abstract codes





The goldblatt book doesn't even talk about functors til the middle

Maybe comma, slice categories are where it's at?

I guess slice is a sepcialization of comma?




a defined predicate.
That matches what I saw.

defined ~ typechecks


defined((comp(f,g) , h) ) == And(defined(comp(f,g), defined(comp(g,h)))
defined(id, f).
defined(G,id).
defined((comp(f,g) , h) ) => ((comp(f,g) , h) ) == (comp(f,comp(g , h) ))

for ever object we need to state that all arrows meeting at that obejct are defined
defined(comp(  )).


morphisms = X Y Bottom
defined is  "not bottom"

Yea. this polymprohic id thing doesn't work
I think you could mediate



That ADJ diagram for an automaton
That could be a finite category, yeah?


Ok this co-design thing actually is kind of compelling?
Constraints in a design are kind of



CoFix f = Cofix :: s -> (s -> f (Cofix f )) -> Cofix f




Baez:
One reason we should all be working with finite fields is 
that then you get an adjunction between FinSet and FinVect and 
never need to think about infinity. :slight_smile:



The ways to encode space:
Polynomials
meshes
amorphous mass of sin cos x y z rationals integrals derivatives
infnite series



Krivine machine
CEK machine
CESK machine
SECD machine
CAM




machines are somwhow similar ot intepreters and evaluators


Regular expressions ~ finite state machines.



evaluation contexts
expressions with holes
They are reified by callcc


Formula = And, Or, 
Provable = Function ( List(Formula)  , Formula , Bool)  



CPS. Takes lambda terms into new form where control is explicit

Compile to categories

Dialectica. Take lambda terms and take it into form that explcility
maniuplates the stack.


I was suggesting that lens are nice for mutational backtracking search

I can embed a pure computation into an impure one just so long as I
undo stuff.


Promises?
Memory locations?

Next A - in the next instance memory location a will have  
Always A
Eventually A

Next Int - in the next instance this will have an int in it
Always Int - permanently allocated
Eventually Int - eventually will have an Int in it.


apply optimizations - 
stare at output - find fusion - fix multi match 
test runtime of nondeteermnistic parser

find small parser example

what should a parser look like?
find micro optimization rules

don't read anything
dont rEAD ANYTHING


Do the s-expression example
Something even simpler
Well, the 3 or 4 things are pretty simple. Maybe give them one more clause.



bracket :: 
bracket
        :: IO a         -- ^ computation to run first ("acquire resource")
        -> (a -> IO b)  -- ^ computation to run last ("release resource")
        -> (a -> IO c)  -- ^ computation to run in-between
        -> IO c         -- returns the value from the in-between computation

is a paradigm for resource discipline
How do we bracket in point free style?
How relevant is the monad?


backtracking search with minimal resource usage.





integration as a pullback (or equalizer)

derivative equation is
Dx = b

So integral is subspace.
Symbolic integral as a pullback. They don't always exist in closed form


unoptimized vs optimized




Finally tagless AD
Staged Metaprogram Reverse AD



http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.418.8811&rep=rep1&type=pdf
There and back again


reverse a list

a -> (b , b -> a)

(x : xs) = (  xs   , fun rev ->  rev <> [x]   )
(x : xs) = (   x :   , fun rev ->  rev <> [x]   )

Hmm. Obviously not so good.



( , ( , ( , )))

(a , b) = (  b , \     ->   )




Embedding system T into classical logic?
system T is a functional programming language.



dynamical systesm as a pattern


I was thinking stuff about how tangent bundles are not computable
But perhaps symbolically they are

sympy charts
{ x : f , y : g , z : h }

Really these are Rn -> Rm functions?
It's a matrix of expressions?
{ x  : f(x,y) , y : g(x,y) }

[  f(x,y), g(x,y), h(x,y) ] # no this is a "matrix" R2 -> R3

Tangent bundles
( x , Dx ) -> x

def local_path(f):
   t = Fresh()
   while  True:
      yield diff(f,x)


Doing "functional programming" with sympy


Point-free sympy

compose(f,   g)
par()
fan
fst
snd

Jets


differentiation as a logical quantifier?
Is this true?
differentiation and integration are adjoint? There isn't something in the middle 
there is there?
 



elem
union
intersetc


in, notin, always dijoint. Union = everything?


TokenSet {
   elem : t -> s -> bool;
   union : s -> s -> s;
   intersect : s -> s -> s;
   difference : s -> s -> s
   subset : s -> s -> bool;
   complement
}


ion "src/example_parsers.ml"
           Sexp.p_sexp_opt
           JustAs.p_count_as JustAs.p_count_as_state JustAs.p_count_as_opt JustAs.p_count_as_state_opt.






dy = p dx
insert these into eq
phi ((x + t dx) , (y + t dy) , (p + t dp))

set t = 0 
derivative_t , set t = 0

There's a dp equation somewhere. and it must be from the idff eq right? No
It's not.

It's a function of the original coordinates and dx dy




Require Import Ascii.
Require Import stdpp.base stdpp.strings.
Definition compile_whitespace : { f | f =  fun s => (eval_M_prop_stream (fun _ => Common.whitespace_char) tt (dummy_stream s) (fun o _ _ => o   ))}.
  cbv -[ascii_eq_dec]. simpl.  eexists. auto. Defined.
Print compile_whitespace.
Extraction compile_whitespace.

Definition compile_whitespace' : { f | f =  fun s => (eval_M_prop_stream (fun _ => Common.whitespace_char) tt (dummy_stream s) (fun o _ _ => o   ))}.
  cbn. simpl.  eexists. auto. Defined.

Extraction compile_whitespace'.

Definition compile_whitespace2 : { f | f =  fun s => (eval_M_prop_stream' (Common.whitespace_char @> Common.whitespace_char) None Unknown tt (dummy_stream s) (fun o _ _ => o   ))}.
  cbv -[ascii_eq_dec]. simpl.  eexists. auto. Defined.

Extraction compile_whitespace2.
Definition compile_lex_one : { f | f =  fun s => (eval_M_prop_stream' (Sexp.lex_one) None Unknown tt (dummy_stream s) (fun o _ _ => o   ))}. simpl.  Defined.
*)



make bench as


(if b then (fun o -> o + 1) else (fun o -> o - 1)) 2
(fun o -> if b then o + 1 else o + 2) 2


Yeah. let_ binding
It seems like I only want...

 let (o,st,m_st) = (do a bunch) in



World passing style


Use index as state
convert compose_parser to other form

axioms are code
+
finally tagless style dsl



There are sort of open <|>

and closed <|>

At the end of an <|> I'd like to let bind


Maybe if we didn't have to explicility pass the state?


In regards to ^ "a"

Sometimes we want to return exactly just the dynamic value
sometimes we want to return a translation
"(" becomes LPAREN. No getting around that

Sometimes we want to inline, sometimes we don't

We could do more agressivei inling if the exact case didn't branch as much


^ "a"

We can use both inline and noninline versions of Call.

or_machine could look at constructor
If they are both peek_mem (because they are raw)
or
ReturnTok

We could merge them.


Call vs Let_

Let_ = Call 

and then or_machine can do recollection


OK
FartMachine7:parts philip$ make bench_sexp
dune build src/benchmarks/bench_sexp.exe --profile release
ulimit -s unlimited; ./_build/default/src/benchmarks/bench_sexp.exe -quota 10
/bin/sh: line 0: ulimit: stack size: cannot modify limit: Operation not permitted
Estimated testing time 1m20s (8 benchmarks x 10s). Change using '-quota'.
ââââââââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââââ
â Name         â Time/Run â  mWd/Run â mjWd/Run â Prom/Run â Percentage â
ââââââââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââââ¤
â sexp:262144  â  66.32ms â  24.08Mw â  29.96kw â  29.44kw â      9.04% â
â sexp:524288  â 149.14ms â  47.74Mw â  57.77kw â  57.26kw â     20.34% â
â sexp:786432  â 197.64ms â  72.15Mw â  87.78kw â  87.27kw â     26.95% â
â sexp:1048576 â 380.02ms â  96.81Mw â 116.88kw â 116.36kw â     51.82% â
â sexp:1310720 â 360.74ms â 120.18Mw â 144.95kw â 144.43kw â     49.20% â
â sexp:1572864 â 552.49ms â 145.86Mw â 175.26kw â 174.74kw â     75.35% â
â sexp:1835008 â 501.75ms â 170.29Mw â 204.53kw â 204.02kw â     68.43% â
â sexp:2097152 â 733.28ms â 193.98Mw â 233.54kw â 233.03kw â    100.00% â
ââââââââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââââ


ââââââââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââââ
â Name         â Time/Run â  mWd/Run â mjWd/Run â Prom/Run â Percentage â
ââââââââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââââ¤
â sexp:262144  â  32.31ms â  23.51Mw â  23.49kw â  22.98kw â      9.20% â
â sexp:524288  â  65.05ms â  46.61Mw â  45.72kw â  45.21kw â     18.52% â
â sexp:786432  â  96.02ms â  70.45Mw â  69.38kw â  68.87kw â     27.33% â
â sexp:1048576 â 127.03ms â  94.55Mw â  92.63kw â  92.11kw â     36.16% â
â sexp:1310720 â 160.61ms â 117.35Mw â 115.30kw â 114.78kw â     45.72% â
â sexp:1572864 â 192.22ms â 142.47Mw â 138.49kw â 137.98kw â     54.72% â
â sexp:1835008 â 227.44ms â 166.33Mw â 161.14kw â 160.63kw â     64.75% â
â sexp:2097152 â 351.29ms â 189.46Mw â 185.35kw â 184.84kw â    100.00% â
ââââââââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââââ






laplace equaiton = 
minimize gradient sqaures
You get a metrix and a volume factor

g_ij * nabla * nabla


We could mark those things as dynamic or static.



ââââââââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââââ
â Name         â Time/Run â  mWd/Run â mjWd/Run â Prom/Run â Percentage â
ââââââââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââââ¤
â sexp:262144  â  34.92ms â  23.51Mw â  23.48kw â  22.97kw â     13.58% â
â sexp:524288  â  63.74ms â  46.61Mw â  45.74kw â  45.23kw â     24.79% â
â sexp:786432  â  97.24ms â  70.45Mw â  69.45kw â  68.94kw â     37.83% â
â sexp:1048576 â 129.41ms â  94.55Mw â  92.50kw â  91.99kw â     50.34% â
â sexp:1310720 â 157.89ms â 117.35Mw â 115.15kw â 114.64kw â     61.42% â
â sexp:1572864 â 191.14ms â 142.47Mw â 138.52kw â 138.01kw â     74.35% â
â sexp:1835008 â 225.15ms â 166.33Mw â 161.37kw â 160.85kw â     87.58% â
â sexp:2097152 â 257.08ms â 189.46Mw â 185.45kw â 184.93kw â    100.00% â
ââââââââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââââ
 coo coo



 lookahead

look at neal k code
How do they get the character check?



Estimated testing time 1m20s (8 benchmarks x 10s). Change using '-quota'.
ââââââââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââââ
â Name         â Time/Run â  mWd/Run â mjWd/Run â Prom/Run â Percentage â
ââââââââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââââ¤
â sexp:262144  â  40.82ms â  23.51Mw â  23.51kw â  23.00kw â      8.09% â
â sexp:524288  â  81.34ms â  46.61Mw â  46.06kw â  45.55kw â     16.12% â
â sexp:786432  â 127.32ms â  70.45Mw â  69.01kw â  68.49kw â     25.23% â
â sexp:1048576 â 150.35ms â  94.55Mw â  92.63kw â  92.11kw â     29.79% â
â sexp:1310720 â 187.50ms â 117.35Mw â 115.21kw â 114.70kw â     37.15% â
â sexp:1572864 â 209.42ms â 142.47Mw â 138.47kw â 137.96kw â     41.50% â
â sexp:1835008 â 433.92ms â 166.33Mw â 161.27kw â 160.76kw â     85.98% â
â sexp:2097152 â 504.67ms â 189.46Mw â 185.02kw â 184.50kw â    100.00% â
ââââââââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââââ


ââââââââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââââ
â Name         â Time/Run â  mWd/Run â mjWd/Run â Prom/Run â Percentage â
ââââââââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââââ¤
â sexp:262144  â  24.90ms â  21.41Mw â  22.83kw â  22.32kw â     13.19% â
â sexp:524288  â  59.48ms â  42.79Mw â  44.44kw â  43.92kw â     31.51% â
â sexp:786432  â  96.18ms â  64.20Mw â  66.90kw â  66.38kw â     50.94% â
â sexp:1048576 â 103.48ms â  85.61Mw â  89.67kw â  89.15kw â     54.81% â
â sexp:1310720 â 152.96ms â 107.03Mw â 111.11kw â 110.59kw â     81.02% â
â sexp:1572864 â 142.93ms â 128.46Mw â 134.12kw â 133.61kw â     75.71% â
â sexp:1835008 â 165.28ms â 149.71Mw â 156.13kw â 155.62kw â     87.55% â
â sexp:2097152 â 188.79ms â 171.24Mw â 179.61kw â 179.10kw â    100.00% â
ââââââââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââââ

ââââââââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââââ
â Name         â Time/Run â  mWd/Run â mjWd/Run â Prom/Run â Percentage â
ââââââââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââââ¤
â sexp:262144  â  32.36ms â  23.51Mw â  23.49kw â  22.98kw â     12.40% â
â sexp:524288  â  67.51ms â  46.61Mw â  45.95kw â  45.43kw â     25.87% â
â sexp:786432  â  97.97ms â  70.45Mw â  69.52kw â  69.00kw â     37.54% â
â sexp:1048576 â 130.01ms â  94.55Mw â  92.61kw â  92.10kw â     49.82% â
â sexp:1310720 â 162.03ms â 117.35Mw â 115.32kw â 114.81kw â     62.09% â
â sexp:1572864 â 194.94ms â 142.47Mw â 138.57kw â 138.06kw â     74.70% â
â sexp:1835008 â 229.82ms â 166.33Mw â 161.40kw â 160.89kw â     88.06% â
â sexp:2097152 â 260.97ms â 189.46Mw â 185.18kw â 184.67kw â    100.00% â
ââââââââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââââ


Turn inlining back on?

star_dump

try   <= if condition

if

(('a' <= t) && ( t <= 'z'))
|| (('A' <= t) && ( t <= 'Z'))
|| (('0' <= t) && ( t <= '9'))

With the specialzied branching

ââââââââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââââ
â Name         â Time/Run â  mWd/Run â mjWd/Run â Prom/Run â Percentage â
ââââââââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââââ¤
â sexp:262144  â  18.25ms â  17.94Mw â  18.13kw â  17.62kw â     12.53% â
â sexp:524288  â  36.30ms â  35.84Mw â  35.16kw â  34.65kw â     24.94% â
â sexp:786432  â  53.79ms â  53.77Mw â  53.04kw â  52.53kw â     36.94% â
â sexp:1048576 â  72.56ms â  71.72Mw â  70.91kw â  70.40kw â     49.84% â
â sexp:1310720 â  90.17ms â  89.66Mw â  87.96kw â  87.45kw â     61.93% â
â sexp:1572864 â 110.30ms â 107.61Mw â 105.66kw â 105.14kw â     75.76% â
â sexp:1835008 â 127.82ms â 125.39Mw â 123.73kw â 123.22kw â     87.79% â
â sexp:2097152 â 145.59ms â 143.45Mw â 141.39kw â 140.88kw â    100.00% â
ââââââââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââââ

Without specialziation

Estimated testing time 1m20s (8 benchmarks x 10s). Change using '-quota'.
ââââââââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââââ
â Name         â Time/Run â  mWd/Run â mjWd/Run â Prom/Run â Percentage â
ââââââââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââââ¤
â sexp:262144  â  20.15ms â  17.94Mw â  18.13kw â  17.62kw â     10.08% â
â sexp:524288  â  39.77ms â  35.84Mw â  35.16kw â  34.64kw â     19.90% â
â sexp:786432  â  59.98ms â  53.77Mw â  52.95kw â  52.44kw â     30.02% â
â sexp:1048576 â  79.67ms â  71.72Mw â  70.98kw â  70.47kw â     39.87% â
â sexp:1310720 â  99.91ms â  89.66Mw â  87.93kw â  87.41kw â     50.00% â
â sexp:1572864 â 126.68ms â 107.61Mw â 105.72kw â 105.21kw â     63.39% â
â sexp:1835008 â 138.12ms â 125.39Mw â 124.00kw â 123.48kw â     69.12% â
â sexp:2097152 â 199.83ms â 143.45Mw â 141.50kw â 140.99kw â    100.00% â
ââââââââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââââ

Master
ââââââââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââ¬âââââââââââââ
â Name         â Time/Run â  mWd/Run â mjWd/Run â Prom/Run â Percentage â
ââââââââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââ¼âââââââââââââ¤
â sexp:262144  â  36.90ms â  23.51Mw â  23.51kw â  23.00kw â     14.15% â
â sexp:524288  â  65.33ms â  46.61Mw â  45.88kw â  45.37kw â     25.06% â
â sexp:786432  â  97.65ms â  70.45Mw â  69.54kw â  69.03kw â     37.46% â
â sexp:1048576 â 130.66ms â  94.55Mw â  92.56kw â  92.05kw â     50.12% â
â sexp:1310720 â 160.89ms â 117.35Mw â 115.11kw â 114.60kw â     61.72% â
â sexp:1572864 â 195.49ms â 142.47Mw â 138.50kw â 137.98kw â     74.99% â
â sexp:1835008 â 230.79ms â 166.33Mw â 160.99kw â 160.48kw â     88.53% â
â sexp:2097152 â 260.68ms â 189.46Mw â 185.15kw â 184.63kw â    100.00% â
ââââââââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââ´âââââââââââââ



Preliminarily it looks like the ocaml compiler takes things very literally


Happy path first
() are actually pretty rare
We're still building up lists in places probably.
Foolishly


 "m1 @> m2" is the better one I think
It has a ismpler definition

a lot of tuple unpacking

fix isn't mutally recursive.
We could do a record for the composed state
we could still replace the whitespace
every little unit hurts

Replacing __ with unit may have gotten us a slight improvement

The @< problem
That fst doesn't play nice


Is it the new stream code? it seems unlikely

It works for count_as

It's gotta be my compositon
or sexp whitespace
How bad is our use of fix?
just lexer and just parser


I think I should unbundle 
state from stream

Do inlining
do compose in ocaml

ugh. I am not sure we're more "PaRTs" than they are really.

Might not even have to unset the guard check.
( very suspicious )

coq fix + let_ bound





oh ye. The jan willems thing
Grobner basis for  modules




k-Algerba ~ monoid object of kronecker product.



nat ~ list ~ free monad

The dumb version



interepter partial evaluations - why coq, mentiona adam bedrock,
fusion
performance tricks


There is an infinite expansion occuring from
Just the acceptable gassed fix of eval_M
Because ocaml_eq blocks
We can't know what the next thing to do is
we can't pattern match out of the machine
so it's unrolling eval_M forever

This is subtle.



Do my branch
merge
new repo no history

look at cody's branch. see if can compile faster





The MetaOcaml style partial evaluation system has the 


The Coq vm_compute tactic is wonderful. It is engineered to be efficient.
It is already a partial evaluation system.
Coq is already sophisticated in evaluating code. What it is less sophistaticated at is
in _selectively_ evaluating code.
With care, this can be achieved by hand through the -[] mechanism.
There are issues with this. This mechanism is not available for the most performant evaluation tactics.
These annotations are to some degree strong suggestions and not guaranteed forbidden unfolding.


The difficulty lies however in shielding code from being evaluated.

1. Modify the coq kernel of computation. This is an extremely
2. A Plugin. Plugins have a bad reputation of being buggy and often out of sync with the compiler as it is rare that a research plugin ahcieves enough buy-in for it to be maintained.
Invasive intrusion into the compiler of a language rarely works out because you are fighting againt the zen of that language (otherwise whatever you are trying to achieve would've been possible). As programming language research project this is exactly what you want, but what one is truly trying to achieve is building what is actually a new programming language while parasitically using the features of the host. Actual production grade languages evolve very slowly. This is actually part of the definition of what it means to be production grade.
3. More ordinary user level programming techniques using the facilities already available. Teach a man to fish

I would heavily be in favor of 3.

Axioms as code. Coq allows the introduction of opaque Axiom identifiers with type signatures.
Upon extraction, one may choose 
The Axioms are opaque by construction. They abolsutely are not unfoldable by the coq kernel.

Recursive blockage. Ultimately, one needs a full model of the extracted language in some sense. However, rather than modeling the entire ocaml language as a data type, this is a final encoding, which is simpler and easier to use (at the expense of flexibility). The process is roughly on par conceptually with the Coq extraction process itself, which is not verified and a potential weak link.

The ocaml compiler and small abstractions. 

Open research questions:
1. How to play nice with proofs and dependent types.
2. Modelling recursive definitions and imperative features
3. Properties of a MetaOcaml-like type system that meshes well with Coq. Code is an axiom and it would be nice to know the theoretical features of adding such axioms to the coq system.




Interpretation and Partial Evaluation

Writing a simple interpeter for the virtual machine is straightforward.
Each constructor corresponds to a natural operation on the stream and the state of the machine.
For the purposes of sanity checking parsers within coq, an interpreter using lists as the stream is convenient.


However, with the goal extracting these machines to efficient partially evaluated ocaml code, there are a number of non trivial features one has to consider.

1. cps.

It is known as folklore that CPS transfromations speed up libraries.
Popular in use parser combinator libraries like Parsec already make that choice.
The distinction between the optimizations happening inside a compiler like GHC and partial evaluation are mostly about end user control. 

An explicit partial evaluation stage is more predictable.


CPS transformations allow explicit representation of control flow and return points for expressions. It is one weakness of the Coq approach as it stands in comparsion with MetaOcaml that it is possible to write expressions that do not partially evaluate in the intended way. MetaOcaml will raise these as type errors and point to the appropriate spot in the code, whereas in the Coq approach this may either result in silent emission of non-optimized code or an ocaml error in the generated code due to inclusion of remaining compile time infracture. From a user standpoint of the ParTS library this is less of an issue, as they do not have explicit access to the constructs that would create such a problem, but it is an issue for implementer of the library.


CPS gives one the flexibility to pull the 

2. The trick.

A dynamically known value can be converted to a statically known value via a pattern match or if-then-else statement. For example, replacing a call on a dynamically know value `b` such as `f b` with `if b then (f true) else (f false)` can allow non trivial partial evaluation and optimization to occur within each of the branches. This however is at the expense of increasing the size of the code, perhaps exponentially if used without care.

- ASP Paper section 6.1

Olivier Danvy, Karoline MalmkjÃ±r, and Jens Palsberg. 1996. Eta-Expansion Does The Trick. ACM Trans. Program. Lang. Syst. 18, 6 (1996), 730Å751. https://doi.org/10.1145/236114.236119

3. Opacity

The Coq evaluation mechanisms are powerful and engineered to be efficient. However, partial evaluation for the purposes of extraction was not their designed intent.
Coq is sophisticated in evaluating code, but it is less sophisticated at _selectively_ evaluating code.
This issue arises because some functions need to be protected in order to be extracted to efficient ocaml version, such as ascii equality. Internally to coq, ascii is defined an an inductive data type with 7 bool fields. If this were to be extracted, it would result in extremely inefficient parsers.
One approach is to use more conservatively evaluating tactics like simpl and cbn.
Another method to achieve this is to explicitly list the functions to be protected from unfolding in the evaluation command `cbv -[ascii_eq_dec]`
This was found to be unacceptable ultimately because many partial evaluations rely on unfolding and using that very definition.

The best most fool proof method to ensure opacity of definitions is to assert them as Coq Axioms. The system is fundamentally incapable of unfolding these definitions, and they can be annotated to extract to efficient Ocaml counterparts. The most performant evaluation tactics will respect these.

It turns out that with a few carefully chosen axioms, one can isolate the areas of code that are intended to evaluate at compile time vs runtime in such a way that this problem disappears.





Axioms
Lambda abstractions
Let_ bindings

Small scale eta expansions, and inlining are exactly the minimal requirements of an optimization functional compiler.


4. Stream fusion



Oleg Strymonas paper

5. Fix

One big difference between Ocaml and Coq is that Coq requires functions to be terminating by default.

A standard technique to achieve this is situations where termination is not obvious is to use "gas", a integer that decrements on every step of the computation and that terminates it when it reaches zero. 
An efficient Ocaml parser will of course not carry around this unnecessary data nor do this unnecessary decrement computation.
A simple method by which to express this nontermination in Coq is via an axiomatic fix function extracting to a Ocaml fix function in the runtime library. It was determined upon experimentiation that this resulted in significant performance overhead, so instead the Coq fix operator was used with guard checking unset in the extraction evaluator code.


There is an argument to be made for parsers being natural corecursive computations, which are computations that remain productive rather than computations that terminate. However, it was determined that using this methodology would significantly complicate the development.


6. Runtime



	



Functional idfferentiation in sympy
Derive "all" applications of derivatives to an expression.
Reaplce df with uninterpeted function symbol sned over to z3.

I guess we could at least confirm invariants this way?
It's a weakening

y(0) = 1
y = y'


Maybe + mean value theorem? (taylor theorem)

ForAll( [x], Exists( [z], And(0 <= z <= x  , y(x) == y(0) + (x - 0) * f'( z ) )

y(x) == y(0) + y'(0) * x + e


The integral form + 
ForAll([x, z] ,   | y(x) - y(0) + x * f'(0) | <= f''(z)  * ( (x - 0)**3   )  ) 

x = y (1 + e) , |e| <= machine_eps



But then what is it gonna do. It's almost certainly going to go into interval mode.


d^n/dt (  (t - a)  )











